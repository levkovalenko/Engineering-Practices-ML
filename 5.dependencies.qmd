---
title: "Dependency management"
author: "Lev Kovalenko"
format:
    revealjs:
        theme: dark
        self-contained: true
---
 
## Почему это важно?
 
- Repeatability
- Replicability
- Reusable
 
::: {.notes}
Сегодня мы поговорим о зависимостях и рядом с ними. Начнем с простого вопроса почему они важны?
 
Вы проводите исследование, пишите код и довольно часто используете библиотеки. Отсюда и появляется вся важность управления зависимостями.
Во-первых, когда вы добавляете новую библиотеку, вам надо сказать об этом вашей команде, чтобы они тоже использовали эту библиотеку той же версии.
Во-вторых, если вам нужно изменить версию уже используемой библиотеки вам надо сообщить об этом всей команде и проверить что с новой версией ваш код будет работать корректно.
В-третьих, вам надо будет передавать информацию о библиотеках разработчикам, которые будут встраивать ваш пайплайн в сервис.
 
Важными моментами управления зависимостями являются:
 
- Повторяемость - при переустановке библиотек у себя выв получаете тот же результат, что и ранее.
 
- Воспроизводимость - при каждой новой сборке окружения вами, вашими коллегами или ci-cd, выбираются и устанавливаются одни и те же зависимости.
 
- Переисползуемость - механизм управления зависимостями позволяет передавать информацию о выбранных версиях библиотек разработчикам.
 
Вам надо поддерживать свои зависимости и следить за их версиями для того чтобы ваши исследования были воспроизводимы и пере используемы. В ручном режиме это делать довольно проблематично, поэтому сообщество python (и не только его) разработало множество инструментов для этого. Сегодня о них и поговорим.
:::
 
## История развития
 
![](images/dm_history.png){fig-align="center"}
 
::: {.notes}
 
Чуть-чуть истории развития питона. Хотя Python появился примерно в 1990-х годах, ему потребовалось довольно много времени, чтобы адаптироваться к шаблонам, которые другие языки программирования уже использовали при распространении программного обеспечения.
До 2000 года, чтобы распространять свою программу, вам практически приходилось загружать ее где-нибудь в Интернете, где вам нужно было указать точные инструкции по установке программного обеспечения в интерпретатор Python. Таким образом, сообщество запустило distutils. Это привело к появлению файлов setup.py, setup.cfg, содержащих инструкции по установке программного обеспечения и другие полезные метаданные.
Сообществу Python требовалось нечто большее, чем distutils, поэтому был создан setuptools. Он был больше как оболочка над стандартной библиотекой. Я не буду вдаваться в подробности того, что предлагается по сравнению со стандартной библиотекой. Одной из ключевых важных функций была easy_install, которая позволяла вам устанавливать другие пакеты в ваш интерпретатор python.
 
И тут начался этап хаоса в зависимостях. Управление зависимостями быстро превратилось в хаос, поскольку сообщество python начало раскалываться, одни использовали distutils, а другие setuptools. Независимо от этого, все пакеты были собраны в одном месте, что вызывает проблему с корневым доступом. Внесение путаницы в процесс развертывания и отклонение от некоторых стандартных принципов безопасности, таких как никогда ничего не запускать от имени пользователя root.
Возникла потребность в изолированной среде, и примерно в 2007 году появился virtualenv. Эта концепция позволяла разработчикам иметь несколько настроенных интерпретаторов Python для каждого проекта, изолируя интерпретатор Python от базовой установки пакетов в Python. Благодаря этому улучшилось управление зависимостями, что дало множество преимуществ, таких как экспорт среды и изоляция среды.
 
Следующие шаги по упорядочению этого хаоса заключались в создании нового пакетного менеджера, ставшего стандартом. Pip был представлен и заменил причудливый easy_install, pip также представил концепцию requirements.txt, устанавливающую стандарт внутри сообщества Python. Файл может быть создан путем извлечения всех зависимостей из текущего окружения virtualenv. Позже, чтобы pip мог прочитать файл и воссоздать ту же виртуальную среду.
 
До 2017 pip и virtualenv заставляли разработчиков страдать, поскольку им приходилось использовать два отдельных инструмента для изоляции своей среды и управления своими зависимостями. Первым шагом по объединению этих инструментов стал pipenv. Несмотря на то, что pipenv был прекрасен для разработчиков, разрабатывающих приложения, он не очень помог библиотекам. Теперь вам нужно обрабатывать еще больше файлов Pipfile, Pipfile.lock, setup.py и т. Д. поэтому на его смену пришел poetry.
 
Poetry - это полноценный менеджер пакетов, который предлагает больше, чем просто управление зависимостями и упаковку. Он также пытается обеспечить соблюдение таких стандартов, как семантическое управление версиями, структура папок и шаблоны упаковки, с которыми, скорее всего, знакомы программисты из других сообществ.
 
Теперь разберем пару инструментов, которые упоминали. Сразу скажу, что мы не будем рассматривать первые инструменты управления зависимостями в питоне, а перейдем к более актуальным.
:::
 
# Pip - default package manager
 
::: {.notes}
Начнем с pip, дефолтного менеджера пакетов для питона. Рассмотрим его возможности, которые могут пригодиться при работе, и обсудим его минусы и плюсы.
:::
 
## Install packages {.smaller .scrollable}
 
Установка пакетов.
```sh
pip install <package1> <package2>
```
 
Установка пакета определенной версии.
```sh
pip install <package1==version1> '<package2>=version2>'
```
 
Установка пакетов из `requirements.txt`.
```sh
pip install -r requirements.txt
```
 
Обновление уже установленного пакета.
```sh
pip install --upgrade <package>
```
 
Использование приватного индекса пакетов.
```sh
pip install --index-url <index-url> <package>
```
 
Установка пакетов из git.
```sh
pip install git+https://<git-package-url>@<version>
```
 
Установка пакетов с расширениями.
```sh
pip install <package>[<extras>]
```
 
Установка пререлизного пакета.
```sh
pip install --pre <package>
```
 
Установка своих пакетов в редактируемой режиме.
```sh
pip install --editable ./<package-path>
```
 
Установка своих пакетов из исходного кода.
```sh
pip install --no-binary <package>
```
 
::: {.notes}
Основные возможности по [установке](https://pip.pypa.io/en/stable/cli/pip_install/#examples) пакетов:
 
- вы можете установить один или несколько пакетов
- можно установить пакеты определенных версий выставив ограничения, или же установить ограничение на минимальную доступную версию
- установить пакеты из файла зависимостей
- обновить уже установленный пакет
- установить пакеты из приватных или отдельных репозиториев(индексов) пакетов
- Установить версию из гита, с точностью до коммита или версии
- Установить пакет с расширением
- Установить пакет пререлизной версии
- установить разрабатываемый пакет в редактируемом режиме для облегчения разработки
- установить пакет из исходного кода.
:::
 
## Dependecy resolver
 
- Появился `backtracking dependencies`
- Увеличена `строгость` резолвера
- Появился `state managment`
- Улучшилась поддержка `constraints`
 
::: {.notes}
- бета-версия появилась в 20.2 (2020-07-29)
- стабильная [версия](https://pip.pypa.io/en/stable/user_guide/#changes-to-the-pip-dependency-resolver-in-20-3-2020) появилась в 20.3 (2020-11-30)
 
Далее он развивался в течении двух лет и достиг довольно неплохих результатов.
 
В этом dependecy resolver появилась поддержка отката версий зависимости, если не находится кандидат, то версии зависимостей могут быть понижены для разрешения зависимостей. Ранее это игнорировалось и ставился первый попавшийся кандидат.
 
Так же теперь resover запрещает ставить несовместимые пакеты и выдает ошибку, ранее он выдавал только warning. Так же он начал обращать свое внимание на constraints и учитывать их при поиске кандидатов и установке зависимостей.
 
Ну и наконец-то в нем появился state managment, то есть он начал адекватно работать текущее окружение начало влиять на выбор совместимых кандидатов.
 
С появлением этого апдейта сборки у pip стали воспроизводимы и повторяемы. В целом текущее развитие pip dependecy resolver направлено на улучшение воспроизводимости окружения. Главное не используйте pip моложе 20.3 версии, там практически нет dependecy resolver.
:::
 
## Requirement dependecies
 
Что бы зафиксировать требуемые зависимости можно:
```shi
pip freeze > requirements.txt
```
\
 
:::: {.columns}
 
::: {.column width="50%"}
Строгая фиксация requirements.txt:
```text
certifi==x.y.z
charset-normalizer==x.y.z
idna==x.y.z
requests==x.y.z
urllib3==x.y.z
```
:::
 
::: {.column width="50%"}
Тонкая настройка requirements.txt:
```text
certifi>=x.y.z
charset-normalizer>=x.y.z
idna>=x.y.z
requests>=x.y.z
urllib3>=x.y.z
```
:::
::::
 
::: {.notes}
Когда вы делитесь своим проектом Python с другими разработчиками, вы можете захотеть, чтобы они использовали те же версии внешних пакетов, что и вы. Возможно, конкретная версия пакета содержит новую функцию, на которую вы полагаетесь, или версия пакета, которую вы используете, несовместима с предыдущими версиями. Поэтому их стоит фиксировать.
 
В pip они фиксируются в иде списка пакетов с их версиями в фале requirements. txt. Pip по сути делает слепок текущего окружения.
 
По умолчанию Pip строго фиксирует версии библиотек, как это показано на слайде. Проблема строгого кодирования версий и зависимостей ваших пакетов заключается в том, что пакеты часто обновляются с исправлениями ошибок и безопасности. Вы, вероятно, захотите использовать эти обновления, как только они будут опубликованы. К сожалению, в pip нет возможности тонка настроить requirements, и нужно их корректировать ручками после каждого pip freeze.
:::
 
## Dev/Prod dependecies
 
Зависимости для развертывания
```sh
# requirements.txt
package==1.0
package==1.0
package==1.0
```
 
Зависитмости для разработки
```sh
# dev_requirements.txt
-r requirements.txt
dev_package==1.0
dev_package==1.0
dev_package==1.0
```
 
::: {.notes}
Еще важный момент это разделение prod и dev зависимостей. И в pip это реализовано очень плохо. Автоматического разделения нет и придётся все зависимости разделять ручками, с учетом, что бывает 2 или 3, а то и больше, уровней зависимостей. Соответственно после каждого freeze нужно самому разделить их на два файла.
:::
 
## Pros&Cons
 
 
:::: {.columns}
 
::: {.column width="50%"}
**Pros**
 
- default tool
- deterministic builds
- dependecy resolver
 
:::
 
::: {.column width="50%"}
**Cons**
 
- no built-in isolation tool
- problematical dev and prod dependency split
- no tools for lib packaging and publishing
- buildings from source
 
:::
::::
 
::: {.notes}
C версии 20.3 появились первые значимые плюсы. Появились воспроизводимые сборки и инструмент разрешения зависимостей, выше мы обсудили какой большой impact они вносят в работу пакетного менеджера.
 
Нет инструмента для изоляции окружения проекта. Это не сильно плохо, но тоже является проблемой. Так как вам надо самому создать виртуальное окружение используя для этого отдельный инструмент.
 
 
Нет возможности разделить зависимости на dev и prod, у вас в одном файле будут содержаться и инструменты разработчика линтеры, форматеры и анализаторы кода, в том числе и jupyter notebook,  и продовские зависимости numpy, sklearn и другие. Это очень сложно разобрать и поддерживать. Кроме этого у вас в одном файле хранятся все ваши зависимости и зависимости зависимостей и так далее, то есть очень много технического мусора, который вы напрямую не используете. В итоге у вас есть один файл — большая помойка зависимостей.
 
Нет инструментов для сборки и публикации пакетов. То есть на вас перекладывается проблема по поддержанию структуры проекта, файлов конфигурации сборки и продовских зависимостей. Всем этим вам придется заниматься вручную и это может стать большой проблемой когда проект станет довольно большим.
 
Сборка из исходного кода. Не могу сказать, что это большая проблема, пока мы говорим про исходный код на питоне. Но, так как питон не очень быстрый язык, то часто используются библиотеки, написанные на C, особенно в data science. Наиболее популярны библиотеки уже имеют бинарные сборки, помещенные в репозиторий пакетов, но не дай бог вам попадется библиотека, которую и придется собирать из исходного кода с элементами C. Тогда ваш проект будет зависеть не только от питоновского окружения, но и от окружения для С, а это не сильно приятно. Так как если у вас другая версия компилятора, то проект не соберется. Ради интереса можете попробовать собрать tensorflow и прочувствовать всю боль.
:::
 
# Conda - package, dependency and environment manager
 
::: {.notes}
Следующий менеджер пакетов, который мы затронем это conda. Он подходит для работы с зависимостями на разных языках: Pyton, R, C/C++ и другими. Давайте рассмотрим ее возможности.
:::
 
## Conda packages
 
:::: {.columns}
 
::: {.column width="50%"}
Структура пакета
```text
.
├── bin
│   └── pyflakes
├── info
│   ├── LICENSE.txt
│   ├── files
│   ├── index.json
│   ├── paths.json
│   └── recipe
└── lib
    └── python3.5
```
:::
 
::: {.column width="50%"}
Поиск пакетов:
```sh
conda search <package>
```
 
Установка пакетов:
```sh
conda install <package>
```
 
Сборка пакетов с помощью [conda-build](https://docs.conda.io/projects/conda-build/en/latest/index.html):
```sh
conda build <my_package>
```
:::
::::
 
::: {.notes}
Пакет conda представляет собой сжатый файл tarball (.tar.bz2) или файл .conda, который содержит:
 
- библиотеки системного уровня.
- Python или другие модули.
- исполняемые программы и другие компоненты.
- метаданные в `info/` каталоге.
- набор файлов, которые устанавливаются непосредственно в `install` префикс.
 
Если смотреть на структуру пакета, то в данные распределяются так:
 
- bin содержит соответствующие двоичные файлы для пакета.
- lib содержит соответствующие файлы библиотек (например, файлы .py).
- info содержит метаданные пакета.
 
Conda отслеживает зависимости между пакетами и платформами. Формат пакета conda идентичен для разных платформ и операционных систем.
 
Так же хочется упомянуть про `noarch` пакеты. Пакеты Noarch — это пакеты, которые не зависят от архитектуры и поэтому должны быть собраны только один раз. Пакеты Noarch являются либо общими, либо Python. Общие пакеты Noarch позволяют пользователям распространять документы, наборы данных и исходный код в пакетах conda. Пакеты Noarch Python сокращают накладные расходы на создание нескольких разных чистых пакетов Python для разных архитектур и версий Python за счет сортировки различий, зависящих от платформы и версии Python, во время установки.
:::
 
## Conda channels
 
Установка из определенного `channel`
```sh
conda install scipy --channel conda-forge
```
Добавление `channel`  по умолчанию
```sh
conda config --add channels new_channel
```
 
Список доступных `channels`:
```text
anaconda
r
conda_forge
bioconda
astropy
metachannel
javascript
private
```
 
::: {.notes}
Собственно, у нас есть пакеты, и появляется закономерный вопрос, а откуда их брать? Тут нам помогут conda channels.
 
У conda есть возможность искать пакеты из разных каналов, так же можно указывать приоритете поиска по каналам, добавлять или удалять каналы. Я привел небольшой список каналов сообщества conda. Наиболее обширный канал это conda-forge. В этот канал публикуется большая часть питоновских пакетов из pypi с помощью сообщества. Очень советую им пользоваться.
 
Большим плюсом является возможность поднятие собственных каналов, в том числе зеркал существующих. То есть, если вы, например, работаете в компании, то вы можете в контуре компании скачивать все эти пакеты в каналы и устанавливать из них. То есть на стороне компании будет находиться весь набор необходимых зависимостей для сборки вашего проекта.
:::
 
## Conda environments {.scrollable}
 
Создание пустого окружения
```sh
conda create --name <myenv>
```
 
Создание питоноского окружения
```sh
conda create -n <myenv> python=3.9
```
 
Установка пакета в определенное окружение
```sh
conda install -n <myenv> <package>
```
 
Создание файла зависимостей
```sh
# только те которые были устанволены в ручную
conda env export --from-history > environment.yml
# все зависимости и сабзависимости
conda env export > environment.yml
```
 
Создание окружения из файла зависимостей
```sh
conda env create -f environment.yml
```
 
Обновление окружения из файла зависимостей
```sh
conda env update -f environment.yml
```
 
::: {.notes}
Замечу что в этом инструменте есть нативная поддержка изолированных окружений. Куда лучше, чем питоновские, которые формируются на основе слепка интерпретатора. Здесь в каждом окружение может находиться несколько версий интерпретатора питона для нормальной работы всех зависимостей. То есть conda рассматривает сам питон как зависимость.
 
Удобно то что все окружения хранятся централизована в кеше конды, есть возможность создавать файлы зависимостей как всех установленных библиотек, так и фиксировать версии только тех библиотек, которые вы сами устанавливали. Так же на основе такого файла можно создать у себя окружение с нуля или обновить существующее.
 
Конда предлагает отличный инструмент менеджмента виртуальных окружений. Я сам им постоянно пользуюсь, как минимум потому что питон в конде изолирован от системного полностью, и я могу легко менять его версии при необходимости.
:::
 
## Pros&Cons
 
:::: {.columns}
 
::: {.column width="50%"}
**Pros**
 
- deterministic builds
- dependecy resolver
- built-in isolation tool
- tools for lib packaging and publishing
- builded binary packages
 
:::
 
::: {.column width="50%"}
**Cons**
 
- very slow dependency resolver
- a lot of steps for preparing package build
- no dependencies split
 
:::
::::
 
::: {.notes}
Основными достоинствами conda являются детерминированные сборки, разрешение конфликтов зависимостей, встроенный менеджер виртуальных окружений, инструменты для упаковки и сбор пакетов. Самое главное, что в conda используются уже собранные бинарные пакеты, что сильно сокращает время работы при установке зависимостей и не требует повторных сборок из исходников.
:::
 
 
# [Mamba](https://mamba.readthedocs.io/en/latest/)
 
::: {.notes}
 
Mamba — это drop-in замена Conda, потрясающего кроссплатформенного менеджера пакетов. На наш взгляд у Conda есть один фатальный недостаток: она слишком медленная, когда много пакетов уже установлено или при одновременной установке нескольких пакетов. Mamba по-прежнему использует Conda почти для всего, кроме разрешения зависимостей.
 
В mamba заменили эту часть Conda на альтернативную реализацию под названием libsolv — C библиотеку, которая уже лежит в основе менеджеров пакетов в Linux системах типа dnf в Fedora или zypper в OpenSuse.
 
Интерфейс командной строки, работа с окружением, формат файла пакета, процесс установки, формат repodata и всё остальное — один-в-один как в Conda и работает на том же питоновском коде, что и Conda. Так что отличие, и правда, только в разрешении зависимостей!
:::
 
# Poetry
::: {.notes}
Ну и наконец мы сегодня рассмотрим новомодный менеджер управления зависимостями python. Если кто-то из вас занимался разработкой на javascript, то вам он покажется чем-то походим на npm или yarn.
:::
 
## Project structure
 
:::: {.columns}
 
::: {.column width="40%"}
Структура проекта
 
```text
poetry-demo
├── pyproject.toml
├── poetry.lock
├── README.md
├── poetry_demo
│   └── __init__.py
└── tests
    └── __init__.py
```
 
Инициализация проекта
```sh
poetry new <project-path>
```
 
:::
 
::: {.column width="60%"}
Pyproject.toml
```toml
[tool.poetry]
name = "poetry-demo"
version = "0.1.0"
description = ""
authors = ["Kovalenko Lev"]
readme = "README.md"
packages = [{include = "poetry_demo"}]
 
[tool.poetry.dependencies]
python = "^3.9"
 
 
[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```
:::
::::
 
::: {.notes}
Во-первых, poetry добавляет свою структуру проекта, в которой создаются основные директории, а также файлы для описания зависимостей и метаинформации проекта. Для создания нового poetry проекта, можно вызвать одну команду и получить готовый шаблон.
 
Стоит уделить вниманию что poetry использует Pyptojeсt file согласно PEP 621 о хранении метаинформации. В нем poetry фиксирует зависимости проекта.
 
Также poetry создает poetry.lock файл, в котором хранится информация о том, какие нужны зависимости, зависимости зависимостей и так далее для сборки окружения, из каких источников ставить эти зависимости.
:::
 
## Dependency install {.scrollable}
Установка пакета
```sh
poetry add <package>
```
 
Установка пакета определенной версии
```sh
# Allow >=2.0.5, <3.0.0 versions
poetry add package@^2.0.5
# Allow >=2.0.5, <2.1.0 versions
poetry add package@~2.0.5
# Allow >=2.0.5 versions, without upper bound
poetry add "package>=2.0.5"
# Allow only 2.0.5 version
poetry add package==2.0.5
```
 
Установка пакета из git
```sh
poetry add git+https://github.com/sdispater/package.git#version
```
 
Установка своих пакетов в редактируемой режиме
```sh
poetry add ---editable ./<package-path>
```
 
Добавление зависимости в определенную группу
```sh
poetry add <package> --group <group>
# для dev зависимостей
poetry add <package> --group dev
poetry add --dev <package>
```
 
Установка пререлизных зависимостей
```sh
poetry add --allow-prereleases <package>
```
 
Установка зависимостей из различных репозиториев
```sh
poetry source add <source> https://<source-link>
poetry add --source <source> <package>
```
 
Установка расширений зависимостей
```sh
poetry add "<package>[<extras>]"
```
 
Посмотреть наличие обновлений
```sh
poetry show --tree
```
 
Провести обновление зависимостей
```sh
# обновить конкретную зависимость
poetry add <package>@latest
poetry update <package>
# обновить под зависимости
poetry update
```
 
::: {.notes}
Poetry позваляет проводить все те же операции что и pip. Работать с разными зависимостями, ставить пределенные версии, устанавливать зависимости из разных источников, прерлизные зависимости, расширения зависимостей. В плане установки зависимостей он полностью идентичен pip.
 
Более того он позволяет группировать зависимости, что позволяет сделать dev/prod split или же выделять какие-то дополнительные зависимости для оптимизации в группы.
 
Также есть функционал для отслеживания обновлений зависимостей и самого обновления зависимостей.
:::
 
## Package build and publish
Сборка пакетов локально
```sh
poetry build
```
 
Публикация собранного пакета
```sh
poetry publish
```
 
Публикация и сборка пакета
```sh
poetry publish --build
```
 
Публикация пакета в приватный репозиторий
```sh
poetry publish -r <repo>
```
 
::: {.notes}
Также хочется отметить cli poetry для сборки и публикации пакетов. Он очень удобен и прост в использовании. Буквально одной командой можно собрать и опубликовать пакет и не нужно дополнительных плясок с бубнами c setup.py и прочим.
:::
 
## Pros&Cons
 
:::: {.columns}
 
::: {.column width="50%"}
**Pros**
 
- deterministic builds
- dependecy resolver
- dependecies groups
- built-in isolation tool
- tools for lib packaging and publishing
- dependencies update tracking
 
:::
 
::: {.column width="50%"}
**Cons**
 
- buildings from source
- a very young tool
 
:::
::::
 
::: {.notes}
Пакетный менеджер собрал все основные best pracices. Единственные два минуса — если нет колес, то будет собирать из исходников, что бывает больно. А также это довольно молодой инструмент и у него встречаются различные баги, я сам встречал баги с работой с несколькими репозиториями, с приватными репозиториями. Думаю, со временем он будет развиваться и решит эти проблемы. Часть из них уже решил если что)
:::
 
## Выводы
 
- pip - дефолтный менеджер пакетов, соответствует всем pep
- conda - лидер по environments, позволяет работать только с бинарными пакетами
- poetry - собрал в себя все лучшие практики, но очень молодая технология
 
 
::: {.notes}
Мы рассмотрели с вами несколько пакетных менеджеров. У всех из них есть свои достоинства и недостатки. Но тут стоит для себя решить, что для вас критично, а что нет.
 
Я для себя и в своей команде использую связку conda и poetry. Conda нужна для работы с непитоновскими зависимостями и менеджмента виртуальных окружений. Poetry занимается установкой питоновских зависимостей и отвечает за сборку и публикацию библиотек.
:::
