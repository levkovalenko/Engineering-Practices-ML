---
title: "Project structure"
author: "Lev Kovalenko"
format: 
    revealjs: 
        theme: dark
        self-contained: true
        echo: true
        source: true
jupyter: "epml"
---

## Из чего состоит DS проект?

[![](https://mermaid.ink/img/pako:eNpd0r1uwjAQB_BXsTzDYGe7oRIQSIciVWHoksVyXLBIbGRfKlWIh-kb9ZFq_KESMvn3PyvWne5Kpe0VBepRoKq1ODoxLr94Z0j4VlB_vKbjGt6sFAOpBYqUbBi0yishT9n8ydXcNYP3waLP4nAIT2qPWpaogvvfSa-8dPqC2ppU2LJY8Aqz-dw7BvvQxUDQCW20OeaYP8Uk5Q38_lgjh8mHF_LbLZBWXazDfGdFlssXss69R6wesWEz8ZmqMqDI-omsjCuRl2lFbtmcvEwgcsfKABJ56T-yKW0_qk61tkx9pupRTQFd0FG5Ueg-7MX1XuwontSoOgrh2At37mhnbuGemNAevo2kgG5SCzpd-v81ovApBh9S1Wu0bp8WLe7b7Q9Pvawj?type=png)](https://mermaid.live/edit#pako:eNpd0r1uwjAQB_BXsTzDYGe7oRIQSIciVWHoksVyXLBIbGRfKlWIh-kb9ZFq_KESMvn3PyvWne5Kpe0VBepRoKq1ODoxLr94Z0j4VlB_vKbjGt6sFAOpBYqUbBi0yishT9n8ydXcNYP3waLP4nAIT2qPWpaogvvfSa-8dPqC2ppU2LJY8Aqz-dw7BvvQxUDQCW20OeaYP8Uk5Q38_lgjh8mHF_LbLZBWXazDfGdFlssXss69R6wesWEz8ZmqMqDI-omsjCuRl2lFbtmcvEwgcsfKABJ56T-yKW0_qk61tkx9pupRTQFd0FG5Ueg-7MX1XuwontSoOgrh2At37mhnbuGemNAevo2kgG5SCzpd-v81ovApBh9S1Wu0bp8WLe7b7Q9Pvawj)


::: {.notes}

```{mermaid}
stateDiagram-v2
    A:DWH
    B:Local Data
    C1:Reseach
    C2:Reseach
    C3:Reseach
    D1:Plots
    D2:Statistics
    D3:Data description
    E1:Dataset
    E2:Dataset
    F1:Model training
    F2:Model training 
    G:Сonclusions
    R: Report 
    A --> B
    B --> A
    B --> C1
    B --> C2
    B --> C3
    C1 --> D3
    C1 --> D1
    C2 --> D2
    C3 --> E1
    C3 --> E2
    E1 --> F1
    E2 --> F2
    F1 --> G
    F2 --> G
    D1 --> R
    D2 --> R
    D3 --> R
    G --> R
```


- Данные - то, с чего начинается любой data science проект. Стоит отметить, сразу несколько деталей касательно данных, о который можно забыть если только решать задачки на kaggle. Первое, скорее всего данными придеться делиться, то есть передавать их другим исследователям, поэтому необходимо центролизованное хранилище данных. Второе, что стоит отметить, данные на самом деле не константные, в процессе исследования вы будете обогащать данные, возможно из третьих источников, а может будут появляться новые данные от заказчика. В любом случае, необходимо решить две задачи: защитить от внешних изменений исходные данные и ввести систему контроля версий данных для отслеживания изменений в данных. Собственно, на идейном уровне эти задачи решают системы контроля верси данных, о которых мы говорили ранее, но как организовать структуру хранения данных в проекте что бы не запутаться в них? 
- Еще есть разведочный анализ  - важный этап любого исследования. Мы уже говорили о важности постановки гипотез перед исследованием на первом занятии, но я хочу еще раз это повторить. Важно, перед началом работы над проектом установить главную гипотезу вашего проекта и все последующие гипотезы должны исходить из нее. Если вы в начале начинается “копаться” в данных, а лишь затем придумываете гипотезу, вы во-первых, можете зайти быстро в тупик, а во-вторых, начать обманывать самих себя, придумывая объяснения случайно найденным зависимостям. По результатам разведочного анализа должны появиться умозаключения в виде текста и графиков. Вы в начале устанавливаете, а что хотите найти или проверить, а затем должны сохранить ваши результаты. На самом деле, не обязательно пытаться за один раз найти всё. Лучше будет дробить это на малые задачи, в ходе которых вы либо что-то проверяете, либо же пытаетесь что-то описать в данных. То есть, вполне одним из результатов разведочного анализа может быть отчет с описанием формата данных, а может каких-то статистических характеристик ваших данных. В том числе исходя из результатов разведочного анализа, вы можете перейти к следующему этапу, формирование датасетов для моделирования.
- Эксперименты с моделями. На самом деле этот этап не должен сильно отличаться от разведочного анализа. Мы ставим гипотезы, строим модели, проверяем их работу и должны делать выводы в том числе закреплять их в виде текста. Однако, модели сами по себе требуют дополнительной работы по своему построению, а также должны оставаться в качестве результатов вашей работы, поэтому этот этап выделяется отдельно. И таких экспериментов у вас будет много. однако помимо отчетов по каждому отдельному исследованию и модели в конце концов у вас должен быть финальный результат по данному этапу, это ваши заключения по результатам тестов моделей, какая архитектура более подходящая, какие модели показали значения метрик лучше и почему и т.д. 

Таким образом, мы видим, что помимо самого кода, для выполнения всех этих манипуляций, в ds проект входят: данные, модели, отчеты в виде текста, графики, возможно ещё какие-то вложения, которые вы будете помещать в свой отчет, не стоит забывать про файлы с конфигурацией, тесты, документация к коду, возможно какие-то вспомогательные файлы для CI и работы с контейнерами… То есть на самом деле, полноценный ds проект имеет множество объектов, которые, если не организовать в строгую структуру, могут превратить проект в месиво из кучи разнородных файлов.

Отсюда и появляется вопрос - как это все организовать?
:::

## [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/){.scrollable}

. . .

```{md}
├── LICENSE
├── Makefile           <- Makefile with commands like `make data` or `make train`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── docs               <- A default Sphinx project; see sphinx-doc.org for details
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator's initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── setup.py           <- Make this project pip installable with `pip install -e`
├── src                <- Source code for use in this project.
│   ├── __init__.py    <- Makes src a Python module
│   │
│   ├── data           <- Scripts to download or generate data
│   │   └── make_dataset.py
│   │
│   ├── features       <- Scripts to turn raw data into features for modeling
│   │   └── build_features.py
│   │
│   ├── models         <- Scripts to train models and then use trained models to make
│   │   │                 predictions
│   │   ├── predict_model.py
│   │   └── train_model.py
│   │
│   └── visualization  <- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py
│
└── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io
```

. . .

Как этим пользоваться?
```sh
pip install cookiecutter
cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

:::{.notes}
Собсвенно решение есть:
Логичная, достаточно стандартизированная, но при этом гибкая структура проекта для выполнения работы по науке о данных и обмена ею. Предлагется она группой drivendata.

В процессе проведения исследвоаний, последнее о чем вы зачастую думаете это организация и обудмывание структуры проекта, так как есть более важные шаги - сами исследования.Поэтому лучше всего начать с чистой, логичной структуры и придерживаться ее на всем протяжении проекта. Основные причины так поступить заключаются в том, что

- Другие люди будут вам благодарны. Шаблонная структура проекта давно появилась в области разработки ПО. Например angular, django ruby on rails и другие масштабные и популярные фреймворки предлагают инициализировать проект согласнно определенной структуре. При соблюдении единой структуры в преокте, новичкам, а такие будут переодически появляться в вашей команде, будет проще осознать где и что находится, не читая всю документацию и код. Было бы очень классно если бы в сообществе DS была бы принята единая структура преокта, было бы куда проще разбираться с любыми проектами.
- Вы сами будете благодарны сами себе. Вы когда-нибудь пытались воспроизвести анализ, который вы сделали несколько месяцев назад или даже несколько лет назад? Что и где нужно запустить? В какой последовательности? А там ли лежат данные? А какой файл нужно подать в тот или иной скрипт? Такие вопросы болезненны и являются симптомами неорганизованности проекта. Хорошая структура проекта поощряет приемы, упрощающие возвращение к старой работе, наприер единая точка запуска проекта или система контроля версий данных.

Авторы cookiecutter data scinece предлагают следующую структуру:

- makefile - единая точка запуска всех скриптов, в которой описаны необходимые команды
- readme - верхнеуровневая документация, в которой описываются какие-то зависимости(типа операционной системы или версии conda), основные команды для подготовки и запуска окружения, стурктура проекта и описание цели проекта.
- data - там где мы хотим хранить все данные. raw туда куда попадает первоначальная выгрузка, external для каких-то внешних данных или дополнительных данных от заказчиков, interim - промежуточные данные, которы могут быть получены в результате очистки данных, фильтрации аномалий, различных преобразований и feature engeniring, processed для хранения итоговых датасетов для моделирования и оценки качества.
- docs - дефолтный проект sphinx, где пишутся все отчеты. 
- models - хранение результатов моделирования
- notebooks - юпитер ноутбуки для иллюстрации некоторых исследвоаний
- references - дополнительные материалы, например статьи по решеаемой задачи, какие-то инстуркции и прочее.
- reports - предполагается использовать sphinx для генерации документации и сгенерированные отчеты складируются в этой папке.
- setup.py и tox.ini - файлы конфигурации проекта
- src - папка для кода, где у нас есть разделение на обработку данных, фичаинжиниринг, моделирвоание и визуализацию.

Ну и конечно отвечая на вопрос, а все ли это надо создавать руками? могу сказать что авторы этйо структуры прописали шаблонизатор на основе cookiecutter который по одной команде создаст для вас всю структуру проекта.
:::

# Идеи шаблона

:::{.notes}
В этот шаблон были вложены некоторые идеи при его создании, вы можете с ними согласиться или же нет, можете на их основе сделать свои шаблоны, поэтому я хотел бы их разобрать каждую отдельно.
:::

## Данные не изменны

> Никогда не редактируйте сырые данные, особенно вручную.

:::{.notes}
Никогда не редактируйте сырые данные, особенно вручную, и тем более в Excel. Не переписывайте сырые данные. Не сохраняйте несколько версий сырых данных. Считайте данные (и их формат) неизменными. Код, который вы пишете, должен передавать сырые данные по конвейеру для получения финального результата. Вам не нужно выполнять все шаги каждый раз, когда вы хотите получить результат для определенного шага, но любой должен иметь возможность воспроизвести конечный результат только с кодом в src и данными в data/raw.
:::

## Jupyter notebook’и для исследований и коммуникаций

> Инструменты, как jupyter notebook могут быть полезны, однако не очень эффективны в вопросе воспроизводимости.

:::{.notes}
Для хранения notebook есть отдельная папка в шаблоне. Хорошей практикой будет разбивать файлы в этой папке на подпапки, например, notebooks/exploratory или notebooks/reports.
Также из-за структуры notebook, git diff может быть очень неудобным для чтения и поэтому рекомендуется после быстрого прототипирования в notebook переносить код в src и импортировать его в notebook. Таким образом оставляя в блокноте небольшие блоки кода и текстовое описание.
:::

## src как python пакет
> Оформляйте код в src как python пакет, готовый к передаче.

:::{.notes}
Ранее мы говорили о том, что хотели бы переиспользовать код, полученный при исследовании, при использовании модели. Для этого лучшим вариантом будет оформлять src как python пакет, который затем мы сможем передать для разработки, например, сервиса для использования модели. Для этого в папке src создается ``__init__.py`` файл. Дальше вы можете использовать либо pip и создать setup.py, либо poetry и в конце исследования упаковать ваш код в готовый пакет. По своему опыту скажу что эта идея не сработала, уже несколько раз в коцне проекта требовалось рефакторить и переписывать код для передачи его бекендерам. Основная причина в том что код написан для обработки целого фрейма данных, а не одного семпла, для этого приходится дублировать код и вносить в него изменения. 
:::

## Анализ - это DAG
> Анализ данных хорошо представляется в виде направленного ациклического графа.

:::{.notes}
Часто в анализе у вас есть длительные по времени этапы, например, предварительной обработки данных или обучения моделей.Если эти шаги уже были выполнены (и вы сохранили вывод где-то, например, в каталоге data/interim, data/processed), вы не хотите ждать их повторного вычисления, при повторном запуске пайплайна.Для того, чтобы реализовать такую логику, хорошо подходят workflow менеджеры, например, make.
:::

## Проблемы

- Логика cli размазана по проекту
- Make имеет сложный и непонятный синтаксис
- Не учтено какое-то версионирвоание данных
- Нет структуры отчета

:::{.notes}
Это одна из лучших сущесвующих структур datascience проекта, но в ней тоже есть недостатки.

Один из них в том, что питоновских фйлах приходится писать блок main для превращения их в скрипты. Из-за этого логика консольного интерфейса размазана по всему преокту и внесение в нее правок проблематично. Одним из решений является использование click и вынесения всей логики в отдельный модуль src, на уровне data, models И других. Так получается, что весь интерфейс вынесен в одном месте, кроме того организован куда удобнее чем через argparse.

Make имеет сложный и не очень понятный синтаксис, это делает лсожным поддержку проекта, особенно когда этот файл разрастается до огромных размеров.  Здесь стоит присмотреться к другим менджерам workflow, например, Paver, Luigi, Airflow, Snakemake, DVC или Joblib. Сегодня мы поразбираем некоторые из них. Так же стоит выделить для правил не один файл, а целую папку типа workflows, где описываются как раз все правила вызовов скриптов по разным модулям.

В шаблоне изначально не предполагается использваоние каких то систем версионирвоания данных и соотвественно нужно будет вносить в него корректировки что бы подключить какие-то инструменты, напрмиер для dvc изменение файлов gitignore.

Авторы структуры проекта не предлагают какой-то структуры для отчетов. А это тоже важная тема, ведь знания являются наиболее важным артефактом DS проекта и их нужно структурировать понятным образом.
:::