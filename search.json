[
  {
    "objectID": "1.entry.html",
    "href": "1.entry.html",
    "title": "Engineering practices in ML",
    "section": "",
    "text": "Познакомиться с повсеместными проблемами отрасли и их глубиной.\n\n\n\nУзнать их негативное влияние на разных уровнях (разработчик, команда, компания, индустрия).\n\n\n\n\nИзучить инструменты и практики позволяющие снизить это влияние.\n\n\n\n\nПоднять уровень компетенций в области ML."
  },
  {
    "objectID": "1.entry.html#почему-такое-название",
    "href": "1.entry.html#почему-такое-название",
    "title": "Engineering practices in ML",
    "section": "Почему такое название?",
    "text": "Почему такое название?\n\n\n\n\n\n\nСовременный data science и machine learning требуют написания большого количества кода. Прошли те дни когда все считалось на бумажках или в экселе. Теперь для разработки ML проектов требуется использование языков программирования общего пользования, библиотек и прочего. Собственно требуется “разработка ПО”. А это область давно и хорошо изучена."
  },
  {
    "objectID": "1.entry.html#инженерные-практики-в-разработке-по",
    "href": "1.entry.html#инженерные-практики-в-разработке-по",
    "title": "Engineering practices in ML",
    "section": "Инженерные практики в разработке ПО",
    "text": "Инженерные практики в разработке ПО\n\nTest driven development\nRefactoring\nDesign Improvement\nContinuous integration - continuous delivery\nPair programming\nAutomated tests\n\n\nВ области разработки ПО уже давно используются различные инженерные практики, общепринятые подходы для развития и увеличения производительности команды разработки ПО. Зачастую они упоминаются в различных agile методологиях, но на самом деле могут существовать отдельно. Применение таких практик позволяет поддерживать хорошее качество кода и архитектуры, быстрее поставлять новые фичи, повышать уровень развития команды. Собственно применение этих практик уже устоявшаяся вещь в индустрии разработки ПО."
  },
  {
    "objectID": "1.entry.html#какой-план",
    "href": "1.entry.html#какой-план",
    "title": "Engineering practices in ML",
    "section": "Какой план?",
    "text": "Какой план?\n\nПонимание проблематики\nОрганизация процесса в команде\nИнструменты воспроизводимости исследований\n\n\nСобственно каков план на этот курс. Сегодня мы будем разбираться с проблематикой. На следующих занятиях будем затрагивать темы организации процесса в команде и инструменты необходимые для воспроизводимости исследований."
  },
  {
    "objectID": "1.entry.html#i-dontt-like-jupyter-notebooks",
    "href": "1.entry.html#i-dontt-like-jupyter-notebooks",
    "title": "Engineering practices in ML",
    "section": "I dont’t like jupyter notebooks1",
    "text": "I dont’t like jupyter notebooks1\n\nскрытые состояния, которые можно забыть или испортить\nвозможность запускать код в произвольном порядке\nнет поддержки линтеров, форматеров и автокомплита\nне поддерживают модульность и переиспользование кода\nпоощрают написание не тестируемого кода\nприходиться писать смешанный код\nне удобны для версионировании в гите\nнет requirements\n\nРезюме: jupyter notebooks поощряют вредные привычки и плохие процессы, а также препятствуют появлению хороших привычек."
  },
  {
    "objectID": "9.data_version.html#почему-это-важно",
    "href": "9.data_version.html#почему-это-важно",
    "title": "Data versioning",
    "section": "Почему это важно?",
    "text": "Почему это важно?\n\n\nВоспроизводимость исследований.\n\n\n\n\nРаспространение данных в команде.\n\n\n\n\nВерсионирование исследований.\n\n\nВспомним правила проведения воспроизводимых исследований, мы уже рассмотрели как работать с вашими зависимостями и какие инструменты есть для управления зависимостями, также рассмотрели методологии управления системой управления версиями кода и как организовать в ней процесс работы в команде. Теперь мы перейдем версионированию и распространению ваших данных.\nПерове и самое важное, что дает версионирование данных, это воспроизводимость исследований. Теперь кроме кода вы можете указать на каких данных было проведено то или иное исследование. Как я уже говорил в правилах воспроизводимых исследований:\n\n«Цель воспроизводимых исследований - привязать конкретные инструкции к анализу данных и экспериментальным данным, чтобы исследование можно было воссоздать, лучше понять и проверить».\n\nИспользуя контроль версий данных, мы по привязываем данные на которых проводился эксперимент к вашему коду, и теперь другие исследователи могут получить те же самые данные, которые использовали вы в своей работе.\nВо-вторых мы решаем проблему того как распространить данные в команде. Когда в команде работает несколько человек, то одновременно может проверяться несколько гипотез. В ходе их проверки могут быть созданы новые версии данных и надо как-то распространить эти данные в команде. Всегда есть доисторический способ выгрузить на флешку и таким образом дать возможность каждому члену команды их к себе скачать. Но это долго и больно, и никак не укладывается в процесс git workflow. Когда мы начинаем версионировать данные, то что бы получить новые версии данных необходимо выполнить одну команду, это сильно упрощает обмен данными.\nДа конечно стоит сказать, что данные можно получить на основе кода, используя исходные данные, но тут есть некоторые ограничения, во-первых, не всегда есть возможность повторно выполнить исследование из-за недостатка ресурсов (это может быть просто долго), во-вторых, над данными могли быть проведены какие-то внешние манипуляции, не описанные в коде (да это плохо, но пока у нас нет никаких ограничений и проверок на это). Версионирование данных позволит решить эти проблемы.\nНу и последний поинт, почему это важно - вы можете начать версионировать исследования. Кроме того, чтобы версионировать входные данные необходимые для эксперимента, вы можете так же версионировать и результаты вашего эксперимента, это же тоже данные - графики, таблицы, метрики, модели. то есть после того как вы внедряете версионирвоание данных, вы можете говорить о полноценном версионировании исследований. То есть к вашему коду привязаны не только входные данные, но и результаты и таким образом вы можете отследить как те или иные исследования и преобразования данных повлияли на качество решаемой задачи."
  },
  {
    "objectID": "9.data_version.html#требования-к-dvc",
    "href": "9.data_version.html#требования-к-dvc",
    "title": "Data versioning",
    "section": "Требования к DVC",
    "text": "Требования к DVC\n\n\nНезависимость от формата данных.\n\n\n\n\nНезависимость от хранилища.\n\n\n\n\nSelf-hosted версия.\n\n\n\n\nWorkflow похож на git.\n\n\n\n\nНет зависимости на уровне кода.\n\n\n\n\nАнализ различий.\n\n\n\n\nПереиспользование в прод.\n\n\nВроде как разобрались с бенефитами контроля версий данных, теперь давайте поймем, что нам нужно от инструмента контроля версий данных, какие требования мы модем ему предъявить.\n\nНезависимость от формата данных. В проектах могут быть различные формата данных, от таблиц excel или csv, до музыки, видео и бинарных данных с датчиков. Наша система контроля версий данных, должна не зависеть от формата данных и уметь работать с любым из них.\nМы уже разобрались, что git для хранения не подходит, а данные должны где-то храниться, поэтому у инструмента контроля версий должно быть какое-то хранилище. Хотелось бы иметь возможность подключения различных хранилищ. Так как в проектах могут быть разные объемы и форматы данных, и под каждый вариант хотелось бы выбирать и разворачивать хранилище в котором такие данные оптимально хранить.\nЭтот момент скорее важен для работы в компании, так как хранить данные на чужих серверах мало какая компания захочет, так как это потенциальная уязвимость и потеря данных. Поэтому необходимо иметь возможность развернуть систему контроля версий и ее хранилище в контуре компании.\nWorkflow похож на git. Поскольку мы ведем контроль версий кода в git, то хотелось, что бы версионирование данных интегрировалось или накладывалось на существующий workflow. Это сильно упростит работу исследователя. Еще было бы хорошо иметь git-like интерфейс.\nНет зависимости на уровне кода. То есть нам не нужно использовать какие-то структуры данных, классы датасетов и моделей предлагаемые инструментом. В результате мы можем запускать и использовать наш код вне зависимости от состояния и доступности dvc. В идеале - код работает даже без установленного инструмента.\nАнализ различий, хотелось, что бы инструмент версионирования данных позволял анализировать изменения в данных, Хотя бы показывать такой же diff как git. А еще лучше наглядно бы показывал, как изменились графики, таблицы метрик и так далее. По сути давал представление о том, что стало лучше, а что хуже.\nВозможность переиcпользовать его в проде для поставки моделей. То есть дать возможность выгрузить нужную версию модели. Так как в любом случае перед нами стоит задача по поставке наших результатов в продакшен."
  },
  {
    "objectID": "9.data_version.html#versioning-problem",
    "href": "9.data_version.html#versioning-problem",
    "title": "Data versioning",
    "section": "Versioning problem",
    "text": "Versioning problem\n\n\nКоманды по науке о данных сталкиваются с вопросами управления данными, связанными с версиями моделей данных и машинного обучения. Как мы вместе отслеживаем изменения в данных, исходном коде и моделях машинного обучения? Как лучше организовать и хранить варианты этих файлов и каталогов?\nДругая проблема в этой области связана с учетом: возможность идентифицировать прошлые входные данные и процессы, чтобы понять их результаты, для обмена знаниями или для отладки.\nПоявляется экспоненциальная сложность проектов по науке о данных."
  },
  {
    "objectID": "9.data_version.html#solve-problem",
    "href": "9.data_version.html#solve-problem",
    "title": "Data versioning",
    "section": "Solve problem",
    "text": "Solve problem\n\n\nУправление версиями данных (DVC) позволяет сохранять версии ваших данных и моделей в коммитах Git, сохраняя их локально или в облачном хранилище. Он также предоставляет механизм для переключения между этим различным содержимым данных. В результате получается единая история для данных, кода и моделей машинного обучения, которую вы можете просматривать — настоящий журнал вашей работы.\nDVC обеспечивает управление версиями данных посредством кодификации. Вы создаете простые метафайлы один раз, описывая, какие наборы данных, артефакты машинного обучения и т. д. нужно отслеживать. Эти метаданные можно поместить в Git вместо больших файлов. Теперь вы можете использовать DVC для создания моментальных снимков данных, восстановления предыдущих версий, воспроизведения экспериментов, записи меняющихся показателей и многого другого."
  },
  {
    "objectID": "9.data_version.html#архитектура-dvc",
    "href": "9.data_version.html#архитектура-dvc",
    "title": "Data versioning",
    "section": "Архитектура DVC",
    "text": "Архитектура DVC"
  },
  {
    "objectID": "13.dvc_problems.html#python-problems",
    "href": "13.dvc_problems.html#python-problems",
    "title": "Any problems?",
    "section": "Python problems",
    "text": "Python problems\nСоздадим два множества\n\na = {\"a\", \"b\", \"c\", \"d\"}\nb = {\"d\", \"a\", \"c\", \"b\"}\n\n\nPython считает их одинаковыми\n\na==b\n\nTrue\n\n\n\n\nА pickle - нет\n\nimport pickle\npickle.dumps(a)==pickle.dumps(b)\n\nTrue"
  },
  {
    "objectID": "13.dvc_problems.html#precision-representation",
    "href": "13.dvc_problems.html#precision-representation",
    "title": "Any problems?",
    "section": "Precision representation",
    "text": "Precision representation\nПредставление \\(\\frac{1}{10}\\)\n\nformat(0.1, \".17g\")\n\n'0.10000000000000001'\n\n\n\n(0.1).as_integer_ratio()\n\n(3602879701896397, 36028797018963968)\n\n\n\nПредставление \\(\\frac{2}{10}\\)\n\nformat(0.2, \".17g\")\n\n'0.20000000000000001'\n\n\n\n(0.2).as_integer_ratio()\n\n(3602879701896397, 18014398509481984)\n\n\n\n\nПредставление \\(\\frac{3}{10}\\)\n\nformat(0.3, \".17g\")\n\n'0.29999999999999999'\n\n\n\n(0.3).as_integer_ratio()\n\n(5404319552844595, 18014398509481984)"
  },
  {
    "objectID": "13.dvc_problems.html#numpy-precision-problems",
    "href": "13.dvc_problems.html#numpy-precision-problems",
    "title": "Any problems?",
    "section": "Numpy precision problems",
    "text": "Numpy precision problems\n\nРазная точность на 32 битных и 64 битных системах\nРазная точность на windows и других системах\nРазные стандартные типы в windows и других системах"
  },
  {
    "objectID": "13.dvc_problems.html#в-чем-проблема-то",
    "href": "13.dvc_problems.html#в-чем-проблема-то",
    "title": "Any problems?",
    "section": "В чем проблема то?",
    "text": "В чем проблема то?\n\n\nНа windows у нас тип переполняется \n\nНа linux стандартный тип не переполняется"
  },
  {
    "objectID": "13.dvc_problems.html#plotly-problems",
    "href": "13.dvc_problems.html#plotly-problems",
    "title": "Any problems?",
    "section": "Plotly problems",
    "text": "Plotly problems\nСоздадим два графика\n\nimport plotly.graph_objects as go\n\nfig1 = go.Figure(go.Scatter(x=[1,2,3], y=[3,2,1]))\nfig2 = go.Figure(go.Scatter(x=[1,2,3], y=[3,2,1]))\n\nPython считает их одинаковыми\n\nfig1==fig2\n\nTrue\n\n\nНо при сохранении в html - разные\n\nfig1.to_html()==fig2.to_html()\n\nFalse\n\n\nHot fix\n\nfig1.to_html(div_id=\"same\")==fig2.to_html(div_id=\"same\")\n\nTrue"
  },
  {
    "objectID": "13.dvc_problems.html#sklearn-problems",
    "href": "13.dvc_problems.html#sklearn-problems",
    "title": "Any problems?",
    "section": "Sklearn problems",
    "text": "Sklearn problems\nСоздадим две модели и зафиксируем random state\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.datasets import fetch_20newsgroups\n\ndata = fetch_20newsgroups()\n\nm1 = TfidfVectorizer(stop_words=[\"the\"]).fit([\"Hello world\", \"Hello, the\"])\nm2 = TfidfVectorizer(stop_words=[\"the\"]).fit([\"Hello world\", \"Hello, the\"])\n\nОни не равны\n\npickle.dumps(m1)==pickle.dumps(m2)\n\nFalse\n\n\nРазличия в них\n\nm1._stop_words_id, m2._stop_words_id\n\n(139989962113792, 139989972364032)"
  },
  {
    "objectID": "13.dvc_problems.html#sklearn-problems-1",
    "href": "13.dvc_problems.html#sklearn-problems-1",
    "title": "Any problems?",
    "section": "Sklearn problems",
    "text": "Sklearn problems\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom utils import points\n\nfor l in range(10):\n    kmeans = KMeans(n_clusters=30, n_init=1, random_state=42)\n    kmeans.fit(points)\n\n    centroids = kmeans.cluster_centers_\n    print('{0:.20f}'.format(kmeans.inertia_))\n\n1008436173.14048218727111816406\n1008436173.14048218727111816406\n1008436173.14048218727111816406\n1008436173.14048218727111816406\n1008436173.14048218727111816406\n1008436173.14048218727111816406\n1008436173.14048218727111816406\n1008436173.14048218727111816406\n1008436173.14048218727111816406\n1008436173.14048218727111816406"
  },
  {
    "objectID": "13.dvc_problems.html#library-problems",
    "href": "13.dvc_problems.html#library-problems",
    "title": "Any problems?",
    "section": "Library problems",
    "text": "Library problems\n\nLogistic Regression\nRandom Forest\nTF-IDF\nAgglomerative Clustering\nt-SNE\nUmap\nAll algorithms"
  },
  {
    "objectID": "13.dvc_problems.html#dl-frameworks",
    "href": "13.dvc_problems.html#dl-frameworks",
    "title": "Any problems?",
    "section": "DL frameworks",
    "text": "DL frameworks\n\npytorch\ntensorflow\njax"
  },
  {
    "objectID": "19.airflow.html#section",
    "href": "19.airflow.html#section",
    "title": "Airflow",
    "section": "",
    "text": "Apache AirFlow — это open-source инструмент, который позволяет разрабатывать, планировать и осуществлять мониторинг сложных рабочих процессов. Главной особенностью является то, что для описания процессов используется язык программирования Python."
  },
  {
    "objectID": "19.airflow.html#dag",
    "href": "19.airflow.html#dag",
    "title": "Airflow",
    "section": "DAG",
    "text": "DAG\n\n\nПроцессы обработки данных, или пайплайны, в Airflow описываются при помощи DAG (Directed Acyclic Graph). Это смысловое объединение задач, которые необходимо выполнить в строго определенной последовательности согласно указанному расписанию. Визуально DAG выглядит как направленный ациклический граф, то есть граф, не имеющий циклических зависимостей.\nВ качестве узлов DAG выступают задачи (Task). Это непосредственно операции, применяемые к данным, например: загрузка данных из различных источников, их агрегирование, индексирование, очистка от дубликатов, сохранение полученных результатов и прочие ETL-процессы. На уровне кода задачи могут представлять собой Python-функции или Bash-скрипты.\nЗа реализацию задач чаще всего отвечают операторы (Operator). Если задачи описывают, какие действия выполнять с данными, то операторы — как эти действия выполнять. По сути, это шаблон для выполнения задач.\nОсобую группу операторов составляют сенсоры (Sensor), которые позволяют прописывать реакцию на определенное событие. В качестве триггера может выступать наступление конкретного времени, получение некоторого файла или строки с данными, другой DAG/Task и так далее."
  },
  {
    "objectID": "19.airflow.html#dag-operators",
    "href": "19.airflow.html#dag-operators",
    "title": "Airflow",
    "section": "DAG Operators",
    "text": "DAG Operators\n\nPythonOperator\nBranchPythonOperator\nBashOperator\nSimpleHttpOperator\nMySqlOperator\nPostgresOperator\nS3FileTransformOperator\nDockerOperator\nKubernetesPodOperator\nSqlSensor\nSlackAPIOperator\nEmailOperator\nDummyOperator\n\n\nВ AirFlow богатый выбор встроенных операторов. Кроме этого, доступно множество специальных операторов — путем установки пакетов поставщиков, поддерживаемых сообществом. Также возможно добавление пользовательских операторов — за счет расширения базового класса BaseOperator. Когда в проекте возникает часто используемый код, построенный на стандартных операторах, рекомендуется его преобразование в собственный оператор.\nПримеры операторов приведены ниже.\nОператор Назначение PythonOperator Исполнение Python-кода BranchPythonOperator Запуск задач в зависимости от выполнения заданного условия BashOperator Запуск Bash-скриптов SimpleHttpOperator Отправка HTTP-запросов MySqlOperator Отправка запросов к базе данных MySQL PostgresOperator Отправка запросов к базе данных PostgreSQL S3FileTransformOperator Загрузка данных из S3 во временную директорию в локальной файловой системе, преобразование согласно указанному сценарию и сохранение результатов обработки в S3 DockerOperator Запуск Docker-контейнера под выполнение задачи KubernetesPodOperator Создание отдельного Pod под выполнение задачи. Используется совместно с K8s SqlSensor Проверка выполнения SQL-запроса SlackAPIOperator Отправка сообщений в Slack EmailOperator Отправка электронных писем DummyOperator «Пустой» оператор, который можно использовать для группировки задач"
  },
  {
    "objectID": "19.airflow.html#airflow-architecture",
    "href": "19.airflow.html#airflow-architecture",
    "title": "Airflow",
    "section": "Airflow architecture",
    "text": "Airflow architecture\n\n\nОснову архитектуры AirFlow составляют следующие компоненты:\nWeb Server — отвечает за пользовательский интерфейс, где предоставляется возможность настраивать DAG и их расписание, отслеживать статус их выполнения и так далее. Metadata DB (база метаданных) — собственный репозиторий метаданных на базе библиотеки SqlAlchemy для хранения глобальных переменных, настроек соединений с источниками данных, статусов выполнения Task Instance, DAG Run и так далее. Требует установки совместимой с SqlAlchemy базы данных, например, MySQL или PostgreSQL.\nScheduler (планировщик) — служба, отвечающая за планирование в Airflow. Отслеживая все созданные Task и DAG, планировщик инициализирует Task Instance — по мере выполнения необходимых для их запуска условий. По умолчанию раз в минуту планировщик анализирует результаты парсинга DAG и проверяет, нет ли задач, готовых к запуску. Для выполнения активных задач планировщик использует указанный в настройках Executor. Для определенных версий БД (PostgreSQL 9.6+ и MySQL 8+) поддерживается одновременная работа нескольких планировщиков — для максимальной отказоустойчивости.\nWorker (рабочий) — отдельный процесс, в котором выполняются задачи. Размещение Worker — локально или на отдельной машине — определяется выбранным типом Executor.\nExecutor (исполнитель) — механизм, с помощью которого запускаются экземпляры задач. Работает в связке с планировщиком в рамках одного процесса. Поддерживаемые типы исполнителей приведены ниже."
  },
  {
    "objectID": "19.airflow.html#proscons",
    "href": "19.airflow.html#proscons",
    "title": "Airflow",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\nНе зависит от языка\nВозможность изоляции\nРаспределенные вычесления\nМониторинг экспериментов\nПростая параметризация и разделение на модули\n\n\nCons\n\nDAG пишутся вами на python\nНет ограничений на ресурсы\nНе умеет работать с git\nНе удобна для командной работы"
  },
  {
    "objectID": "7.code_styles.html#почему-это-важно",
    "href": "7.code_styles.html#почему-это-важно",
    "title": "CodeStyle",
    "section": "Почему это важно?",
    "text": "Почему это важно?\n\n\nНачинающий программист \n\nОпытный программист \n\n\n\nПотому что датасаентисты работают с кодом. В процессе работы приходиться писать и дебажить код, изучать библиотеки, примеры использования инструментов, а также читать и разбирать код коллег. Это все может занимать значительную долю времени. Для экономии своего времени и времени коллег (ну кому охота полдня читать код?) приходиться договариваться о стиле и структуре кода и приводить код к одному виду. Это ключевой фактор, который характеризует читабельность кода.\nВ процессе сбора материала для презентации я набрел на исследования, в которых датчики следили за движениями глаз начинающего и опытного программистов. Code style как стандарт разработки\nОбратите внимание, чем занимается опытный программист. Он выделяет блоки кода, декомпозирует их и читает поблочно, выделяя ключевые части и анализируя их работу. Начинающий же мечется построчно, просто пытается понять, что тут вообще происходит.\nВидео довольно старое, от 2012 года, и запись была направлена на исследование процессов мозга программиста. Чуть подробнее можно почитать тут."
  },
  {
    "objectID": "7.code_styles.html#основная-мотивация-появления-codestyle",
    "href": "7.code_styles.html#основная-мотивация-появления-codestyle",
    "title": "CodeStyle",
    "section": "Основная мотивация появления CodeStyle",
    "text": "Основная мотивация появления CodeStyle\n\nУскорить понимания и ревью кода.\nУменьшить стилистическое разнообразие кода.\nУменьшить сложность кода.\nЗапретить использование плохих практик."
  },
  {
    "objectID": "7.code_styles.html#читаемость-кода",
    "href": "7.code_styles.html#читаемость-кода",
    "title": "CodeStyle",
    "section": "Читаемость кода",
    "text": "Читаемость кода\nЕсть 2 способа ускорить чтение и понимания кода:\n\nПостоянно наращивать базу знаний разработчиков по тому, как может выглядеть код.\nПривести весь код к одному стандарту, задать тот самый code style\n\n\nКонечно стоит идти по второму пути, поскольку это снизит нагрузку на разработчика, выделили следующие поинты, которые даст повышение читабельности кода:\n\nЕсли вы обеспечили историческое написание понятного кода, то, сколько бы не приходило и не уходило разработчиков, вы всегда имеете равное качество кода, что позволяет проекту динамично расти вне зависимости от его объема.\nЕсли ваш код написан понятно, новый специалист быстро разберется и начнет приносить пользу. Есть такое понятие, как точка самоокупаемости сотрудника. Это точка, с которой сотрудник начинает приносить только пользу. А если код написан понятно, новым сотрудникам научится читать ваш код, без необходимости детально разбираться. И чем быстрее он это сделает, тем быстрее перестанет задавать тонны вопросов остальным специалистам, отнимая у них время. А время специалистов, которые прошли точку самоокупаемости, значительно дороже для команды и компании с точки зрения потенциальной ценности приносимой продукту.\nНе нужно будет постоянно спотыкаться о чью-то оригинальность и специфическое оформление.\nВаш мозг меньше сопротивляется, т.к. не нужно вникать в чужую стилистику. Ментальных ресурсов на чтение понятного кода нужно намного меньше.\nОчень скоро, после прибытия в новую компанию, программист начнет делиться впечатлениями с бывшими коллегами и друзьями. И он либо скажет, что тут все круто, либо выделит негативные моменты в работе с кодом.\nЗадача программиста лежит на более низком уровне, чем будущее всей компании, но важно донести понимание, что понятность и читаемость кода сейчас влияет на динамику дальнейшей разработки."
  },
  {
    "objectID": "7.code_styles.html#section",
    "href": "7.code_styles.html#section",
    "title": "CodeStyle",
    "section": "",
    "text": "Собственно, действительно можно собрать свой собственный набор стилей для написания кода. И для этого есть множество удобных инструментов, которые позволяют автоматизировать множество проверок стиля и дополнить их написанием своих собственных."
  },
  {
    "objectID": "7.code_styles.html#autoformatters",
    "href": "7.code_styles.html#autoformatters",
    "title": "CodeStyle",
    "section": "AutoFormatters",
    "text": "AutoFormatters\n\n\nНа сегодняшний день есть несколько популярных автоформаттеров для питона. Yapf, Autopep8 и Black. Какой выбирать решайте сами.\n\nYapf - google\nAutopep8 - Hideo Hattori\nBlack - Python Software Foundation\n\nВ качестве инструмента форматирования согласно PEP 8 используют autopep8 (есть и другие, но этот самый популярный). Он запускается через командную строку, поддерживает множество аргументов. Но ожидать, что ваш Python-код станет повсюду единообразным не стоит, поскольку PEP 8 не дает строгих рекомендаций. Но уж лучше с ним, чем без него.\nYAPF является не официальным продуктом от Google, но кодом, который, так случилось, владеет Google. YAPF основан на таком инструменте форматирования кода Си-подобных языков, как clang-format. Он берет код и форматирует его в соответствии с заданными настройками, причем отформатирует код даже в случае, если он не нарушает PEP 8. Например, вы можете задать необходимую компоновку словарей, а все остальное сделать как в Black. Эти настройки сохраняются и передаются другим программистам.\nBlack является наиболее строгим инструментом форматирования. С одной стороны, это помогает придерживаться одного стиля форматирования кода, с другой, некоторые правила могут вызывать внутреннее отторжение. Кроме того, в отличие от YAPF, он не поддерживает множество настроек. По факту Black имеет две перенастраиваемые опции:\n\nизменение допустимой длины строки (по умолчанию стоит 88),\nразрешение использования одинарных кавычек (по умолчанию разрешаются только двойные).\nПоддержка isort\n\nКак привести код в порядок мы обсудили, теперь же поговорим о том, как отслеживать качество кода и находить отклонения от code style."
  },
  {
    "objectID": "7.code_styles.html#linters",
    "href": "7.code_styles.html#linters",
    "title": "CodeStyle",
    "section": "Linters",
    "text": "Linters\n\n\n\nLinter\nCategory\nDescription\n\n\n\n\nPylint\nLogical & Stylistic\nChecks for errors, tries to enforce a coding standard, looks for code smells\n\n\nPyFlakes\nLogical\nAnalyzes programs and detects various errors\n\n\nPycodestyle\nStylistic\nChecks against some of the style conventions in PEP8\n\n\nPydocstyle\nStylistic\nChecks compliance with Python docstring conventions\n\n\nBandit\nLogical\nAnalyzes code to find common security issues\n\n\nMyPy\nLogical\nChecks for optionally-enforced static types\n\n\n\n\nТеперь давайте обсудим их, зачем же их так много.\nОни разбиваются на две группы: Logical и Stylistic.\nПервая позволяет находить, паттерны опасного кода и потенциальное неоднозначное поведение. Стилистический линтеринг предназначен для проверки стилистических договоренностей.\n\nPylint сложный и мощный анализатор. Который умеет многое из коробки.\nPyFlake базовый flake linter который анализирует программу и ищет потенциальные ошибки\nPycodestyle - проверка на соглашение по pep8\nPydocstyle - проверка docstrings по соглашению\nBandit - проверка на безопасность кода, уязвимости в используемых библиотеках и прочее\nMyPy - проверка типов на основе аннотаций\n\nЭти инструменты направлены на проверку соглашений по написанию кода, которые были заключены, на поиск плохих практик, ошибок."
  },
  {
    "objectID": "7.code_styles.html#flake8-plugins",
    "href": "7.code_styles.html#flake8-plugins",
    "title": "CodeStyle",
    "section": "Flake8 plugins",
    "text": "Flake8 plugins\n\n\nХочется отметить среди линтеров flake8. Это по-настоящему мощный инструмент который объединяет в себе все другие за счет расширений и плагинов. Вы видите наиболее популярную часть плагинов и расширений для flake8.\nТо есть за счет компоновки плагинов вы можете настроить все необходимые вам инструменты в одной библиотеке и проверять все одной командой, при этом за вас соберут все дублирующиеся ошибки и варнинги и выведут в приоритетном порядке.\nЕсли сравнивать с pylint это инструмент, который включает в себя сразу большое количество проверок, но не дает гибкого интерфейса для настройки и расширения проверок, то flake8 его во много выигрывает за счет огромной экосистемы плагинов, которые увеличивают его мощь и строгость.\nОдин из принципов python dzen звучит что complex is better than complicated. Это как раз и применимо в этом сравнении, составное лучше сложного."
  },
  {
    "objectID": "7.code_styles.html#анализаторы-кода---radon",
    "href": "7.code_styles.html#анализаторы-кода---radon",
    "title": "CodeStyle",
    "section": "Анализаторы кода - Radon",
    "text": "Анализаторы кода - Radon\n\nЦикломатическая сложность\nМетрика Холстеда\nИндекс поддерживаемости кода\n\n\nА как вообще команде проверить что договорённости, которые она заключила помогают в достижении изначальных целей? Как проверить читабельность кода?\nДля этого существуют анализаторы кода, например, Radon. Он позволяет посчитать некоторые метрики:\n\nЦикломатическая сложность - Чем больше в коде переходов (if-else), циклов, генераторов, обработчиков исключений и логических операторов — тем больше у программы вариантов исполнения и тем сложнее удержать в голове различные состояния системы. Метрика, которая измеряет сложность кода, опираясь на количество этих операций, называется цикломатической сложностью программы. При подсчете ее с помощью радона вы получите список файлов, классов, методов и функций в вашем проекте и их индекс сложности, от очень простого до очень сложного. Индекс укажет на перегруженные логикой места, которые можно разбить на куски помельче, упростить или переписать (если есть такая возможность — алгоритм может быть очень сложным сам по себе и попытки его разбить на куски могут только ухудшить понимабельность кода).\nМетрика Холстеда - Тут считается количество уникальных операторов и операндов в коде и их общее количество. Полученные значения подставляются в формулы и получается набор чисел, который описывает сложность программы и количество усилий, предположительно затраченное на написание и понимание кода.\nИндекс поддерживаемости кода - Этот индекс говорит нам о том, насколько сложно будет поддерживать или редактировать кусок программы. Этот параметр рассчитывается на основе чисел, полученных из метрик, посчитанных выше."
  },
  {
    "objectID": "7.code_styles.html#mypy-проверка-типов",
    "href": "7.code_styles.html#mypy-проверка-типов",
    "title": "CodeStyle",
    "section": "Mypy проверка типов",
    "text": "Mypy проверка типов\n\n\nFrom Python…\ndef fib(n):\n    a, b = 0, 1\n    while a < n:\n        yield a\n        a, b = b, a+b\n\n…to statically typed Python\ndef fib(n: int) -> Iterator[int]:\n    a, b = 0, 1\n    while a < n:\n        yield a\n        a, b = b, a+b\n\n\n\nКак вы знаете в питоне есть аннотации типов. Mypy использует для статической проверки типов и собственно проверяет везде ли и корректно ли они указаны.\nMypy имеет мощную современную систему типов с такими функциями, как двунаправленный вывод типов, обобщения, вызываемые типы, абстрактные базовые классы, множественное наследование и типы кортежей.\nЗачем это DS? У них же один dataframe или numpy array. На самом деле такой подход увеличивает читаемость кода, сразу понятно какой dataframe к вам пришел, какие стобцы в нем есть и каких типов. Есть разные библиотеки для этого, например, pandera, но об этом мы поговорим позже.\nОсновная цель добавления статической типизации или аннотирования типов заключается в повышении читаемости, вам и вашим коллегам не придётся догадываться что и какого типа нужно передать той или иной функции. Да и такой подход позволит избавится от части ошибок, которые вы можете допустить в коде."
  },
  {
    "objectID": "7.code_styles.html#pre-commit-автоматизация",
    "href": "7.code_styles.html#pre-commit-автоматизация",
    "title": "CodeStyle",
    "section": "Pre-commit автоматизация",
    "text": "Pre-commit автоматизация\n.pre-commit-config.yaml\ndefault_stages: [commit, push]\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n    -   id: check-ast\n    -   id: check-merge-conflict\n-   repo: https://github.com/python/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n-   repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n    -   id: isort\n        name: isort (python)\n-   repo: https://github.com/PyCQA/flake8\n    rev: 4.0.1\n    hooks:\n    -   id: flake8\n        additional_dependencies: [\n            yastyleguide==0.1.0\n        ]\n\nИтак, теперь мы знаем, как использовать линтеры и автоформатеры, и поняли, насколько они полезны! Следующий шаг — начать применять их в наших проектах. Это можно сделать с помощью хуков перед комитом. Хуки перед комитом позволяют нам проверять наш код на наличие проблем со стилем и форматированием каждый раз, когда происходит коммит изменения, тем самым обеспечивая сохранение единого стиля на протяжении всего проекта.\nТо есть в репозиторий не попадает плохого кода. Мы не даем человеку его писать и комитить, а уже потом когда-нибудь рефакторить. Мы делаем условие что в репозиторий код попадает как минимум в соответствии ос стилем и уровнем качества удовлетворяющим линтеры.\nЭто также снимает с самого разработчика ответственность за запуск различных линтеров и форматеров, облегчает рутинную работу так сказать."
  },
  {
    "objectID": "7.code_styles.html#выводы",
    "href": "7.code_styles.html#выводы",
    "title": "CodeStyle",
    "section": "Выводы",
    "text": "Выводы\n\nЧитаемость и качество важны.\nЗа качеством нужно следить.\nНужно автоматизировать рутинные операции."
  },
  {
    "objectID": "7.code_styles.html#как-работает-python",
    "href": "7.code_styles.html#как-работает-python",
    "title": "CodeStyle",
    "section": "Как работает python?",
    "text": "Как работает python?\n\n\nPython не преобразует свой код в машинный код, который может понять аппаратное обеспечение. Он фактически преобразует его в то, что называется байт-кодом. Таким образом, внутри Python компиляция происходит, но не на машинном языке. Он транслирует в байтовый код (.pyc или .pyo), и этот байтовый код не может быть понят процессором. Поэтому нам нужен интерпретатор, называемый виртуальной машиной python, для выполнения байт-кодов.\nИсходный код Python выполняет следующие действия для создания исполняемого кода:\n\nШаг 1: Транслятор Python читает исходный код или инструкцию Python. Затем проверяется правильность форматирования инструкции, т. е. проверяется синтаксис каждой строки. Если он обнаруживает ошибку, он немедленно останавливает перевод и показывает сообщение об ошибке.\nШаг 2: Если ошибки нет, т. е. если инструкция или исходный код Python правильно отформатированы, то компилятор переводит их в эквивалентную форму на промежуточном языке, называемом «Байт-код».\nШаг 3: Затем байт-код отправляется на виртуальную машину Python (PVM), которая является интерпретатором Python. PVM преобразует байт-код Python в машинно-исполняемый код. Если во время этой интерпретации возникает ошибка, то преобразование останавливается с сообщением об ошибке.\n\nДавайте повнимательнее разберемся с шагами 1 И 2, как именно питон проверяет синтаксис каждой строки и в какую структуру данных преобразует код для перевода его в byte code."
  },
  {
    "objectID": "7.code_styles.html#abstract-syntax-tree",
    "href": "7.code_styles.html#abstract-syntax-tree",
    "title": "CodeStyle",
    "section": "Abstract syntax tree",
    "text": "Abstract syntax tree\n\n\n\n\n\nАбстрактное синтаксическое дерево (AST) — это структура данных, используемая для рассуждения о грамматике языка программирования в контексте инструкций, представленных в исходном коде.\nТакой подход используется и в python для преобразования кода в байт-коды.\n\nПри наличии некоторого текстового исходного кода компилятор сначала размечает текст, чтобы идентифицировать ключевые слова языка программирования, переменные, литералы и т. д. Каждая лексема представляет собой «атом» инструкции.\nЗатем токены перестраиваются в AST, дерево, узлы которого являются «атомами» инструкций, и ограничивает отношения между атомами на основе грамматики языка программирования. Например, AST делает явным наличие вызова функции, соответствующих входных аргументов, инструкций, составляющих функцию, и т. д.\nЗатем компилятор может применить несколько оптимизаций к AST и в конечном итоге преобразовать его в двоичный код.\n\nСобственно, большинство анализаторов кода используют AST для анализа. В нем можно рассматривать взаимодействия нод с друг-другом, сравнивать это с паттернами, искать плохие практики и даже генерировать код в нужном стиле. В питоне есть специальный модуль ast для этого."
  },
  {
    "objectID": "7.code_styles.html#ast",
    "href": "7.code_styles.html#ast",
    "title": "CodeStyle",
    "section": "AST",
    "text": "AST\n\nimport ast\n\ncode = \"one_plus_two = 1 + 2\"\ntree = ast.parse(code)\nprint(ast.dump(tree, indent=4))\n\nModule(\n    body=[\n        Assign(\n            targets=[\n                Name(id='one_plus_two', ctx=Store())],\n            value=BinOp(\n                left=Constant(value=1),\n                op=Add(),\n                right=Constant(value=2)))],\n    type_ignores=[])\n\n\n\nТут чуть-чуть углубимся в сам питон. В нем есть модуль ast, который мы можем импортить и использовать.\nСначала это может быть неочевидно, но результат, сгенерированный на ast.dump()самом деле, представляет собой дерево:\n\nСлова, начинающиеся с заглавной буквы, являются узлами дерева.\nАтрибутами узлов являются либо ребра дерева, либо метаданные."
  },
  {
    "objectID": "7.code_styles.html#ast-1",
    "href": "7.code_styles.html#ast-1",
    "title": "CodeStyle",
    "section": "AST",
    "text": "AST\n\n\nДавайте переработаем вывод в диаграмму со следующими соглашениями:\n\nОдин прямоугольник для каждого узла, выделенный жирным шрифтом соответствующий тип узла.\nАтрибуты узлов, собирающие метаданные, отображаются синим цветом.\nДругие атрибуты узла аннотируются их типом.\nУзлы связаны на основе их атрибутов.\n\nИмея под рукой эту визуализацию, мы можем наблюдать несколько вещей.\n\nКорнем дерева является Module узел.\nУ Module есть атрибут body - список инструкций в модуле.\nНаш пример состоит из одной операции присваивания и, следовательно, Module.body содержит только один Assign узел.\nОперация присваивания имеет правую часть, определяющую операцию для выполнения, и левую часть, определяющую место назначения операции.\nСправа у нас бинарная операция, которая содержит два операнда(константы) и операцию - сложение.\nСлева же имя переменной и ссылка ctx на переменную в программе.\n\nСобственно осознав эту структуру данных мы можем по ней передвигаться."
  },
  {
    "objectID": "7.code_styles.html#ast-visit-functions",
    "href": "7.code_styles.html#ast-visit-functions",
    "title": "CodeStyle",
    "section": "AST visit functions",
    "text": "AST visit functions\n\nimport ast\n\ncode = \"one_plus_two = 1 + 2\"\ntree = ast.parse(code)\nfor node in ast.walk(tree):\n        print(node.__class__.__name__)\n\nModule\nAssign\nName\nBinOp\nStore\nConstant\nAdd\nConstant\n\n\n\nfor name, value in ast.iter_fields(tree):\n        print(name, value)\n\nbody [<ast.Assign object at 0x7fb176dc4c10>]\ntype_ignores []\n\n\n\nfor node in ast.iter_child_nodes(tree):\n        print(node.__class__.__name__)\n\nAssign\n\n\n\nДавайте рассмотрим функции для итерирования по дереву. Например функция walk позволяет совершать обход в глубину ast. А функции iter_fields, iter_child_nodes просмотреть поля ноды или просмотреть ее потомков, они не рекурсивны и применяются только к одному узлу дерева.\nС помощью них можно анализировать ast дерево, но это не очень удобно, поэтому предлагаю взглянуть на NodeVisitor."
  },
  {
    "objectID": "7.code_styles.html#nodevisitor",
    "href": "7.code_styles.html#nodevisitor",
    "title": "CodeStyle",
    "section": "NodeVisitor",
    "text": "NodeVisitor\n\nimport ast\n\nclass BinOpVisitor(ast.NodeVisitor):\n\n    def visit_BinOp(self, node):\n        print(f\"found BinOp at line: {node.lineno}\")\n        self.generic_visit(node)\n\ncode = \"one_plus_two = 1 + 2\"\ntree = ast.parse(code)\nBinOpVisitor().visit(tree)\n\nfound BinOp at line: 1\n\n\n\nЭто инструмент для рекурсивного спуска по ast. При этом использует generic_visit, то есть смотрит есть ли у него функции visit_<NODE_NAME> и если есть вызывает ее. Соответственно вам нужно определить такие функция для определенных nodes и вы сможете проверять различные конструкции в коде использую такой класс."
  },
  {
    "objectID": "7.code_styles.html#а-что-если-написать-свой-линтер",
    "href": "7.code_styles.html#а-что-если-написать-свой-линтер",
    "title": "CodeStyle",
    "section": "А что если написать свой линтер?",
    "text": "А что если написать свой линтер?\n\nfrom typing import Any, Generator, Tuple\n\n__version__ = \"0.1.0\"\nERROR = Tuple[int, int, str, Any]\nERROR_GENERATOR = Generator[ERROR, None, None]\n\nclass NoConstantPlugin:\n    name = __name__\n    version = __version__\n\n    def __init__(self, tree: ast.AST):\n        self._tree = tree\n\n    def run(self) -> ERROR_GENERATOR:\n        visitor = Visitor()\n        visitor.visit(self._tree)\n\n        for line, col, msg in visitor.errors:\n            yield line, col, msg, type(self)\n\n\nЭто сделать достаточно легко, как я говорил ранее flake8 имеет огромное количество плагинов и написать их не составляет труда. Давайте напишем плагин, который будет запрещать использовать константы отличные от 0, 1, 10 и 100.\nДля этого вам нужно создать класс, в конструктор которого будет передаваться abstract syntax tree. А также он будет иметь метод-генератор, который будет выдавать ошибки при их наличии.\nОсталось только разобраться с классом visitor и зарегистрировать наш плагин."
  },
  {
    "objectID": "7.code_styles.html#plugin-visitor",
    "href": "7.code_styles.html#plugin-visitor",
    "title": "CodeStyle",
    "section": "Plugin visitor",
    "text": "Plugin visitor\n\nimport ast\n\nclass Visitor(ast.NodeVisitor):\n    errors: list[tuple[int, int, str]] = []\n   \n    def visit_Constant(self, node: ast.AST):\n        if node.value not in (0, 1, 10, 100):\n            self.errors.append((node.lineno, node.col_offset, \"NAC100 Not allowed CONSTANT in code\"))\n        self.generic_visit(node)\n\nТеперь запустим\n\ncode = \"one_plus_two = 1 + 2\"\ntree = ast.parse(code)\nnext(NoConstantPlugin(tree).run())\n\n(1, 19, 'NAC100 Not allowed CONSTANT in code', __main__.NoConstantPlugin)\n\n\n\nДля класса visitor мы можем воспользоваться ast.NodeVisitor и написать функцию для посещения node Constant. В ней сделаем проверку на значение константы, если оно отлично, то добавляем в список ошибку, если нет, то все верно и мы продолжаем итерироваться по ast.\nТеперь нужно сделать так что бы наш код мог использоваться flake8."
  },
  {
    "objectID": "7.code_styles.html#добавляем-плагин-flake8",
    "href": "7.code_styles.html#добавляем-плагин-flake8",
    "title": "CodeStyle",
    "section": "Добавляем плагин Flake8",
    "text": "Добавляем плагин Flake8\nsetup.py\nflake8_entry_point = # ...\n\nsetuptools.setup(\n    # snip ...\n    entry_points={\n        flake8_entry_point: [\n            'NAC = flake8_example:NoConstantPlugin',\n        ],\n    },\n    # snip ...\n)\npyproject.toml\n[tool.poetry.plugins.\"flake8.extension\"]\nNAC = \"flake8_example:NoConstantPlugin\"\n\nЧто бы его зарегистрировать надо добавить в информацию о проекте flake8 entry point, в котором указано где определен ваш класс плагина.\nНа слайде показано как это сделать если вы используете традиционный setup.py и как если вы используете pyptoject. После этого ваш пакет можно собрать, опубликовать и устанавливать вместе с flake8 и другими плагинами для анализа кода."
  },
  {
    "objectID": "15.workflow_manager.html#почему-это-важно",
    "href": "15.workflow_manager.html#почему-это-важно",
    "title": "Workflow managers",
    "section": "Почему это важно?",
    "text": "Почему это важно?\n\n\nФиксация последовательности исполнения.\n\n\n\n\nУпрощение воспроизведения результатов.\n\n\n\n\nРаспределение вычеслений.\n\n\nПервое и самое важное мы фиксируем последовательность действий для повторения экспериментов. С помощью таких иснтрументов мы описываем как, в какой последовательности, с какими параметрами надо выполнять все написанные скрипты.\nТак же мы получаем единый интерфейс для всего проекта, то есть будет использоваться не cli скриптов, а workflow manager для запуска и получения результатов. Это упрощает воспроизвдение исследваоний, потому что не нужно будет ручками запускать все команды, или ипользовать какие-то самописные скрипты типа sh скриптов. То есть человеку который захочет воспроизвести исследвоания достаточно будет знать как пользоваться workflow manager, а не разбираться в ваших скриптах.\nНу и последний поинт, такие инстурменты позволяют проводить распределенные вычесления и берет на себя весь оверхед связанный с распределением и синхронизацией задач. Вам не потребуется прописывать это на уровне кода."
  },
  {
    "objectID": "15.workflow_manager.html#основные-требвоания",
    "href": "15.workflow_manager.html#основные-требвоания",
    "title": "Workflow managers",
    "section": "Основные требвоания",
    "text": "Основные требвоания\n\nПонятный и простой синтаксис\n\n\n\nМодульность\n\n\n\n\nПараметризация\n\n\n\n\nПостроение DAG на основе файлов\n\n\n\n\nПоддержка ограничений на ресурсы\n\n\n\n\nПоддержка виртуальных окружений\n\n\n\n\nНе зависимость от языка\n\n\n\n\nРапсределенные вычесления\n\n\n\n\nПравила рядом с кодом\n\n\nПервое что важно - простой и понятный синтаксис. Я не хочу в угоды озвученных ранее поинтов тратить огромные усилия на поддержку пайплайна. Сразу можно сказать что новые члены команды или другие исследваотели врядли разберутся в нем, да и вернутся к проекту с таким пайплайном будет тяжело.\nВторое - модульность, огромный файл с кучей правил, тоже тяжело для осознания, хочется отделять очистку данных от обучения моделей. Это будет интуитивно понятно, если правила будут разделены на несколько модулей, будет понятно куда добавлять новые правила и где искать нужные.\nЕще важна возможность параметризации правил, я не хочу писать десять правил, что бы у меня выполнилось одно и тоже правило но с разными параметрами, я хочу перечислить возможные параметры для этого правила и пусть workflow manager сам запустит это правило столько раз сколько нужно.\nПоскольку наше исследование это некоторый ациклический граф, а основными артефактами являются данные, метркии, графики, то есть некоторые файлы, то важно что бы менджер выстраивал последовательность действий на основе файлов. Конечно можно дополнительно прописывать последовательность исполнения или зависимости между правилами, но у нас это и так будет задано на уровне файлов, зачем делать дополнительную работу.\nТак же хочется что бы могли ограничивать ресурсы требуемые для исполнения правил, что бы workflow manager мог эффективно их паралелить и не было такого, чо бы два правила требовали GPU в один момент времени. То есть не появлялось каких-то конфликтов за ресурсы. если инструмент не поддерживает такие ограничения, то для DS он бесполезен, так как могут появляться окнфликты и ошибки при выполнении.\nЕще один момент, это поддержка виртуальных окружений. Иногда требуется добавлять специфичные зависимости для определенных правил или менять язык исполнения с python на c++. Хочется что бы workflow менджер все этог запускать в некоторых изолирвоанных средах окружения. Ну и по этой же причине хочется не зависеть от определенного языка, так как бывает необходимость сменить один язык на другой.\nПоддрежка распределенных вычеслений тоже важный момент. Раз уж мы используем такой инструмент, то классно было бы уметь запускать все задачи распределенно и синхронизировать их.\nНу и последнее, я хочу спокойно хранить в git все правила, что бы они также версионировались и собственно инстурмент мог по моему репозиторию все запустить."
  },
  {
    "objectID": "15.workflow_manager.html#directed-acyclic-graph",
    "href": "15.workflow_manager.html#directed-acyclic-graph",
    "title": "Workflow managers",
    "section": "Directed Acyclic Graph",
    "text": "Directed Acyclic Graph"
  },
  {
    "objectID": "11.neptune.html#section",
    "href": "11.neptune.html#section",
    "title": "Neptune.ai",
    "section": "",
    "text": "Neptune — это хранилище метаданных машинного обучения, предназначенное для исследовательских групп, которые проводят множество экспериментов. Чувствуете, инструмент специально для DS. Вы можете складывать все данные, модели, графики, картинки, любые метаданные для DS.С помощью одной строки кода артефакты Neptune позволяют создавать версии наборов данных, моделей и других файлов с локального диска или любого хранилища, совместимого с S3."
  },
  {
    "objectID": "11.neptune.html#neptune-components",
    "href": "11.neptune.html#neptune-components",
    "title": "Neptune.ai",
    "section": "Neptune components",
    "text": "Neptune components\n\nneptune-client\napp.neprune\n\n\nнептун-клиент– Клиентская библиотека Python (API), которую вы используете для регистрации и запроса метаданных построения модели. Поэтому установить его можно с помощью любого питоновского пакетного менеджера.\napp.neptune.ai– веб-приложение для визуализации, сравнения, мониторинга и совместной работы."
  },
  {
    "objectID": "11.neptune.html#использование",
    "href": "11.neptune.html#использование",
    "title": "Neptune.ai",
    "section": "Использование",
    "text": "Использование\nimport neptune.new as neptune\nfrom sklearn.datasets import load_wine\n...\n\nrun = neptune.init_run()\ndata = load_wine()\nX_train, X_test, y_train, y_test = train_test_split(...)\n\nPARAMS = {\"n_estimators\": 10, \"max_depth\": 3, ...}\nrun[\"parameters\"] = PARAMS\n\nclf = RandomForestClassifier(**PARAMS)\n...\n\ntest_f1_score = f1_score(y_test, y_test_pred.argmax(axis=1), average=\"macro\")\nrun[\"test_f1\"] = test_f1_score\nrun[\"model\"].upload(\"model.pkl\")"
  },
  {
    "objectID": "11.neptune.html#просмотре-данных",
    "href": "11.neptune.html#просмотре-данных",
    "title": "Neptune.ai",
    "section": "Просмотре данных",
    "text": "Просмотре данных"
  },
  {
    "objectID": "11.neptune.html#анализ-метаданных",
    "href": "11.neptune.html#анализ-метаданных",
    "title": "Neptune.ai",
    "section": "Анализ метаданных",
    "text": "Анализ метаданных"
  },
  {
    "objectID": "11.neptune.html#что-можно-отслеживать",
    "href": "11.neptune.html#что-можно-отслеживать",
    "title": "Neptune.ai",
    "section": "Что можно отслеживать",
    "text": "Что можно отслеживать\n\nData\nLearning logs\nTF/Pytorch checkpoints\nModels\nMetadata\nNotebooks\n\n\nОбычно вы будете создавать новую версию каждый раз, когда запускаете скрипт, выполняющий обучение модели, повторное обучение или анализ. E У вас зафиксируются данные, модели, метрики и графики полученные в ходе выполнения.\nРеестр моделей позволяет управлять метаданными и жизненным циклом моделей отдельно от экспериментов.\nДля каждой модели можно создавать и отслеживать версии модели. Для управления жизненным циклом модели вы можете отдельно контролировать этап каждой версии модели. Есть интеграция с экосистемами ML, и вы можете фиксировать в neptune промежуточные состояния моделей.\nДля распространения результатов на команду вы можете хранить, например, последний набор данных проверки для вашей задачи машинного обучения в специально отведенном месте\nС помощью расширения neptune-notebook вы можете делать снимки контрольных точек Jupyter Notebook и сравнивать их в специальном разделе приложения."
  },
  {
    "objectID": "11.neptune.html#ограничения",
    "href": "11.neptune.html#ограничения",
    "title": "Neptune.ai",
    "section": "Ограничения",
    "text": "Ограничения\n\nКоличество логов-часов\nОбъем хранилища\n\n\nВ индивидуальном плане вы получаете месячную квоту в 200 часов мониторинга и лимит хранилища в 100 ГБ.\nЕсли вы студент, профессор, участник конкурса Kaggle или исследователь, вы получите бесплатный доступ к нашей лицензии Neptune Team с увеличенным количеством часов регистрации в месяц.\nЛог-часы учитываются таким образом, что если после отправки на сервер какой-то информации в течении 10 минут произойдет еще один запрос, то время записи логов будет потрачено на это время, собственно после этого следующее событие также должно произойти в течении 10 минут."
  },
  {
    "objectID": "14.templates.html#из-чего-состоит-ds-проект",
    "href": "14.templates.html#из-чего-состоит-ds-проект",
    "title": "Project structure",
    "section": "Из чего состоит DS проект?",
    "text": "Из чего состоит DS проект?\n\n\n\nstateDiagram-v2\n    A:DWH\n    B:Local Data\n    C1:Reseach\n    C2:Reseach\n    C3:Reseach\n    D1:Plots\n    D2:Statistics\n    D3:Data description\n    E1:Dataset\n    E2:Dataset\n    F1:Model training\n    F2:Model training \n    G:Сonclusions\n    R: Report \n    A --> B\n    B --> A\n    B --> C1\n    B --> C2\n    B --> C3\n    C1 --> D3\n    C1 --> D1\n    C2 --> D2\n    C3 --> E1\n    C3 --> E2\n    E1 --> F1\n    E2 --> F2\n    F1 --> G\n    F2 --> G\n    D1 --> R\n    D2 --> R\n    D3 --> R\n    G --> R\n\n\n\nstateDiagram-v2\n    A:DWH\n    B:Local Data\n    C1:Reseach\n    C2:Reseach\n    C3:Reseach\n    D1:Plots\n    D2:Statistics\n    D3:Data description\n    E1:Dataset\n    E2:Dataset\n    F1:Model training\n    F2:Model training \n    G:Сonclusions\n    R: Report \n    A --> B\n    B --> A\n    B --> C1\n    B --> C2\n    B --> C3\n    C1 --> D3\n    C1 --> D1\n    C2 --> D2\n    C3 --> E1\n    C3 --> E2\n    E1 --> F1\n    E2 --> F2\n    F1 --> G\n    F2 --> G\n    D1 --> R\n    D2 --> R\n    D3 --> R\n    G --> R\n\n\n\n\n\n\nДанные - то, с чего начинается любой data science проект. Стоит отметить, сразу несколько деталей касательно данных, о который можно забыть если только решать задачки на kaggle. Первое, скорее всего данными придеться делиться, то есть передавать их другим исследователям, поэтому необходимо центролизованное хранилище данных. Второе, что стоит отметить, данные на самом деле не константные, в процессе исследования вы будете обогащать данные, возможно из третьих источников, а может будут появляться новые данные от заказчика. В любом случае, необходимо решить две задачи: защитить от внешних изменений исходные данные и ввести систему контроля версий данных для отслеживания изменений в данных. Собственно, на идейном уровне эти задачи решают системы контроля верси данных, о которых мы говорили ранее, но как организовать структуру хранения данных в проекте что бы не запутаться в них?\nЕще есть разведочный анализ - важный этап любого исследования. Мы уже говорили о важности постановки гипотез перед исследованием на первом занятии, но я хочу еще раз это повторить. Важно, перед началом работы над проектом установить главную гипотезу вашего проекта и все последующие гипотезы должны исходить из нее. Если вы в начале начинается “копаться” в данных, а лишь затем придумываете гипотезу, вы во-первых, можете зайти быстро в тупик, а во-вторых, начать обманывать самих себя, придумывая объяснения случайно найденным зависимостям. По результатам разведочного анализа должны появиться умозаключения в виде текста и графиков. Вы в начале устанавливаете, а что хотите найти или проверить, а затем должны сохранить ваши результаты. На самом деле, не обязательно пытаться за один раз найти всё. Лучше будет дробить это на малые задачи, в ходе которых вы либо что-то проверяете, либо же пытаетесь что-то описать в данных. То есть, вполне одним из результатов разведочного анализа может быть отчет с описанием формата данных, а может каких-то статистических характеристик ваших данных. В том числе исходя из результатов разведочного анализа, вы можете перейти к следующему этапу, формирование датасетов для моделирования.\nЭксперименты с моделями. На самом деле этот этап не должен сильно отличаться от разведочного анализа. Мы ставим гипотезы, строим модели, проверяем их работу и должны делать выводы в том числе закреплять их в виде текста. Однако, модели сами по себе требуют дополнительной работы по своему построению, а также должны оставаться в качестве результатов вашей работы, поэтому этот этап выделяется отдельно. И таких экспериментов у вас будет много. однако помимо отчетов по каждому отдельному исследованию и модели в конце концов у вас должен быть финальный результат по данному этапу, это ваши заключения по результатам тестов моделей, какая архитектура более подходящая, какие модели показали значения метрик лучше и почему и т.д.\n\nТаким образом, мы видим, что помимо самого кода, для выполнения всех этих манипуляций, в ds проект входят: данные, модели, отчеты в виде текста, графики, возможно ещё какие-то вложения, которые вы будете помещать в свой отчет, не стоит забывать про файлы с конфигурацией, тесты, документация к коду, возможно какие-то вспомогательные файлы для CI и работы с контейнерами… То есть на самом деле, полноценный ds проект имеет множество объектов, которые, если не организовать в строгую структуру, могут превратить проект в месиво из кучи разнородных файлов.\nОтсюда и появляется вопрос - как это все организовать?"
  },
  {
    "objectID": "14.templates.html#cookiecutter-data-science",
    "href": "14.templates.html#cookiecutter-data-science",
    "title": "Project structure",
    "section": "Cookiecutter Data Science",
    "text": "Cookiecutter Data Science\n\n├── LICENSE\n├── Makefile           <- Makefile with commands like `make data` or `make train`\n├── README.md          <- The top-level README for developers using this project.\n├── data\n│   ├── external       <- Data from third party sources.\n│   ├── interim        <- Intermediate data that has been transformed.\n│   ├── processed      <- The final, canonical data sets for modeling.\n│   └── raw            <- The original, immutable data dump.\n│\n├── docs               <- A default Sphinx project; see sphinx-doc.org for details\n│\n├── models             <- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n│\n├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n│\n├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n│   └── figures        <- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze > requirements.txt`\n│\n├── setup.py           <- Make this project pip installable with `pip install -e`\n├── src                <- Source code for use in this project.\n│   ├── __init__.py    <- Makes src a Python module\n│   │\n│   ├── data           <- Scripts to download or generate data\n│   │   └── make_dataset.py\n│   │\n│   ├── features       <- Scripts to turn raw data into features for modeling\n│   │   └── build_features.py\n│   │\n│   ├── models         <- Scripts to train models and then use trained models to make\n│   │   │                 predictions\n│   │   ├── predict_model.py\n│   │   └── train_model.py\n│   │\n│   └── visualization  <- Scripts to create exploratory and results oriented visualizations\n│       └── visualize.py\n│\n└── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io\n\n\nКак этим пользоваться?\npip install cookiecutter\ncookiecutter https://github.com/drivendata/cookiecutter-data-science\n\nСобсвенно решение есть: Логичная, достаточно стандартизированная, но при этом гибкая структура проекта для выполнения работы по науке о данных и обмена ею. Предлагется она группой drivendata.\nВ процессе проведения исследвоаний, последнее о чем вы зачастую думаете это организация и обудмывание структуры проекта, так как есть более важные шаги - сами исследования.Поэтому лучше всего начать с чистой, логичной структуры и придерживаться ее на всем протяжении проекта. Основные причины так поступить заключаются в том, что\n\nДругие люди будут вам благодарны. Шаблонная структура проекта давно появилась в области разработки ПО. Например angular, django ruby on rails и другие масштабные и популярные фреймворки предлагают инициализировать проект согласнно определенной структуре. При соблюдении единой структуры в преокте, новичкам, а такие будут переодически появляться в вашей команде, будет проще осознать где и что находится, не читая всю документацию и код. Было бы очень классно если бы в сообществе DS была бы принята единая структура преокта, было бы куда проще разбираться с любыми проектами.\nВы сами будете благодарны сами себе. Вы когда-нибудь пытались воспроизвести анализ, который вы сделали несколько месяцев назад или даже несколько лет назад? Что и где нужно запустить? В какой последовательности? А там ли лежат данные? А какой файл нужно подать в тот или иной скрипт? Такие вопросы болезненны и являются симптомами неорганизованности проекта. Хорошая структура проекта поощряет приемы, упрощающие возвращение к старой работе, наприер единая точка запуска проекта или система контроля версий данных.\n\nАвторы cookiecutter data scinece предлагают следующую структуру:\n\nmakefile - единая точка запуска всех скриптов, в которой описаны необходимые команды\nreadme - верхнеуровневая документация, в которой описываются какие-то зависимости(типа операционной системы или версии conda), основные команды для подготовки и запуска окружения, стурктура проекта и описание цели проекта.\ndata - там где мы хотим хранить все данные. raw туда куда попадает первоначальная выгрузка, external для каких-то внешних данных или дополнительных данных от заказчиков, interim - промежуточные данные, которы могут быть получены в результате очистки данных, фильтрации аномалий, различных преобразований и feature engeniring, processed для хранения итоговых датасетов для моделирования и оценки качества.\ndocs - дефолтный проект sphinx, где пишутся все отчеты.\nmodels - хранение результатов моделирования\nnotebooks - юпитер ноутбуки для иллюстрации некоторых исследвоаний\nreferences - дополнительные материалы, например статьи по решеаемой задачи, какие-то инстуркции и прочее.\nreports - предполагается использовать sphinx для генерации документации и сгенерированные отчеты складируются в этой папке.\nsetup.py и tox.ini - файлы конфигурации проекта\nsrc - папка для кода, где у нас есть разделение на обработку данных, фичаинжиниринг, моделирвоание и визуализацию.\n\nНу и конечно отвечая на вопрос, а все ли это надо создавать руками? могу сказать что авторы этйо структуры прописали шаблонизатор на основе cookiecutter который по одной команде создаст для вас всю структуру проекта."
  },
  {
    "objectID": "14.templates.html#данные-не-изменны",
    "href": "14.templates.html#данные-не-изменны",
    "title": "Project structure",
    "section": "Данные не изменны",
    "text": "Данные не изменны\n\nНикогда не редактируйте сырые данные, особенно вручную.\n\n\nНикогда не редактируйте сырые данные, особенно вручную, и тем более в Excel. Не переписывайте сырые данные. Не сохраняйте несколько версий сырых данных. Считайте данные (и их формат) неизменными. Код, который вы пишете, должен передавать сырые данные по конвейеру для получения финального результата. Вам не нужно выполнять все шаги каждый раз, когда вы хотите получить результат для определенного шага, но любой должен иметь возможность воспроизвести конечный результат только с кодом в src и данными в data/raw."
  },
  {
    "objectID": "14.templates.html#jupyter-notebookи-для-исследований-и-коммуникаций",
    "href": "14.templates.html#jupyter-notebookи-для-исследований-и-коммуникаций",
    "title": "Project structure",
    "section": "Jupyter notebook’и для исследований и коммуникаций",
    "text": "Jupyter notebook’и для исследований и коммуникаций\n\nИнструменты, как jupyter notebook могут быть полезны, однако не очень эффективны в вопросе воспроизводимости.\n\n\nДля хранения notebook есть отдельная папка в шаблоне. Хорошей практикой будет разбивать файлы в этой папке на подпапки, например, notebooks/exploratory или notebooks/reports. Также из-за структуры notebook, git diff может быть очень неудобным для чтения и поэтому рекомендуется после быстрого прототипирования в notebook переносить код в src и импортировать его в notebook. Таким образом оставляя в блокноте небольшие блоки кода и текстовое описание."
  },
  {
    "objectID": "14.templates.html#src-как-python-пакет",
    "href": "14.templates.html#src-как-python-пакет",
    "title": "Project structure",
    "section": "src как python пакет",
    "text": "src как python пакет\n\nОформляйте код в src как python пакет, готовый к передаче.\n\n\nРанее мы говорили о том, что хотели бы переиспользовать код, полученный при исследовании, при использовании модели. Для этого лучшим вариантом будет оформлять src как python пакет, который затем мы сможем передать для разработки, например, сервиса для использования модели. Для этого в папке src создается __init__.py файл. Дальше вы можете использовать либо pip и создать setup.py, либо poetry и в конце исследования упаковать ваш код в готовый пакет. По своему опыту скажу что эта идея не сработала, уже несколько раз в коцне проекта требовалось рефакторить и переписывать код для передачи его бекендерам. Основная причина в том что код написан для обработки целого фрейма данных, а не одного семпла, для этого приходится дублировать код и вносить в него изменения."
  },
  {
    "objectID": "14.templates.html#анализ---это-dag",
    "href": "14.templates.html#анализ---это-dag",
    "title": "Project structure",
    "section": "Анализ - это DAG",
    "text": "Анализ - это DAG\n\nАнализ данных хорошо представляется в виде направленного ациклического графа.\n\n\nЧасто в анализе у вас есть длительные по времени этапы, например, предварительной обработки данных или обучения моделей.Если эти шаги уже были выполнены (и вы сохранили вывод где-то, например, в каталоге data/interim, data/processed), вы не хотите ждать их повторного вычисления, при повторном запуске пайплайна.Для того, чтобы реализовать такую логику, хорошо подходят workflow менеджеры, например, make."
  },
  {
    "objectID": "14.templates.html#проблемы",
    "href": "14.templates.html#проблемы",
    "title": "Project structure",
    "section": "Проблемы",
    "text": "Проблемы\n\nЛогика cli размазана по проекту\nMake имеет сложный и непонятный синтаксис\nНе учтено какое-то версионирвоание данных\nНет структуры отчета\n\n\nЭто одна из лучших сущесвующих структур datascience проекта, но в ней тоже есть недостатки.\nОдин из них в том, что питоновских фйлах приходится писать блок main для превращения их в скрипты. Из-за этого логика консольного интерфейса размазана по всему преокту и внесение в нее правок проблематично. Одним из решений является использование click и вынесения всей логики в отдельный модуль src, на уровне data, models И других. Так получается, что весь интерфейс вынесен в одном месте, кроме того организован куда удобнее чем через argparse.\nMake имеет сложный и не очень понятный синтаксис, это делает лсожным поддержку проекта, особенно когда этот файл разрастается до огромных размеров. Здесь стоит присмотреться к другим менджерам workflow, например, Paver, Luigi, Airflow, Snakemake, DVC или Joblib. Сегодня мы поразбираем некоторые из них. Так же стоит выделить для правил не один файл, а целую папку типа workflows, где описываются как раз все правила вызовов скриптов по разным модулям.\nВ шаблоне изначально не предполагается использваоние каких то систем версионирвоания данных и соотвественно нужно будет вносить в него корректировки что бы подключить какие-то инструменты, напрмиер для dvc изменение файлов gitignore.\nАвторы структуры проекта не предлагают какой-то структуры для отчетов. А это тоже важная тема, ведь знания являются наиболее важным артефактом DS проекта и их нужно структурировать понятным образом."
  },
  {
    "objectID": "16.make.html#makefile",
    "href": "16.make.html#makefile",
    "title": "Make",
    "section": "Makefile",
    "text": "Makefile\n\nИз чего делаем? (реквизиты) ---> Как делаем? (команды) ---> Что делаем? (цели)\n\n<цели>: <реквизиты>\n    <команда #1>\n    ...\n    <команда #n>\n\nmake — утилита, автоматизирующая процесс преобразования файлов из одной формы в другую. Чаще всего это компиляция исходного кода в объектные файлы и последующая компоновка в исполняемые файлы или библиотеки.\nУтилита использует специальные make-файлы, в которых указаны зависимости файлов друг от друга и правила для их удовлетворения. На основе информации о времени последнего изменения каждого файла make определяет и запускает необходимые программы.\nСам скрипт состоит из набора правил, которые в свою очередь описываются:\n\nцелями (то, что данное правило делает);\nреквизитами (то, что необходимо для выполнения правила и получения целей);\nкомандами (выполняющими данные преобразования)."
  },
  {
    "objectID": "16.make.html#примеры",
    "href": "16.make.html#примеры",
    "title": "Make",
    "section": "Примеры",
    "text": "Примеры\nVirtual Environment Management\ncreate_env:\n    python3.10 -m venv .venv\n\nactivate_env: create_env\n    activate ./.venv/bin/activate\nDependency Management\nupdate_deps:\n    @poetry update\n\ndeps_install_no_dev: update_deps\n    @poetry install --no-dev\n\ndeps_install: update_deps\n    @poetry install\n\ndeps_export: update_deps\n    @poetry export --without-hashes --output requirements.txt\nCode Quality\ncheck_flake8:\n    @poetry run flake8 ./fundom --count --show-source --statistics\n\ncheck_isort:\n    @poetry run isort **/*.py --check-only\n\ntest:\n    @poetry run pytest\n\ncheck: check_flake8 check_isort test\n    @echo \"All checks are finished!\"\n\nsetup_pre_commit:\n    @pre-commit install"
  },
  {
    "objectID": "16.make.html#proscons",
    "href": "16.make.html#proscons",
    "title": "Make",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\nНе зависит от языка\nDAG на основе файлов\nПравила рядом с кодом\nПаралельное выполнение\n\n\nCons\n\nНе тривиальный синтаксис\nСложное разбиение на модули\nРучная параметризация\nНет виртуальных окружений\nНет ограничений на ресурсы\nНет распределенных вычеслений"
  },
  {
    "objectID": "20.docs.html#проблемы-ds-отчетов",
    "href": "20.docs.html#проблемы-ds-отчетов",
    "title": "Reports",
    "section": "Проблемы DS отчетов",
    "text": "Проблемы DS отчетов\n\n\nОтчет пишут в конце проекта: - Описывают конечное решение - Теряются промежуточные эксперименты - Теряется описание мотивации к некоторым шагам - Теряется часть результатов\n\nОтчет пишут в процессе проекта: - Фиксируют все шаги исследования - Легко делиться знаниями в рабочей команде - В любой момент можно продемонстрировать результаты\n\n\n\nСегодня мы поговорим о документации к DS проекту. Начнем с того - какие в ней есть проблемы. Точнее в процессах большинства проектов. Наверное вы знаете, что по итогам DS проектов пишутся статьи и, собственно, проблемы которые в них есть рождаются от того, что документация пишется по итогу, а не в процессе работы над проектом.\nПолучается, что документация или статья описывает только конечное решение, а не путь к его получению. А для того что бы воспроизвести решение, нужно знать какой путь был проделан. Изз-аз этого проблематично воспроизвести работу по статье. Так как потеряны связующие моменты - промежуточные выводы. Например вы построили модель для классификации обращений и проинтерпретировали ее. Увидели, что наибольший вес в ней имеют мусорные токены типа приветствий и ФИО людей. Отсюда появляется мотивация для очистки обращений от таких токенов и повторное проведение моделирования. Если не описать это, то в будущем будет потеряна информация, а почему же делались те или иные шаги в решении.\nКроме этих выводов теряются какие-то промежуточные результаты экспериментов. Да и не только промежуточные. Если вовремя не описать какие-то результаты, их довольно легко потерять и не воспроизвести в конце работ над проектом. Таким образом можно потерять важные результаты, которые влияли на ваше исследование.\nЭти проблемы решает ведение документации по ходу проекта. Есо записывать результаты каждого эксперимента, каждого шага исседваония, то вы их точно не потеряете. И все записи будут у вас храниться, а если все это вести внутри гита, то и версионироваться. Ранее я уже говорил что мы сделали версионирование исследований, то есть начали версионирвоать данные + код + результаты, сюда стоит добавить и версионирование документации, самих отчетов.\nКроме этого написание отчета в процессе работы позволит легко делиться знаниями внутри команды, можно без труда узнать результаты работы других членов команды, не созваниваясь с ними и не обсуждаю всю проделанную работу. Это сэклномит время на омуникации и позволит исследваотелям не держать все знания в голове. Ну и на последок из плюшек - это наличие промежуточного отчета позволит показывать промежуточные результаты заказчикам без особых затруднений. Не нужно будет готовить презентации с нуля, можно будет использовать для демонстрации ваш отчет."
  },
  {
    "objectID": "20.docs.html#четкая-цель",
    "href": "20.docs.html#четкая-цель",
    "title": "Reports",
    "section": "Четкая цель",
    "text": "Четкая цель\n\nОбщее описание проблемы\nЦели и задачи проекта\nБизнес-постановка задачи\nОсновные метрики и требования приемки\nВид результата\n\n\nОдним из самых разочаровывающих и расточительных усилий является разработка чего-то, что на самом деле никому не нужно. И тем не менее, все мы иногда становимся жертвами этого (по крайней мере, я знаю, что бывал).\nЧтобы снизить этот риск, начинать любой проект стоит с четкой цели. Описать, а какую проблему вы все таки решаете, какая цель у проекта и какие задачи для достижения это цели нужно решить. Задокументируйте бизнес-цели заказчика. Определите, как ваш проект по науке о данных удовлетворит его потребности. И какие требования к нему будут выдвигаться. Так же стоит зафиксировать что будет являться результатом проекта в каком виде он будет.\nНа своем опыте могу сказать, что стоит документировать то, что вы не собираетесь делать (выходящее за рамки вашего проекта)."
  },
  {
    "objectID": "20.docs.html#данные",
    "href": "20.docs.html#данные",
    "title": "Reports",
    "section": "Данные",
    "text": "Данные\n\nИсточники данных\nОписание данных\nПрепроцессинг\nАнализ данных\n\n\nЧто стоит описывать про данные?\nИсточники данных - Описание источников, откуда были взяты данные. Каким образом планируется забирать данные из этих источников. - Какие данные используются для модели? - Почему были выбраны именно эти данные (и исключены другие наборы данных)? - Как были получены данные? - Где находятся данные? - Как часто обновляются данные?\nОписание данных - Какие данные будут использоваться на проекте, в каком они формате, какая у них структура. - Как выглядят данные? (среднее значение, медиана, мода, асимметрия, ожидаемый объем данных и т. д.) - Какие известные проблемы в данных?\nПрепроцессинг - Какие шаги были сделаны по предобработке данных, приведения их к нормальному виду. - Как вы изменяли данные (преобразования, импутации, применялись другие методы очистки данных и т. д.)\nАнализ данных - Результаты проведения EDA. Данный раздел может содержать скриншоты из Jupyter с комментариями и выводами. - Какие есть закономерности в данных?\nДокументирование данных поможет во многих отношениях. Основная цель документации - помочь исследователям, которые захотят работать с теми же данными или данными из того же источника. Подробное описание некоторых моментов сильно облегчит жизнь последующим исследователям, а так же позволит распространить эту информацию в вашей команде."
  },
  {
    "objectID": "20.docs.html#эксперименты",
    "href": "20.docs.html#эксперименты",
    "title": "Reports",
    "section": "Эксперименты",
    "text": "Эксперименты\n\nПроверяемая гипотеза\nИспользуемые признаки, целевую переменную\nПрепроцессинг признаков\nПостановка эксперимента\nВыводы\n\n\nПереходим к тому, что близко сердцу каждого специалиста по данным, — к научному методу. Этот основной процесс проходит через цикл выдвижения гипотезы, проведения эксперимента и измерения результатов. Большинство проектов по науке о данных также проходят эти этапы — часто повторяя их несколько раз. В процессе проведения эксперимента опишите:\n\nКакую гипотезу вы проверяете?\n\nКакую гипотезу вы проверяете?\nДля чего в целом вы проводите данное исследование?\nКакой результат вы ожидаете?\n\nДанные для исследования:\n\nКакой датасет вы использовали в исследовании?\nПример данных, которые вы взяли для исследования?\nКакие манипуляции вы делали над данными?\nПример данных, которые вы получили?\nКак количественно и качественно изменился датасет?\n\nПостановка эксперимента:\n\nКакие инструменты вы используете?\nПочему именно они?\nЕсть ли какая-то специфика эксперимента? Расскажите о ней.\n\nПолученные результаты:\n\nЧто вы получили в результате эксперимента?\nПокажите таблицы, графики, метрики моделей и прочее.\n\nВыводы:\n\nКак вы интерпретируете свои результаты?\nЧто важного можно из них вынести?\nКакие следующие шаги вы видите?\n\n\nЗадокументируйте результаты и выводы — как со статистической точки зрения, так и с точки зрения влияния на бизнес. Используйте эту информацию для планирования возможных последующих экспериментов и проектной работы."
  },
  {
    "objectID": "20.docs.html#общие-выводы",
    "href": "20.docs.html#общие-выводы",
    "title": "Reports",
    "section": "Общие выводы",
    "text": "Общие выводы\n\nВыводы по результатам экспериментов\nИзвлеченные уроки\nРезультаты эксплуатации\n\n\nПод конец проектной работы стоит указать какие этапы исследования были проведены, какие были результаты проверки гипотез. Описание трудностей, с которыми столкнулись исследователи в процессе выполнения проекта. Ну и описание результатов опытной эксплуатации модели, удалось ли заказчику достигнуть поставленных целей."
  },
  {
    "objectID": "20.docs.html#markdown",
    "href": "20.docs.html#markdown",
    "title": "Reports",
    "section": "Markdown",
    "text": "Markdown\n\n\nPros\n\nПростой синтаксис\nПоддерживает картинки, html, математические формулы\nЛегко генерирует html или pdf\nРендерится в gitlab/github\nМожно настроить локальные ссылки\n\n\nCons\n\nНет возможности включать произвольный контент-файл\nНет способа сборки в единый отчет"
  },
  {
    "objectID": "20.docs.html#latex",
    "href": "20.docs.html#latex",
    "title": "Reports",
    "section": "Latex",
    "text": "Latex\n\n\nPros\n\nПоддерживает произвольные контент-файлы\nЕсть система сборки в единый отчет\nХорошая поддержка модульности\nЛегко генерирует pdf\nУмеет создавать презентации\n\n\nCons\n\nНе рендерится в gitlab/github\nСложный синтаксис\nТребует настройки\nСложные шаблоны и файлы стилей\nПлатформозависим"
  },
  {
    "objectID": "20.docs.html#sphinx",
    "href": "20.docs.html#sphinx",
    "title": "Reports",
    "section": "Sphinx",
    "text": "Sphinx\n\n\nPros\n\nПоддерживает произвольные контент-файлы\nЕсть система сборки в единый отчет\nПоддерживает разные синтаксисы (markdown, rst)\nЛегко генерирует html или pdf\nПростые шаблоны и стили на css\n\n\nCons\n\nТребует настройки\nОчень щепетилен к путям"
  },
  {
    "objectID": "20.docs.html#quarto-markdown",
    "href": "20.docs.html#quarto-markdown",
    "title": "Reports",
    "section": "Quarto markdown",
    "text": "Quarto markdown\n\n\nPros\n\nПоддерживает картинки, html, математические формулы\nЕсть система сборки в единый отчет\nЛегко генерирует html или pdf\nУмеет запускать код\nУмеет создавать презентации\n\n\nCons\n\nТребует настройки\nНет возможности включать произвольный контент-файл"
  },
  {
    "objectID": "4.git_workflow.html",
    "href": "4.git_workflow.html",
    "title": "Git workflow",
    "section": "",
    "text": "Вкратце рассмотрим структуру git - популярную системы контроля версий. Она позволяет совместно работать над проектом множеству разработчиков. В большинстве workflow есть некоторый единый удаленный репозиторий, который хранит в себе все изменения от всех разработчиков. У каждого разработчика есть локальный репозиторий, синхронизация репозиториев происходит с помощью push и fetch команд. Кроме локально репозитория у разработчика есть индекс (некоторая база отслеживающая файлы и изменения в них) и workspace. При создании нового файла его сначала нужно добавить в index командой git add, а потом создать коммит git commit. Откатиться на изменения в индексе или локальном репозитории можно командами checkout и checkout HEAD. Сравнить состояние workspace можно командами deff и diff HEAD. Команды pull и rebase позволяют подтягивать изменения с удаленного репозитория сразу и в локальный репозиторий, и в workspace. В целом это основной набор команд, который вам пригодиться и ими вы чаще всего будете пользоваться при работе с git."
  },
  {
    "objectID": "4.git_workflow.html#какие-workflow-существуют",
    "href": "4.git_workflow.html#какие-workflow-существуют",
    "title": "Git workflow",
    "section": "Какие workflow существуют?",
    "text": "Какие workflow существуют?\n\ngitflow\ngithub flow\nforking workflow\ndata science lifecycle process\n\n\nСегодня мы рассмотрим 4 подхода: git, github, forking flow подходы, используемые в разработке, data science lifecycle process это уже некоторая попытка адаптировать flow под исследования."
  },
  {
    "objectID": "4.git_workflow.html#gitflow",
    "href": "4.git_workflow.html#gitflow",
    "title": "Git workflow",
    "section": "Gitflow",
    "text": "Gitflow\n\n\n\n\n\n\nGitflow одна из первых методологий управления проектом направленное на периодические, не частые релизы. В ней есть основная ветка master, в которой хранятся все релизы. До релиза разрабатываемый код находится в ветке develop. Каждое нововведение (фичу) один разработчик разрабатывает в отдельной ветке, после этого она сливается с веткой develop. Когда выполнен весь объем фич для релиза, из develop создается ветка release, в которой ведется доработка релиза и потом она вливается в master и тегируется, а также вливается в develop. Теги отвечают за версии релизов. В случае обнаружение багов в релизе появляется ветка hotfix в которой исправляются ошибки и она снова вливается в master и develop.\nКак видите, методология достаточно тяжеловесная и применима в основном в enterprise разработке, так как она подстраивается под большинство фреймворков управления проектов по водопадной модели. В исследования применить такую методологию можно, но в результате вы получите очень много overhead для проекта по управлению ветками."
  },
  {
    "objectID": "4.git_workflow.html#github-flow",
    "href": "4.git_workflow.html#github-flow",
    "title": "Git workflow",
    "section": "Github flow",
    "text": "Github flow\n\n\n\n\n\n\nGithub flow это модификация gitflow предложенная github. Она направлена на снижение количества веток и упрощение поддерживания проекта. В ней оставаться две ветке - master и feature. Разработка новой функциональности проводится в отдельной фича-ветке, после этого происходит merge в master, после этого происходит новый релиз проекта.\nЭта методология построена для проектов, управляемых в различных гибких методологиях. Одной из важных рекомендаций авторов, является открытие merge request заранее и обсуждение в нем различных моментов заранее, до окончания работ. В целом методология достаточно легковесная и гибкая, может подойти для проведения исследований, но в исследовании нет потребности в такой частоте релизов на начальных этапах."
  },
  {
    "objectID": "4.git_workflow.html#forking-workflow",
    "href": "4.git_workflow.html#forking-workflow",
    "title": "Git workflow",
    "section": "Forking workflow",
    "text": "Forking workflow\n\n\n\n\n\n\nForking workflow это методология поверх github workflow для разработки opensource проектов. Основная сложность в opensource связана с тем, что изначально доступа к изменению в репозитории проекта у потенциального контрибьютора нет, поэтому ему предлагается следующая последовательность шагов:\n\nРазработчик «разветвляет» «официальный» серверный репозиторий. Это создает их собственную копию на стороне сервера.\nНовая серверная копия клонируются в их локальную систему.\nВ локальный клон добавлен удаленный путь Git для «официального» репозитория.\nСоздана новая локальная ветка функции.\nРазработчик вносит изменения в новую ветку.\nДля изменений создаются новые коммиты.\nВетвь помещается в собственную серверную копию разработчика.\nРазработчик открывает запрос на перенос из новой ветки в «официальный» репозиторий.\nЗапрос на перенос утверждается для слияния и объединяется в исходный серверный репозиторий.\n\nТаким образом организована разработка в opensource проектах. Могу сказать, что данная методология для исследований подходит плохо, из нее можно позаимствовать идею раздельных репозиториев, что если в команде много исследователей, то возможно их нужно разнести по разным репозиториям и периодически вытаскивать полезные нововведения в “официальный” репозиторий."
  },
  {
    "objectID": "4.git_workflow.html#data-branch",
    "href": "4.git_workflow.html#data-branch",
    "title": "Git workflow",
    "section": "Data branch",
    "text": "Data branch\n\n\nВводятся различные ветки, например есть data branches, в которых реализуется код для обработки данных, пишется отчет об этом и тесты для кода. На слайде представлена последовательность действий в такой ветке."
  },
  {
    "objectID": "4.git_workflow.html#explore-and-experiment-branches",
    "href": "4.git_workflow.html#explore-and-experiment-branches",
    "title": "Git workflow",
    "section": "Explore and experiment branches",
    "text": "Explore and experiment branches\n \n\nДля исследования данных предлагается использовать explore branches. Эти ветки остаются висеть в истории гита без мерджа, потому что код, который проводит исследование нужен разово, чтобы подтвердить или опровергнуть гипотезу.\nДля экспериментов есть две стратегии, если эксперимент опроверг подход, то такая ветка не получает merge request и просто оставаться висеть в истории, так как знания о неудачных экспериментах тоже важны и влияют на принятие следующих решений в развитии проекта.\nДля успешного эксперимента ветка переходит в ветку моделей."
  },
  {
    "objectID": "4.git_workflow.html#model-branches",
    "href": "4.git_workflow.html#model-branches",
    "title": "Git workflow",
    "section": "Model branches",
    "text": "Model branches\n\n\nКогда был найден удачный подход то появляется новая ветка model branch в которой происходит настройка и исследование модели, а также написание отчетов и тестов. После этого происходит влитие ветки в master branch и релиз модели."
  },
  {
    "objectID": "4.git_workflow.html#хорошая-методология",
    "href": "4.git_workflow.html#хорошая-методология",
    "title": "Git workflow",
    "section": "Хорошая методология?",
    "text": "Хорошая методология?\n\n\nПлюсы\n\nВ основной ветке только важный код.\nСохраняется информация о всех исследованиях.\nИмеет “логичные” разделения веток для разных задач.\n\n\nМинусы\n\nСложно автоматизировать воспроизведение всех исследований.\nИнформация об исследованиях “размазана” по репозиторию.\nВ теории выглядит хорошо, а на практике…\n\n\n\n\nМетодология конечно неплохая, но сильно оторвана от реальности. По факту в проекте большая часть работы связана с обработкой и подготовкой данных, так как чистые датасеты вы встретите только на kaggle, в реальности придется очень долго разбираться с данными и возвращаться к ним постоянно. Поэтому хотелось бы как-то просто обновлять результаты уже проведенных исследований.\nВ целом методология рабочая, но больше подходит для уже существующего проекта, где есть собранные и подготовленные датасеты и основной задачей является моделирование."
  },
  {
    "objectID": "4.git_workflow.html#выводы",
    "href": "4.git_workflow.html#выводы",
    "title": "Git workflow",
    "section": "Выводы",
    "text": "Выводы\n\nflow должен быть удобным команде\nне надо его перегружать, если нет необходимости\nиногда, не надо пытаться сразу объединять все результаты исследований\n\n\nСамое важное не то какую методологию вы выберите, а то что вы этой методологии будете следовать всей командой. Все эти git workflow направлены на унификацию процесса развития проекта с точки зрения гита.\nНа мой взгляд наиболее рабочей методологией будет github workflow, с оговоркой, что релизом будет не каждый merge в master, а тег. Все остальное с названиями веток и прочим, большой когнитивный оверхед для работы исследователей."
  },
  {
    "objectID": "12.dvc.html#section",
    "href": "12.dvc.html#section",
    "title": "DVC",
    "section": "",
    "text": "DVC — это средство управления версиями данных , автоматизации рабочих процессов машинного обучения и управления экспериментами , использующее преимущества существующего набора инструментов для разработки программного обеспечения, с которым вы уже знакомы (Git, ваша IDE, CI/CD и т. д.). DVC помогает специалистам по обработке данных и машинному обучению управлять большими наборами данных, обеспечивать воспроизводилось проектов и улучшать совместную работу."
  },
  {
    "objectID": "12.dvc.html#dvc-modules",
    "href": "12.dvc.html#dvc-modules",
    "title": "DVC",
    "section": "DVC modules",
    "text": "DVC modules\n\nDVC cli\nDVC extentions\nDVC vscode\nDVC live\nIterative Studio"
  },
  {
    "objectID": "12.dvc.html#dvc-storages",
    "href": "12.dvc.html#dvc-storages",
    "title": "DVC",
    "section": "DVC storages",
    "text": "DVC storages\n\nAmazon S3\nMicrosoft Azure Blob Storage\nGoogle Cloud Storage\nSSH\nHDFS\nHTTP\nLocal"
  },
  {
    "objectID": "12.dvc.html#использование",
    "href": "12.dvc.html#использование",
    "title": "DVC",
    "section": "Использование",
    "text": "Использование\nИнициализация dvc репозитория\ndvc init\ngit commit -m \"Initialize DVC\"\nДобавление данных\ndvc add data/data.csv\ngit add data/data.csv.dvc data/.gitignore\ngit commit -m \"Add raw data\"\nОбновление данных\ndvc commit\ngit commit -m \"Change data\"\nДобавление хранилища\ndvc remote add -d --project gdrive gdrive://<url>\ngit add .dvc/config\ngit commit -m \"Configure remote storage\"\nОтправка в хранилище\ndvc push\nВыгрузка из хранилиища\ndvc pull\nПереключение между версиями\ngit checkout <git-reversion>\ndvc checkout\n\nИнициализация репозитория, добавление и обновление данных для отслеживания, добавление удаленного хранилища, команды для синхронизации с ним и переключение между версиями данных.\nТак же стоит сказать про то, что у dvc есть git hooks, которые позволят dvc исполняться вместе с вашими командами гиту."
  },
  {
    "objectID": "12.dvc.html#ci-cd",
    "href": "12.dvc.html#ci-cd",
    "title": "DVC",
    "section": "CI-CD",
    "text": "CI-CD\n\n\n\n\n\n\n\n\nТак как мы в команде используем dvc, то уделим ему чутка побольше времени. Например тому как его можно встроить в CI.\nМы достаточно много сил вкладываем в развитее компетенции MlOps в команде. Одним из инструментов, который качественно улучшил процесс работы - DVC\nВообще зачем нужен CI-CD в DS проектах?\nВо-первых, мы можем автоматизировать и принудительно проводить тестирование. Ну не совсем тестировать, скорее проводить smoke test проведенных исследований. Проверять воспроизводиться они или нет.\n\nПроверка данных: мы можем проверить, что после запуска скриптов мы получили те же данные. Более сложные проверки - проверка по схеме или проверка согласованности конвейера — правильные формы, типы данных и т. д.\nПроверка модели: Опять же проверить, что модель осталась идентичной после исполнения скриптов. Так же есть более сложные проверки, например оценка качества на отложенной выборке, проверка воспроизводимости и тд.\n\nВо-вторых, мы можем организовать поставку моделей и отчетов. Так как мы можем версионирвоать все метрики и результаты, то сгенерировать отчет очень просто. Забегая вперед, dvc позволяет генерировать отчеты об экспериментах, но это подробнее разберем позже.\nЧто же делает тут DVC - устраняет необходимость в создании баз данных версий, использовании специальных структур файлов/папок или написании специального кода интерфейса. Без DVC пришлось бы строить свою систему версионирования артефактов и пытаться ее привязать к результатам исполнения. Вместо этого DVC хранит метаинформацию в Git («кодирование» данных и моделей машинного обучения), в то же время отправляя фактическое содержимое данных в облачное хранилище. DVC также обеспечивает управляемую метриками навигацию в репозиториях Git."
  },
  {
    "objectID": "12.dvc.html#dvc-live",
    "href": "12.dvc.html#dvc-live",
    "title": "DVC",
    "section": "DVC live",
    "text": "DVC live\nfrom dvclive import Live\n\nwith Live() as live:\n    live.log_param(\"epochs\", NUM_EPOCHS)\n\n    for epoch in range(NUM_EPOCHS):\n        train_model(...)\n        metrics = evaluate_model(...)\n        for metric_name, value in metrics.items():\n            live.log_metric(metric_name, value)\n        live.next_step()\n\n\nУ dvc есть функционал схожий с neptune, вы можете сохранять результаты в процессе обучения модели, снимать параметры обучения и метрики, более того так результат можно получить сразу в vscode в отдельном расширении. есть интеграции с keras, pytorch\nНу и самый смак все это без ограничений и бесплатно."
  },
  {
    "objectID": "12.dvc.html#dvc-api",
    "href": "12.dvc.html#dvc-api",
    "title": "DVC",
    "section": "DVC api",
    "text": "DVC api\nimport dvc.api\nwith dvc.api.open(\n    'get-started/data.csv',\n    repo='https://github.com/iterative/dataset-registry'\n) as f:\n    # ... f is a file-like object\n\nDvc имеет также питоновское апи которое позволяет получать определённые версии данных и моделей из хранилища. Более того для этого можно использовать другой репозиторий.\nЭто очень классно, так как разработчики могут сослаться на ваш репозиторий и получить нужные версии моделей на своей стороне в приложении, вместо того что бы предавать на флешке и диске модели. Это упрощает поставку моделей и поддержку ml решения."
  },
  {
    "objectID": "12.dvc.html#proscons",
    "href": "12.dvc.html#proscons",
    "title": "DVC",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\nЛюбые данные и хранилища\nGit like инстурмент\nЕсть live-tracking обучения\nПереиспользуемо в прод\nИнтегрируемо с разными Фреймворками\nНе требует изменений в коде\nУлучшение CI-CD\nМного документации\nЕсть свои инструменты для колоборативной работы\n\n\nCons\n\nНе всегда воспринимает gitignore файлы\nПолностью зависит от репозитория\nНемного деревянный инструмент"
  },
  {
    "objectID": "3.team_work.html",
    "href": "3.team_work.html",
    "title": "Team work",
    "section": "",
    "text": "Представьте, вы работаете в огромной команде над каким-то исследованием в области анализа данных. Собственно, вопрос, какие были бы общие артефакты у этой команды? Чем бы вам понадобилось делиться с коллегами по работе?\n\n\n\n\n\nОбщие договоренности.\n\n\n\n\nЗадачи, план исследований.\n\n\n\n\nКод, окружение, среда выполнения.\n\n\n\n\nЗнания(отчеты об исследованиях).\n\n\n\n\nИсходные, подготовленные данные и модели.\n\n\nВо-первых, это общие договоренности: ролевая модель, различные процессы, требования. То есть некоторые условности, которые приняты в команде, для решения спорных моментов и некоторой унификации артефактов, кода, задач и отчетов.\nВо-вторых, это задачи. В команде нужно как-то организовать рабочий процесс, разделить ответственность и обязанности на членов команду, выбрать основные направления для исследования, определить ключевые точки в исследованиях. Это все нужно сделать что бы каждый член команды знал, что ему нужно делать, какие эксперименты проводить, какой код писать. При этом нужно организовать так что бы обязанности не дублировались или дублировались по минимуму.\nВ-третьих, код и окружение. Вам всем придется писать код для экспериментов, переиспользовать его, поддерживать, периодически обновлять и перезапускать. Кроме написания кода, вам придется этот код запускать, для этого нужно воспроизводимое окружение, а лучше общая инфраструктура.\nВ-четвертых, это знания полученные в ходе исследования. Их нужно фиксировать, обновлять, версионировать и передавать команде. Вот, например, сотрудник проводил важные эксперименты, но вышел в отпуск или заболел, нужно как-то его знания о проведенных исследованиях, полученных результатах и сформулированных выводах. Для всего этого надо как-то управлять знаниями.\nВ-пятых, нужно решить с большими и громоздкими артефактами — данными и моделями, их нужно как-то хранить и версионировать, а еще делиться с коллегами. Более того нужно писать такой код, который бы выдавал воспроизводимые результаты, всегда один и тот же бинарный объект или массив данных."
  },
  {
    "objectID": "3.team_work.html#примеры-contributing",
    "href": "3.team_work.html#примеры-contributing",
    "title": "Team work",
    "section": "Примеры Contributing",
    "text": "Примеры Contributing\n\npython devguide\ncontribute to scipy\nsklearn contributing\ncontribute to tensorflow\n\n\nКак видите в них описывалось все, начиная от того, как развернуть проект, как оформить merge request, как написать документацию к изменениям, заканчивая часами работы core developers и тем как отвечать на вопросы в stack overflow.\nДовольно обширные своды правил и информации как разрабатывать проект. Такие практики стоит перенимать и задействовать в своих командах."
  },
  {
    "objectID": "3.team_work.html#что-стоит-включать-в-договоренности",
    "href": "3.team_work.html#что-стоит-включать-в-договоренности",
    "title": "Team work",
    "section": "Что стоит включать в договоренности?",
    "text": "Что стоит включать в договоренности?\n\nВсе где возникают разногласия в команде\n\n\nКак и куда писать код\nКак и куда писать тесты\nКак настроить среду разработки\nКак и куда писать документацию\nКак и куда сохранять данные\nКак и где составлять задачи\nКак брать задачу на себя\nКак оформлять merge requests\nКто за что отвечает\nУ кого можно получить помощь по процессам\nКакие инструменты можно и нельзя использоваться\nКак этими инструментами пользоваться\nКак организован CI в проекте\nЧто писать в комитах\nКак происходит починка багов\nКакой у проекта development cycle\nУ кого можно получить помощь по процессам\n\nи многое другое.\n\nВсе это стоит фиксировать и поддерживать в актуальном состоянии. Это полезно. Полезно по двум причинам:\n\nКейс первый. Вы работаете с Петей и Васей. Вы договорились с Васей писать код в стиле А и начали работать. Петя об этом не знает и пишет его по-своему, в стиле Б. Когда вы объединяете свои наработки, у вас получается не консистентный код, в одном модуле функции и классы, в другом весь код написан в глобальной области как один скрипт. Получается, что текущие наработки нужно будет рефакторить и приводить в порядок. А все это произошло потому что информация не была передана всем участникам команды. Если вы о чем-то договорились, то это стоит зафиксировать в общедоступном месте и донести до всех членов команды.\nКейс второй. К вам в команду пришел новый сотрудник. Для того что бы рассказать ему все и помочь настроить проект вам потребуется несколько дней с ним общаться и передавать знания. При этом важно ничего не забыть. Но если все договоренности зафиксированы, то сотрудник сможет сам разобраться с этим и к вам придет уже с конкретными вопросами."
  },
  {
    "objectID": "3.team_work.html#резюме",
    "href": "3.team_work.html#резюме",
    "title": "Team work",
    "section": "Резюме",
    "text": "Резюме\n\nДоговоренности фиксируются в общедоступном месте.\nДоговоренности несут пользу процессу и членам команды.\nДоговоренности можно отменять, если они мешают команде."
  },
  {
    "objectID": "3.team_work.html#defenition-of-ready-dor",
    "href": "3.team_work.html#defenition-of-ready-dor",
    "title": "Team work",
    "section": "Defenition of ready (DoR)",
    "text": "Defenition of ready (DoR)\n\nМотивация появления задачи.\nПроверяемая гипотеза.\nНа каких данных проводить исследование.\nОписание действий для обработки данных.\nКонкретный ожидаемый результат.\n\n\nВажно иметь список критериев, описывающий состояние задачи, когда ее можно взять в работу. Поскольку задачи в проекте могут возникать не только от тимлида, но и от самих исследователей, так как во время эксперимента были выявлены какие-то аномалии в данных или же необходимо проверить какие-то дополнительные гипотезы, нужно, что бы такие договоренности были в команде и все могли формировать достаточно подробные задачи.\nВ задаче стоит указать, почему она вообще появилась и какой impact она сделает для проекта, например, в проекте по классификации электронных сообщений проводилось исследование по ключевым словам для каждой темы, в какой-то теме ключевыми словами стали здравствуйте и привет, соответственно в проекте появилась задача по удалению приветствий и в ней стоит упомянуть причину ее появления.\nНужно указать проверяемую гипотезу, то есть что именно нужно проверить в ходе исследования. Например, проверить наличие зависимости между признаками, или выбор стратегии заполнения пропусков для такого-то признака, или выделение ключевых слов на основе какого-то подхода, или же построение такой-то модели на данных проекта.\nТак же важно указать какие данные стоит использовать в исследовании, потому что если в проекте хранятся промежуточные версии данных, то сложно с ходу разобраться, а какая из них нужна для проверки гипотезы.\nЕсли нет готовых данных или задача заключается в обработке данных, то нужно описать какие действия нужно предпринять для подготовки данных, что бы исполнитель задачи не догадывался как нужно объединить два датасета что бы получилась корректная выборка.\nНу и последнее конкретный результат, то есть что вы ожидаете при выполнении задачи, например, если вернутся к примеру, про классификацию писем, то ожидаемый результат, что приветствия пропадут из списков ключевых слов, или же какая-то модель получит более высокие метрики. То есть, что вы, как автор задачи, ожидаете после ее исполнения.\nЕсли вы как автор не можете сформулировать такую задачу и для нее нужны дополнительные знания, то стоит сначала создать задачу для получения этих знаний, например, задача по выбору источника данных погоды или же задача по исследованию применения нейростей определенной архитектуры в задачах определенной тематики."
  },
  {
    "objectID": "3.team_work.html#типы-задач",
    "href": "3.team_work.html#типы-задач",
    "title": "Team work",
    "section": "Типы задач",
    "text": "Типы задач\n\nЗадачи обработки данных.\nЗадачи проведения исследований.\nВспомогательные задачи.\n\n\nЗадачи обработки данных появляются в основном на первых этапах исследования и связаны с очисткой и подготовкой датасета. Постановка такой задачи должна содержать:\n\nмотивацию для появления этой задачи, зачем она нужна, как будет использоваться результат;\nназвание предполагаемого(ых) датасета(ов) для выполнения, в процессе работы исполнитель может их изменить при необходимости (если поймет, что есть данные более подходящего формата);\nописание действий над данными: фильтрация, очистка, заполнение пропусков, объединение и тд.\nпри необходимости фиксируются параметры или же ссылки на участки кода, которые необходимо применить;\nдополнительная известная информация, касающаяся данной задачи.\n\nЗадачи проведения исследований описывают постановку исследования, такие задачи составляют наибольшую часть всех задач и появляются на протяжении всего проекта. В описании карточки необходимо указать:\n\nнаправление исследования, в какой области мы копаемся;\nпроверяемую гипотезу в рамках исследования;\nожидаемый результат, что подтвердит выдвинутую гипотезу.\n\nВспомогательные задачи появляются на протяжении всего проекта, они связаны с различными дополнительными работами или исправлением багов. В постановке такой задачи описывается или баг, или работы, которые надо провести. Для описания бага стоит отметить:\n\nВ чем заключается баг?\nКак его воспроизвести?\nГде проблема в коде? В отчете?\nКакие файлы надо изменить?\nЧто надо настроить и как?"
  },
  {
    "objectID": "3.team_work.html#defenition-of-done-dod",
    "href": "3.team_work.html#defenition-of-done-dod",
    "title": "Team work",
    "section": "Defenition of Done (DoD)",
    "text": "Defenition of Done (DoD)\n\nСоставлен отчет о проведенном исследовании.\nКод соответствует принятым стандартам качества.\nДля новой функциональности написаны тесты.\nПройдено ревью кода и исследования.\n\n\nDoD - Состояние задачи, когда она готова. Членам команды так же важно понимать, что нужно сделать что бы считать задачу выполненной.\nЭто знание необходимо что бы понимать, какой объем работ нужно провести для выполнения задачи, помимо самого исследования, потому что, напоминаю, вы работаете в команде и важно не только провести исследование, но и составить для него описание, которое позволило бы делиться знаниями с другими членами команды, привести в порядок код и оттестировать его.\nНа слайде приведен пример что может быть в DoD, в целом эта договоренность формируется также командой и все члены команды должны понимать зачем им это нужно."
  },
  {
    "objectID": "10.git_lfs.html#section",
    "href": "10.git_lfs.html#section",
    "title": "Git LFS",
    "section": "",
    "text": "Git-LFS предназначен для хранения [относительно] больших файлов в Git серверах, и является расширением Git. Изначально он появился что бы решить проблему, когда дизайнеры добавляли свои макеты в репозиторий, при большом количестве таких файлов гит начинал тормозить, поэтому появился git large file system. LFS позволяет перенести большие файлы на отдельный удаленный сервер. Вместо них он сохраняет легковесные указатели в репозиторий. Когда вы извлекаете версию с таким указателем, LFS просто ищет исходный файл (возможно, на сервере, если он не находится в своем собственном, специальном кэше) и загружает его для вас."
  },
  {
    "objectID": "10.git_lfs.html#установка",
    "href": "10.git_lfs.html#установка",
    "title": "Git LFS",
    "section": "Установка1",
    "text": "Установка1\nУстановить расширение командной строки git\nsudo apt install git-lfs\nИнициализировать в репозитории\ngit lfs install\n\nКак установить git lfs? По сути вам нужно сначала добавить бинарный модуль в систему. Точно также, как и git. После этого его надо инициализировать в репозитории.\n\nНе все службы размещения кода уже поддерживают LFS. Однако пользователю GitLab и GitHub"
  },
  {
    "objectID": "10.git_lfs.html#использование",
    "href": "10.git_lfs.html#использование",
    "title": "Git LFS",
    "section": "Использование",
    "text": "Использование\nДобавить правила для больших файлов\ngit lfs track \"*.csv\"\nДобавить в индекс .csv файл\ngit add path/to/file.csv\nЗакомитить и отправить в удаленный репозиторий\ngit commit -m \"add file.csv\"\ngit push\n\nКак его использовать. Тут тоже все просто - добавляем правило для отслеживания больших файлов. Их может быть несколько, а потом все просто, работаем как с обычным файлом, добавляем в индекс, комитим и пушим. Только тут наш файл полетит в lfs хранилище."
  },
  {
    "objectID": "10.git_lfs.html#ограничения",
    "href": "10.git_lfs.html#ограничения",
    "title": "Git LFS",
    "section": "Ограничения",
    "text": "Ограничения\n\nПо объему lfs хранилища\nНа трафик к lfs хранилищу\nДля CI инструментов\n\n\nЭтот инструмент был бы прекрасен, если бы владельцы площадок типа gitlab или github не вводили жесткие ограничения на размер хранилища и объем трафика.\nЕсли вы отправите файл размером 500 МБ в Git LFS, вы будете использовать 500 МБ выделенного вам хранилища и не использовать трафик. Если вы измените 1 байт и снова отправите файл, вы будете использовать еще 500 МБ хранилища и не будете использовать трафик, в результате чего общее использование для этих двух отправок составит 1 ГБ хранилища.\nЕсли вы спулите файл размером 500 МБ, который отслеживается с помощью LFS, вы будете использовать 500 МБ трафика, выделенного владельцем репозитория. Если соавтор вносит изменение в файл, а вы скачиваете новую версию в свой локальный репозиторий, вы будете использовать еще 500 МБ трафика, в результате чего общее использование для этих двух загрузок составит 1 ГБ трафика.\nЕсли GitHub Actions скачивает файл размером 500 МБ, который отслеживается с помощью LFS, он будет использовать 500 МБ трафика, выделенной владельцем репозитория.\nЕсли вы используете более 1 ГБ пропускной способности в месяц без покупки пакета данных, поддержка Git LFS отключается в вашей учетной записи до следующего месяца.\nКонечно можно использовать selfhosted версию для lfs хранилища и для pet проектов никто не мешает вам это сделать, но все-таки это специализированные хранилища с определенным апи и в компании потребуется дополнительные ресурсы для поддержки такого решения. А в популярных системах размещения репозиториев за это надо платить денежку."
  },
  {
    "objectID": "10.git_lfs.html#proscons",
    "href": "10.git_lfs.html#proscons",
    "title": "Git LFS",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\nGit extention\nSelf-hosted версия\nЛюбые данные\nНе требует изменений в коде\n\n\nCons\n\nТребует LFS хранилище\nНет анализа diff версий\nНе переиспользуемо в прод\nМало документации\nПолностью зависит от репозитория"
  },
  {
    "objectID": "6.docker.html#что-это-такое",
    "href": "6.docker.html#что-это-такое",
    "title": "Docker",
    "section": "Что это такое?",
    "text": "Что это такое?\n\n\nКак я уже говорил про воспроизводимые исследования - хорошо бы было передать вместе с кодом и данными все окружение, так скажем, сделать слепок системы со всеми библиотеками и инструментами.\nВ разработке ПО для этого используется docker. Docker — это открытая платформа для разработки, доставки и эксплуатации приложений. Он позволяет создавать неограниченное количество изолированных контейнеров.\nО, а кто знает, что такое контейнер? > Изолированный linux-процесс или же независимая среда выполнения с выделенными только для неё ресурсами железа. Очень похоже на виртуальную машину…. Но это не она.\nДокер позволяет отвязать исследование от вашей инфраструктуры и окружения, ну или же наоборот связать их вместе, кому как больше нравится считать. Суть в том, что ваше исследование теперь всегда и везде запускается в вашем окружении и инфраструктуре.\nЧуть-чуть расскажу историю , как и кем этот проект создавался. Проект начат как внутренняя собственническая разработка компании dotCloud, основанной Соломоном Хайксом (Solomon Hykes) в 2008 году с целью построения публичной PaaS-платформы с поддержкой различных языков программирования. В марте 2013 года код Docker был опубликован под лицензией Apache 2.0. В дальнейшем компания сменила название на docker. Желая подчеркнуть смену своего курса."
  },
  {
    "objectID": "6.docker.html#section",
    "href": "6.docker.html#section",
    "title": "Docker",
    "section": "",
    "text": "Как уже устроен докер? Docker использует архитектуру клиент-сервер. Docker клиент общается с демоном Docker, который берет на себя тяжесть создания, запуска, распределения ваших контейнеров. Оба, клиент и сервер могут работать на одной системе, вы можете подключить клиент к удаленному демону docker. Клиент и сервер общаются через сокет или через RESTful API.\nЧтобы понимать, из чего состоит docker, вам нужно знать о трех компонентах:\n\nобразы (images)\nреестр (registries)\nконтейнеры\n\nDocker-образ — это read-only шаблон. Например, образ может содержать операционку Ubuntu c NGiNX и приложением на ней. Образы используются для создания контейнеров. Сам образ описывается в Dockerfile. Docker позволяет легко создавать новые образы, обновлять существующие, или вы можете скачать образы созданные другими людьми. Образы — это компонента сборки docker-а.\nDocker-реестр хранит образы. Есть публичные и приватные реестры, из которых можно скачать либо загрузить образы. Публичный Docker-реестр — это Docker Hub. Компонента распространения.\nВ контейнерах содержится все, что нужно для работы приложения. Каждый контейнер создается из образа. Контейнеры могут быть созданы, запущены, остановлены, перенесены или удалены. Каждый контейнер изолирован и является безопасной платформой для приложения. Это компонента работы."
  },
  {
    "objectID": "6.docker.html#ds-docker-image",
    "href": "6.docker.html#ds-docker-image",
    "title": "Docker",
    "section": "DS docker image",
    "text": "DS docker image\nСоздаем DockerFile\ntouch DockerFile\nВыбираем базовый образ\nFROM nvidia/cuda:11.7.0-cudnn8-runtime-ubuntu20.04\nДобавляем conda:\nRUN apt update && \\\n    apt install -y curl gpg && \\\n    curl https://repo.anaconda.com/pkgs/misc/gpgkeys/anaconda.asc | \\\n    gpg --dearmor > /etc/apt/trusted.gpg.d/anaconda.gpg && \\\n    echo 'deb https://repo.anaconda.com/pkgs/misc/debrepo/conda stable main' > /etc/apt/sources.list.d/conda.list && \\\n    apt update && \\\n    apt install -y conda graphviz && \\\n    touch ~/.bashrc && \\\n    /opt/conda/bin/conda init bash\nСоздаем окружение и устанавливаем основные зависимости:\nRUN . ~/.bashrc && \\\n    conda install -y -n base -c conda-forge mamba  && \\\n    mamba create -y --name research python=3.10.6  && \\\n    mamba install -y -n research -c conda-forge -c bioconda snakemake conda-build gcc git && \\\n    mamba clean -ya\nСоздаем рабочую директорию\nWORKDIR /workspace\nУстанавливаем poetry зависимости (опциональный шаг)\nCOPY pyproject.toml poetry.lock ./\nRUN . /opt/conda/etc/profile.d/conda.sh &&\\\n    conda activate research &&\\\n    pip install poetry==1.2.2 && \\\n    poetry config virtualenvs.in-project false && \\\n    poetry config virtualenvs.path /opt/conda/envs && \\\n    poetry install --no-root --no-interaction --no-ansi\n\nДля создания образа, нужно создать docker file. Затем указать какой образ будет использоваться, если нужны видеокарты, то стоит использовать образ от nvideo. В нем есть cuda, cudnn и поддержка видеокарт.\nДалее устанавливаем conda, это не очень просто, потому что нужно сделать несколько магических шагов с добавлением репозитория conda. А также инициализация conda в bash shell.\nПосле этого можно создать в ней окружение, например, окружение и поставить основные зависимости типа python, git, gcc и других пакетов которые потребуются для сборки библиотек.\nПосле этого можно создать рабочую директорию.\nТак же можно посмотреть на опциональный шаг - установка зависимостей poetry. Можно скопировать файлы зависимостей и установить их. Но так делать не рекомендую, так как размер базовоо образа для вашего решения увеличиться в разы. Как же с этим работать?"
  },
  {
    "objectID": "6.docker.html#как-использовать-это",
    "href": "6.docker.html#как-использовать-это",
    "title": "Docker",
    "section": "Как использовать это?",
    "text": "Как использовать это?\nСборка образа\ndocker build -t research_image .\nЗапуск контейнера\ndocker run -it --gpus all -v <path/to/project>:/workspace/project --name research research_image /bin/bash\nДобавить запуск jupyter\n#!/bin/bash\n\nsource /opt/conda/etc/profile.d/conda.sh\nconda activate snakemake\n# Run jupyetr lab\njupyter lab --ip 0.0.0.0 --port 8888 --no-browser --allow-root\nВключить его в docker\nENV JUPYTER_TOKEN=\"password\" \\\n    PATH=\"/opt/conda/bin/conda:$PATH\" \\\n    SHELL=\"/bin/bash\"\nCOPY docker/entrypoint.sh /opt/docker/bin/entrypoint.sh\nENTRYPOINT [ \"/bin/bash\", \"/opt/docker/bin/entrypoint.sh\" ]\nПодключиться из vscode\n\nDev containers > Attach to Running Container > research\n\n\nДля начала нужно собрать образ, после этого его запустить. Когда вы его запустите то попадете в его консоль.\nИспользовать так не очень практично. Лучше установить в него jupyter и настроить его запуск через entrypoint.sh.\nЛучшим вариантом будет подключиться к нему из vscode и таким образом вы будете работать через vscode внутри контейнера. Очень удобный инструмент, который позволит изолировать вашу работу и сделать ее более воспроизводимой."
  },
  {
    "objectID": "6.docker.html#выводы",
    "href": "6.docker.html#выводы",
    "title": "Docker",
    "section": "Выводы",
    "text": "Выводы\n\nИзоляция\nВоспроизводимость\nВерсионирование\nПередача\n\n\nПодведем итоги, с помощью docker вы можете изолировать свое окружение и спокойно в нем проводить эксперименты. Так же инструкции его создания зафиксированы и спокойно могут быть воспроизведены. Более того docker позволяет создать образы на основе контейнеров, можно создать новый образ и разместить его в реестре. Также docker позволяет версионировать образы на уровне тегов в реестре. Ну и таким образом его можно передать, более того можно выгрузить докер образ в архив и таким образом его передать.\nСогласитесь, докер удивительный и полезный инструмент."
  },
  {
    "objectID": "5.dependencies.html#почему-это-важно",
    "href": "5.dependencies.html#почему-это-важно",
    "title": "Dependency management",
    "section": "Почему это важно?",
    "text": "Почему это важно?\n\nRepeatability\nReplicability\nReusable\n\n\nСегодня мы поговорим о зависимостях и рядом с ними. Начнем с простого вопроса почему они важны?\nВы проводите исследование, пишите код и довольно часто используете библиотеки. Отсюда и появляется вся важность управления зависимостями. Во-первых, когда вы добавляете новую библиотеку, вам надо сказать об этом вашей команде, чтобы они тоже использовали эту библиотеку той же версии. Во-вторых, если вам нужно изменить версию уже используемой библиотеки вам надо сообщить об этом всей команде и проверить что с новой версией ваш код будет работать корректно. В-третьих, вам надо будет передавать информацию о библиотеках разработчикам, которые будут встраивать ваш пайплайн в сервис.\nВажными моментами управления зависимостями являются:\n\nПовторяемость - при переустановке библиотек у себя выв получаете тот же результат, что и ранее.\nВоспроизводимость - при каждой новой сборке окружения вами, вашими коллегами или ci-cd, выбираются и устанавливаются одни и те же зависимости.\nПереисползуемость - механизм управления зависимостями позволяет передавать информацию о выбранных версиях библиотек разработчикам.\n\nВам надо поддерживать свои зависимости и следить за их версиями для того чтобы ваши исследования были воспроизводимы и пере используемы. В ручном режиме это делать довольно проблематично, поэтому сообщество python (и не только его) разработало множество инструментов для этого. Сегодня о них и поговорим."
  },
  {
    "objectID": "5.dependencies.html#история-развития",
    "href": "5.dependencies.html#история-развития",
    "title": "Dependency management",
    "section": "История развития",
    "text": "История развития\n\n\nЧуть-чуть истории развития питона. Хотя Python появился примерно в 1990-х годах, ему потребовалось довольно много времени, чтобы адаптироваться к шаблонам, которые другие языки программирования уже использовали при распространении программного обеспечения. До 2000 года, чтобы распространять свою программу, вам практически приходилось загружать ее где-нибудь в Интернете, где вам нужно было указать точные инструкции по установке программного обеспечения в интерпретатор Python. Таким образом, сообщество запустило distutils. Это привело к появлению файлов setup.py, setup.cfg, содержащих инструкции по установке программного обеспечения и другие полезные метаданные. Сообществу Python требовалось нечто большее, чем distutils, поэтому был создан setuptools. Он был больше как оболочка над стандартной библиотекой. Я не буду вдаваться в подробности того, что предлагается по сравнению со стандартной библиотекой. Одной из ключевых важных функций была easy_install, которая позволяла вам устанавливать другие пакеты в ваш интерпретатор python.\nИ тут начался этап хаоса в зависимостях. Управление зависимостями быстро превратилось в хаос, поскольку сообщество python начало раскалываться, одни использовали distutils, а другие setuptools. Независимо от этого, все пакеты были собраны в одном месте, что вызывает проблему с корневым доступом. Внесение путаницы в процесс развертывания и отклонение от некоторых стандартных принципов безопасности, таких как никогда ничего не запускать от имени пользователя root. Возникла потребность в изолированной среде, и примерно в 2007 году появился virtualenv. Эта концепция позволяла разработчикам иметь несколько настроенных интерпретаторов Python для каждого проекта, изолируя интерпретатор Python от базовой установки пакетов в Python. Благодаря этому улучшилось управление зависимостями, что дало множество преимуществ, таких как экспорт среды и изоляция среды.\nСледующие шаги по упорядочению этого хаоса заключались в создании нового пакетного менеджера, ставшего стандартом. Pip был представлен и заменил причудливый easy_install, pip также представил концепцию requirements.txt, устанавливающую стандарт внутри сообщества Python. Файл может быть создан путем извлечения всех зависимостей из текущего окружения virtualenv. Позже, чтобы pip мог прочитать файл и воссоздать ту же виртуальную среду.\nДо 2017 pip и virtualenv заставляли разработчиков страдать, поскольку им приходилось использовать два отдельных инструмента для изоляции своей среды и управления своими зависимостями. Первым шагом по объединению этих инструментов стал pipenv. Несмотря на то, что pipenv был прекрасен для разработчиков, разрабатывающих приложения, он не очень помог библиотекам. Теперь вам нужно обрабатывать еще больше файлов Pipfile, Pipfile.lock, setup.py и т. Д. поэтому на его смену пришел poetry.\nPoetry - это полноценный менеджер пакетов, который предлагает больше, чем просто управление зависимостями и упаковку. Он также пытается обеспечить соблюдение таких стандартов, как семантическое управление версиями, структура папок и шаблоны упаковки, с которыми, скорее всего, знакомы программисты из других сообществ.\nТеперь разберем пару инструментов, которые упоминали. Сразу скажу, что мы не будем рассматривать первые инструменты управления зависимостями в питоне, а перейдем к более актуальным."
  },
  {
    "objectID": "5.dependencies.html#install-packages",
    "href": "5.dependencies.html#install-packages",
    "title": "Dependency management",
    "section": "Install packages",
    "text": "Install packages\nУстановка пакетов.\npip install <package1> <package2>\nУстановка пакета определенной версии.\npip install <package1==version1> '<package2>=version2>'\nУстановка пакетов из requirements.txt.\npip install -r requirements.txt\nОбновление уже установленного пакета.\npip install --upgrade <package>\nИспользование приватного индекса пакетов.\npip install --index-url <index-url> <package>\nУстановка пакетов из git.\npip install git+https://<git-package-url>@<version>\nУстановка пакетов с расширениями.\npip install <package>[<extras>]\nУстановка пререлизного пакета.\npip install --pre <package>\nУстановка своих пакетов в редактируемой режиме.\npip install --editable ./<package-path>\nУстановка своих пакетов из исходного кода.\npip install --no-binary <package>\n\nОсновные возможности по установке пакетов:\n\nвы можете установить один или несколько пакетов\nможно установить пакеты определенных версий выставив ограничения, или же установить ограничение на минимальную доступную версию\nустановить пакеты из файла зависимостей\nобновить уже установленный пакет\nустановить пакеты из приватных или отдельных репозиториев(индексов) пакетов\nУстановить версию из гита, с точностью до коммита или версии\nУстановить пакет с расширением\nУстановить пакет пререлизной версии\nустановить разрабатываемый пакет в редактируемом режиме для облегчения разработки\nустановить пакет из исходного кода."
  },
  {
    "objectID": "5.dependencies.html#dependecy-resolver",
    "href": "5.dependencies.html#dependecy-resolver",
    "title": "Dependency management",
    "section": "Dependecy resolver",
    "text": "Dependecy resolver\n\nПоявился backtracking dependencies\nУвеличена строгость резолвера\nПоявился state managment\nУлучшилась поддержка constraints\n\n\n\nбета-версия появилась в 20.2 (2020-07-29)\nстабильная версия появилась в 20.3 (2020-11-30)\n\nДалее он развивался в течении двух лет и достиг довольно неплохих результатов.\nВ этом dependecy resolver появилась поддержка отката версий зависимости, если не находится кандидат, то версии зависимостей могут быть понижены для разрешения зависимостей. Ранее это игнорировалось и ставился первый попавшийся кандидат.\nТак же теперь resover запрещает ставить несовместимые пакеты и выдает ошибку, ранее он выдавал только warning. Так же он начал обращать свое внимание на constraints и учитывать их при поиске кандидатов и установке зависимостей.\nНу и наконец-то в нем появился state managment, то есть он начал адекватно работать текущее окружение начало влиять на выбор совместимых кандидатов.\nС появлением этого апдейта сборки у pip стали воспроизводимы и повторяемы. В целом текущее развитие pip dependecy resolver направлено на улучшение воспроизводимости окружения. Главное не используйте pip моложе 20.3 версии, там практически нет dependecy resolver."
  },
  {
    "objectID": "5.dependencies.html#requirement-dependecies",
    "href": "5.dependencies.html#requirement-dependecies",
    "title": "Dependency management",
    "section": "Requirement dependecies",
    "text": "Requirement dependecies\nЧто бы зафиксировать требуемые зависимости можно:\npip freeze > requirements.txt\n\n\n\n\nСтрогая фиксация requirements.txt:\ncertifi==x.y.z\ncharset-normalizer==x.y.z\nidna==x.y.z\nrequests==x.y.z\nurllib3==x.y.z\n\nТонкая настройка requirements.txt:\ncertifi>=x.y.z\ncharset-normalizer>=x.y.z\nidna>=x.y.z\nrequests>=x.y.z\nurllib3>=x.y.z\n\n\n\nКогда вы делитесь своим проектом Python с другими разработчиками, вы можете захотеть, чтобы они использовали те же версии внешних пакетов, что и вы. Возможно, конкретная версия пакета содержит новую функцию, на которую вы полагаетесь, или версия пакета, которую вы используете, несовместима с предыдущими версиями. Поэтому их стоит фиксировать.\nВ pip они фиксируются в иде списка пакетов с их версиями в фале requirements. txt. Pip по сути делает слепок текущего окружения.\nПо умолчанию Pip строго фиксирует версии библиотек, как это показано на слайде. Проблема строгого кодирования версий и зависимостей ваших пакетов заключается в том, что пакеты часто обновляются с исправлениями ошибок и безопасности. Вы, вероятно, захотите использовать эти обновления, как только они будут опубликованы. К сожалению, в pip нет возможности тонка настроить requirements, и нужно их корректировать ручками после каждого pip freeze."
  },
  {
    "objectID": "5.dependencies.html#devprod-dependecies",
    "href": "5.dependencies.html#devprod-dependecies",
    "title": "Dependency management",
    "section": "Dev/Prod dependecies",
    "text": "Dev/Prod dependecies\nЗависимости для развертывания\n# requirements.txt\npackage==1.0\npackage==1.0\npackage==1.0\nЗависитмости для разработки\n# dev_requirements.txt\n-r requirements.txt\ndev_package==1.0\ndev_package==1.0\ndev_package==1.0\n\nЕще важный момент это разделение prod и dev зависимостей. И в pip это реализовано очень плохо. Автоматического разделения нет и придётся все зависимости разделять ручками, с учетом, что бывает 2 или 3, а то и больше, уровней зависимостей. Соответственно после каждого freeze нужно самому разделить их на два файла."
  },
  {
    "objectID": "5.dependencies.html#proscons",
    "href": "5.dependencies.html#proscons",
    "title": "Dependency management",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\ndefault tool\ndeterministic builds\ndependecy resolver\n\n\nCons\n\nno built-in isolation tool\nproblematical dev and prod dependency split\nno tools for lib packaging and publishing\nbuildings from source\n\n\n\n\nC версии 20.3 появились первые значимые плюсы. Появились воспроизводимые сборки и инструмент разрешения зависимостей, выше мы обсудили какой большой impact они вносят в работу пакетного менеджера.\nНет инструмента для изоляции окружения проекта. Это не сильно плохо, но тоже является проблемой. Так как вам надо самому создать виртуальное окружение используя для этого отдельный инструмент.\nНет возможности разделить зависимости на dev и prod, у вас в одном файле будут содержаться и инструменты разработчика линтеры, форматеры и анализаторы кода, в том числе и jupyter notebook, и продовские зависимости numpy, sklearn и другие. Это очень сложно разобрать и поддерживать. Кроме этого у вас в одном файле хранятся все ваши зависимости и зависимости зависимостей и так далее, то есть очень много технического мусора, который вы напрямую не используете. В итоге у вас есть один файл — большая помойка зависимостей.\nНет инструментов для сборки и публикации пакетов. То есть на вас перекладывается проблема по поддержанию структуры проекта, файлов конфигурации сборки и продовских зависимостей. Всем этим вам придется заниматься вручную и это может стать большой проблемой когда проект станет довольно большим.\nСборка из исходного кода. Не могу сказать, что это большая проблема, пока мы говорим про исходный код на питоне. Но, так как питон не очень быстрый язык, то часто используются библиотеки, написанные на C, особенно в data science. Наиболее популярны библиотеки уже имеют бинарные сборки, помещенные в репозиторий пакетов, но не дай бог вам попадется библиотека, которую и придется собирать из исходного кода с элементами C. Тогда ваш проект будет зависеть не только от питоновского окружения, но и от окружения для С, а это не сильно приятно. Так как если у вас другая версия компилятора, то проект не соберется. Ради интереса можете попробовать собрать tensorflow и прочувствовать всю боль."
  },
  {
    "objectID": "5.dependencies.html#conda-packages",
    "href": "5.dependencies.html#conda-packages",
    "title": "Dependency management",
    "section": "Conda packages",
    "text": "Conda packages\n\n\nСтруктура пакета\n.\n├── bin\n│   └── pyflakes\n├── info\n│   ├── LICENSE.txt\n│   ├── files\n│   ├── index.json\n│   ├── paths.json\n│   └── recipe\n└── lib\n    └── python3.5\n\nПоиск пакетов:\nconda search <package>\nУстановка пакетов:\nconda install <package>\nСборка пакетов с помощью conda-build:\nconda build <my_package>\n\n\n\nПакет conda представляет собой сжатый файл tarball (.tar.bz2) или файл .conda, который содержит:\n\nбиблиотеки системного уровня.\nPython или другие модули.\nисполняемые программы и другие компоненты.\nметаданные в info/ каталоге.\nнабор файлов, которые устанавливаются непосредственно в install префикс.\n\nЕсли смотреть на структуру пакета, то в данные распределяются так:\n\nbin содержит соответствующие двоичные файлы для пакета.\nlib содержит соответствующие файлы библиотек (например, файлы .py).\ninfo содержит метаданные пакета.\n\nConda отслеживает зависимости между пакетами и платформами. Формат пакета conda идентичен для разных платформ и операционных систем.\nТак же хочется упомянуть про noarch пакеты. Пакеты Noarch — это пакеты, которые не зависят от архитектуры и поэтому должны быть собраны только один раз. Пакеты Noarch являются либо общими, либо Python. Общие пакеты Noarch позволяют пользователям распространять документы, наборы данных и исходный код в пакетах conda. Пакеты Noarch Python сокращают накладные расходы на создание нескольких разных чистых пакетов Python для разных архитектур и версий Python за счет сортировки различий, зависящих от платформы и версии Python, во время установки."
  },
  {
    "objectID": "5.dependencies.html#conda-channels",
    "href": "5.dependencies.html#conda-channels",
    "title": "Dependency management",
    "section": "Conda channels",
    "text": "Conda channels\nУстановка из определенного channel\nconda install scipy --channel conda-forge\nДобавление channel по умолчанию\nconda config --add channels new_channel\nСписок доступных channels:\nanaconda\nr\nconda_forge\nbioconda\nastropy\nmetachannel\njavascript\nprivate\n\nСобственно, у нас есть пакеты, и появляется закономерный вопрос, а откуда их брать? Тут нам помогут conda channels.\nУ conda есть возможность искать пакеты из разных каналов, так же можно указывать приоритете поиска по каналам, добавлять или удалять каналы. Я привел небольшой список каналов сообщества conda. Наиболее обширный канал это conda-forge. В этот канал публикуется большая часть питоновских пакетов из pypi с помощью сообщества. Очень советую им пользоваться.\nБольшим плюсом является возможность поднятие собственных каналов, в том числе зеркал существующих. То есть, если вы, например, работаете в компании, то вы можете в контуре компании скачивать все эти пакеты в каналы и устанавливать из них. То есть на стороне компании будет находиться весь набор необходимых зависимостей для сборки вашего проекта."
  },
  {
    "objectID": "5.dependencies.html#conda-environments",
    "href": "5.dependencies.html#conda-environments",
    "title": "Dependency management",
    "section": "Conda environments",
    "text": "Conda environments\nСоздание пустого окружения\nconda create --name <myenv>\nСоздание питоноского окружения\nconda create -n <myenv> python=3.9\nУстановка пакета в определенное окружение\nconda install -n <myenv> <package>\nСоздание файла зависимостей\n# только те которые были устанволены в ручную\nconda env export --from-history > environment.yml\n# все зависимости и сабзависимости\nconda env export > environment.yml\nСоздание окружения из файла зависимостей\nconda env create -f environment.yml\nОбновление окружения из файла зависимостей\nconda env update -f environment.yml\n\nЗамечу что в этом инструменте есть нативная поддержка изолированных окружений. Куда лучше, чем питоновские, которые формируются на основе слепка интерпретатора. Здесь в каждом окружение может находиться несколько версий интерпретатора питона для нормальной работы всех зависимостей. То есть conda рассматривает сам питон как зависимость.\nУдобно то что все окружения хранятся централизована в кеше конды, есть возможность создавать файлы зависимостей как всех установленных библиотек, так и фиксировать версии только тех библиотек, которые вы сами устанавливали. Так же на основе такого файла можно создать у себя окружение с нуля или обновить существующее.\nКонда предлагает отличный инструмент менеджмента виртуальных окружений. Я сам им постоянно пользуюсь, как минимум потому что питон в конде изолирован от системного полностью, и я могу легко менять его версии при необходимости."
  },
  {
    "objectID": "5.dependencies.html#proscons-1",
    "href": "5.dependencies.html#proscons-1",
    "title": "Dependency management",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\ndeterministic builds\ndependecy resolver\nbuilt-in isolation tool\ntools for lib packaging and publishing\nbuilded binary packages\n\n\nCons\n\nvery slow dependency resolver\na lot of steps for preparing package build\nno dependencies split\n\n\n\n\nОсновными достоинствами conda являются детерминированные сборки, разрешение конфликтов зависимостей, встроенный менеджер виртуальных окружений, инструменты для упаковки и сбор пакетов. Самое главное, что в conda используются уже собранные бинарные пакеты, что сильно сокращает время работы при установке зависимостей и не требует повторных сборок из исходников."
  },
  {
    "objectID": "5.dependencies.html#project-structure",
    "href": "5.dependencies.html#project-structure",
    "title": "Dependency management",
    "section": "Project structure",
    "text": "Project structure\n\n\nСтруктура проекта\npoetry-demo\n├── pyproject.toml\n├── poetry.lock\n├── README.md\n├── poetry_demo\n│   └── __init__.py\n└── tests\n    └── __init__.py\nИнициализация проекта\npoetry new <project-path>\n\nPyproject.toml\n[tool.poetry]\nname = \"poetry-demo\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"Kovalenko Lev\"]\nreadme = \"README.md\"\npackages = [{include = \"poetry_demo\"}]\n \n[tool.poetry.dependencies]\npython = \"^3.9\"\n \n \n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\n\nВо-первых, poetry добавляет свою структуру проекта, в которой создаются основные директории, а также файлы для описания зависимостей и метаинформации проекта. Для создания нового poetry проекта, можно вызвать одну команду и получить готовый шаблон.\nСтоит уделить вниманию что poetry использует Pyptojeсt file согласно PEP 621 о хранении метаинформации. В нем poetry фиксирует зависимости проекта.\nТакже poetry создает poetry.lock файл, в котором хранится информация о том, какие нужны зависимости, зависимости зависимостей и так далее для сборки окружения, из каких источников ставить эти зависимости."
  },
  {
    "objectID": "5.dependencies.html#dependency-install",
    "href": "5.dependencies.html#dependency-install",
    "title": "Dependency management",
    "section": "Dependency install",
    "text": "Dependency install\nУстановка пакета\npoetry add <package>\nУстановка пакета определенной версии\n# Allow >=2.0.5, <3.0.0 versions\npoetry add package@^2.0.5\n# Allow >=2.0.5, <2.1.0 versions\npoetry add package@~2.0.5\n# Allow >=2.0.5 versions, without upper bound\npoetry add \"package>=2.0.5\"\n# Allow only 2.0.5 version\npoetry add package==2.0.5\nУстановка пакета из git\npoetry add git+https://github.com/sdispater/package.git#version\nУстановка своих пакетов в редактируемой режиме\npoetry add ---editable ./<package-path>\nДобавление зависимости в определенную группу\npoetry add <package> --group <group>\n# для dev зависимостей\npoetry add <package> --group dev\npoetry add --dev <package>\nУстановка пререлизных зависимостей\npoetry add --allow-prereleases <package>\nУстановка зависимостей из различных репозиториев\npoetry source add <source> https://<source-link>\npoetry add --source <source> <package>\nУстановка расширений зависимостей\npoetry add \"<package>[<extras>]\"\nПосмотреть наличие обновлений\npoetry show --tree\nПровести обновление зависимостей\n# обновить конкретную зависимость\npoetry add <package>@latest\npoetry update <package>\n# обновить под зависимости\npoetry update\n\nPoetry позваляет проводить все те же операции что и pip. Работать с разными зависимостями, ставить пределенные версии, устанавливать зависимости из разных источников, прерлизные зависимости, расширения зависимостей. В плане установки зависимостей он полностью идентичен pip.\nБолее того он позволяет группировать зависимости, что позволяет сделать dev/prod split или же выделять какие-то дополнительные зависимости для оптимизации в группы.\nТакже есть функционал для отслеживания обновлений зависимостей и самого обновления зависимостей."
  },
  {
    "objectID": "5.dependencies.html#package-build-and-publish",
    "href": "5.dependencies.html#package-build-and-publish",
    "title": "Dependency management",
    "section": "Package build and publish",
    "text": "Package build and publish\nСборка пакетов локально\npoetry build\nПубликация собранного пакета\npoetry publish\nПубликация и сборка пакета\npoetry publish --build\nПубликация пакета в приватный репозиторий\npoetry publish -r <repo>\n\nТакже хочется отметить cli poetry для сборки и публикации пакетов. Он очень удобен и прост в использовании. Буквально одной командой можно собрать и опубликовать пакет и не нужно дополнительных плясок с бубнами c setup.py и прочим."
  },
  {
    "objectID": "5.dependencies.html#proscons-2",
    "href": "5.dependencies.html#proscons-2",
    "title": "Dependency management",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\ndeterministic builds\ndependecy resolver\ndependecies groups\nbuilt-in isolation tool\ntools for lib packaging and publishing\ndependencies update tracking\n\n\nCons\n\nbuildings from source\na very young tool\n\n\n\n\nПакетный менеджер собрал все основные best pracices. Единственные два минуса — если нет колес, то будет собирать из исходников, что бывает больно. А также это довольно молодой инструмент и у него встречаются различные баги, я сам встречал баги с работой с несколькими репозиториями, с приватными репозиториями. Думаю, со временем он будет развиваться и решит эти проблемы. Часть из них уже решил если что)"
  },
  {
    "objectID": "5.dependencies.html#выводы",
    "href": "5.dependencies.html#выводы",
    "title": "Dependency management",
    "section": "Выводы",
    "text": "Выводы\n\npip - дефолтный менеджер пакетов, соответствует всем pep\nconda - лидер по environments, позволяет работать только с бинарными пакетами\npoetry - собрал в себя все лучшие практики, но очень молодая технология\n\n\nМы рассмотрели с вами несколько пакетных менеджеров. У всех из них есть свои достоинства и недостатки. Но тут стоит для себя решить, что для вас критично, а что нет.\nЯ для себя и в своей команде использую связку conda и poetry. Conda нужна для работы с непитоновскими зависимостями и менеджмента виртуальных окружений. Poetry занимается установкой питоновских зависимостей и отвечает за сборку и публикацию библиотек."
  },
  {
    "objectID": "18.dvc.html#dvc-pipelines",
    "href": "18.dvc.html#dvc-pipelines",
    "title": "DVC",
    "section": "DVC pipelines",
    "text": "DVC pipelines\nPipeline\nstages:\n  prepare: ... # stage 1 definition\n  train: ... # stage 2 definition\n  evaluate: ... # stage 3 definition\nStage\nstages:\n  prepare:\n    cmd: source src/cleanup.sh\n    deps:\n      - src/cleanup.sh\n      - data/raw\n    outs:\n      - data/clean.csv\n\nКонвейеры представляют рабочие процессы данных, которые вы хотите надежно воспроизвести , чтобы результаты были согласованными. Типичный процесс конвейерной обработки включает в себя: - Добавить исходные данные в dvc - Определить этапы пайплайна - stages - Добавить выходные данные и дополнительные данные, например метрики, графики и параметры.\nStages обычно берут некоторые данные и запускают некоторый код, создавая выходные данные (например, модель ML). Конвейер формируется путем их взаимозависимости, что означает, что выходные данные одного этапа становятся входными данными другого и так далее. Технически это называется графом зависимостей (DAG).\nКаждый этап обертывает исполняемую команду оболочки и определяет любые файловые зависимости , а также выходные данные . Давайте посмотрим на примерный этап: он зависит от файла скрипта, который он запускает, а также от необработанных входных данных (в идеале уже отслеживаемых DVC)."
  },
  {
    "objectID": "18.dvc.html#stage-parametrs",
    "href": "18.dvc.html#stage-parametrs",
    "title": "DVC",
    "section": "Stage parametrs",
    "text": "Stage parametrs\nstages:\n  train:\n    cmd: ...\n    deps: ...\n    params: # from params.yaml\n      - learning_rate\n      - nn.epochs\n      - nn.batch_size\n    outs: ...\n\nБолее детализированным типом зависимости является параметр ( paramsполе dvc.yaml) или гиперпараметры в машинном обучении. Это любые значения, используемые внутри вашего кода для настройки обработки данных или иным образом влияющие на выполнение этапа. Например, для обучения нейронной сети обычно требуются значения batch size и epochs count\nВместо того, чтобы жестко задавать значения параметров, ваш код может считывать их из структурированного файла (например, в формате YAML). DVC может отслеживать любую пару ключ/значение в файле поддерживаемых параметров ( params.yamlпо умолчанию)."
  },
  {
    "objectID": "18.dvc.html#stage-outputs",
    "href": "18.dvc.html#stage-outputs",
    "title": "DVC",
    "section": "Stage outputs",
    "text": "Stage outputs\n\nFiles\nMetrics\nPlots\n\n\nВыходные данные этапа — это файлы (или каталоги), полученные после исполнения пайплайна, например модели машинного обучения, промежуточные артефакты, а также графики и метрики. Эти файлы кэшируются DVC автоматически и отслеживаются с помощью dvc.lock файлов (или .dvc файлов).\nВыходы могут быть зависимостями последующих этапов (как объяснялось ранее). Поэтому, когда они изменяются, DVC может также потребоваться воспроизвести последующие этапы (обрабатывается автоматически).\nТипы выходов:\n\nФайлы и каталоги: обычно данные для передачи на промежуточные этапы, а также окончательные результаты пайплайна (например, набор данных или модель машинного обучения).\nМетрики : DVC поддерживает небольшие текстовые файлы, которые обычно содержат метрики производительности модели на этапах оценки, проверки или тестирования жизненного цикла машинного обучения. DVC позволяет сравнивать полученные метрики друг с другом с помощью dvc metrics diff и представляет результаты в виде таблицы с помощью dvc metrics show или dvc exp show.\nГрафики : различные виды данных, которые можно изобразить в виде графика. Например, сравните статистику производительности машинного обучения или непрерывные показатели из нескольких экспериментов. dvc plots show может создавать диаграммы для определенных файлов данных или отображать для вас пользовательские файлы изображений, или вы можете сравнивать разные файлы с файлами dvc plots diff."
  },
  {
    "objectID": "18.dvc.html#proscons",
    "href": "18.dvc.html#proscons",
    "title": "DVC",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\nНе зависит от языка\nDAG на основе файлов\nПравила рядом с кодом\nПростая параметризация\nYaml формат\nВресионирование результатов\nМониторинг эксперимента\n\n\nCons\n\nСложное разбиение на модули\nDAG deps пишутся вами\nНет виртуальных окружений\nНет ограничений на ресурсы\nНет распределенных вычеслений"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Engineering practices in ML",
    "section": "",
    "text": "Для удобства коммуникаций предлагаю общаться в Telegram.\n\n\n\\[0.4\\cdot\\text{ДЗ} + 0.4\\cdot\\text{ФП} + 0.2\\cdot\\text{Т}\\]\nДЗ - домашние задания.\nФП - финальная презентация.\nТ - Теоретический вопрос.\nСводная таблица будет обновляться.\n\n\n\n\nВыбрать и опубликовать ML проект на GitHub.\nПеревести все jupyter notebook’и в скрипты (если это уже существующий проект, можете оставить jupyter notebook, но также создайте папку со скриптами).\nЗаполнить информацию о себе в сводной таблице\n\nДля тех, у кого нет своего проекта, советуем ознакомиться со следующими исследованиями, которые вы можете взять в качестве своего проекта, либо же составить свое исследование на их основе:\n\nA Statistical Analysis & ML workflow of Titanic\nTitanic: A complete approach for Data Scientists\nTitanic - Advanced Feature Engineering Tutorial\nExploratory Tutorial - Titanic\n\nДанное домашнее задание оценивается на 10 баллов, если оно сделано в срок. За каждый день опоздания в сдаче \\(-1\\) от исходной оценки. Дедлайн 15.11.2022.\n\n\n\n\nВыбрать пакетный менеджер.\nСохранить зависимости в поддерживаемом формате. (2 балла)\nРазделить dev и prod зависимости. (2 балла)\nНастроить сборку пакета и публикацию в pypi-test. (2 балла)\nОписать шаги в readme.md: (4 балла)\n\nустановка пакетного менеджера.\nразвертывание окружения.\nсборка пакета.\nссылка на пакет в pypi-test.\nустановка пакета из pypi-test.\n\n\nРезультатом работы будет добавление файлов зависимостей в репозиторий, а также описания. Если в проект не будет добавлено файлов, то оценить его не представляется возможным. Дедлайн 22.11.2022.\n\n\n\n\nОтформатировать код с помощью isort и black/autopep8/yapf.\nВыбрать набор плагинов для flake8 (от 5 штук).\nЗаписать выбранные форматеры, линтеры и плагины в readme.md. (1 балла)\nЗафиксировать настройки форматера и линтера в pyproject.toml или setup.cfg (1 балла)\nНастроить и добавить pre-commit в проект. (1 балла)\nПровести анализ кода с помощью flake8 и плагинов и зафиксировать проблемы в файле linting.md (1 балла)\nПровести рефакторинг выявленных проблем. (3 балла)\nНаписать плагин для одной из проблем кода, опубликовать его и добавить в зависимости (3 балла)\n\nОтсутствие type hint в сигнатуре методов и функций.\nИспользование циклов для обработки матричных операции pandas и numpy.\nСокращенные названия переменных и одного, двух или трех символов.\nИспользование цикла, там где можно использовать listcomprehension.\nСоздание переменных в глобальной области.\nPandas view assign вместо использования .loc.\nИспользование тренарного оператора в тренарном операторе.\n\n\nРезультатом работы будет добавлением файла со списком выявленных проблем, описанием используемых линтеров, внесение изменений в код (рефакторинг) и добавление ссылки на репозиторий своего линтера. Если в проект не будет добавлено файлов, то оценить его не представляется возможным. Дедлайн 29.11.2022.\n\n\n\n\nВыбрать инструмент для версионирования данных.\nПодключить к нему публичное хранилище данных.\nДобавить данные в него и начать их версионировать.\nДобавить модели и полученные метрики в него.\n\nРезультатом добавление кода/файлов для использования системы контроля данных Данное домашнее задание оценивается на 10 баллов, если оно сделано в срок. За каждый день опоздания в сдаче \\(-1\\) от исходной оценки. Дедлайн 06.12.2022.\n\n\n\n\nВыбрать инструмент для построения пайплайнов:\n\nMake\nSnakemake\nDVC\nAirflow\n\nРеализовать CLI для своих экспериментов (3 балла)\nОписать dag для вашего пайплайна обучения (4 балла)\nПрописать в readme как запускать ваш пайплайн (3 балла)\n\nРезультатом добавление кода/файлов для использования системы контроля данных. Данное домашнее задание оценивается на 10 баллов, если оно сделано в срок. За каждый день опоздания в сдаче \\(-1\\) от исходной оценки. Дедлайн 18.12.2022.\n\n\n\n\nВ чем заключается кризис воспроизводимости? Какие бывают причины не воспроизводимости исследования?\nПравила проведения воспроизводимых исследований. Как они помогают достичь воспроизводимости?\nКакие общие артефакты у команды исследователей? Как организовать рабочий процесс в команде?\nКак проблемы решает управление зависимостями? Какие есть наилучшие практики dependecy management?\nЧто такое docker? Как он может пригодиться DS исследователю?\nЗачем в DS проекте поддерживать codestyle? Какие есть инструменты для этого?\nЧто такое версионирование данных? Какие проблемы оно позволяет решить?\nПо каким параметрам можно сравнить два инструмента контроля версий? Приведите пример такого сравнения.\nВ чем разница между пространственным хранением версий данных и git-like? Какие достоинства и недостатки есть у каждого из них.\nКакие задачи решают workflow managers? Каким образом можно сравнивать такие инструменты? Приведите пример сравнения.\nКак писать отчет по проекту?"
  },
  {
    "objectID": "17.snakemake.html#section",
    "href": "17.snakemake.html#section",
    "title": "Snakemake",
    "section": "",
    "text": "Система управления рабочим процессом Snakemake — это инструмент для создания воспроизводимых и масштабируемых задач анализа данных. Workflow описываются с помощью DSL языка на основе Python. Их можно легко масштабировать для серверов, кластеров, сетевых и облачных сред без необходимости изменять определение рабочего процесса. Наконец, рабочие процессы Snakemake могут включать в себя описание необходимого программного обеспечения, которое будет автоматически развернуто в любой среде выполнения.\nSnakemake следует парадигме GNU Make: рworkflow определяются с точки зрения правил, которые определяют, как создавать выходные файлы из входных файлов. Зависимости между правилами определяются автоматически, создавая DAG заданий, которые можно автоматически распараллеливать."
  },
  {
    "objectID": "17.snakemake.html#syntax",
    "href": "17.snakemake.html#syntax",
    "title": "Snakemake",
    "section": "Syntax",
    "text": "Syntax\nSnakemake rule\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/A.fastq\"\n    output:\n        \"mapped_reads/A.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - > {output}\"\nStart snakemake\nsnakemake -np mapped_reads/A.bam\n\nSnakemake отличается от других текстовых систем рабочего процесса следующим образом. Подключаясь к интерпретатору Python, Snakemake предлагает язык определения, который является расширением Python .с синтаксисом для определения правил и конкретных свойств рабочего процесса. Это позволяет Snakemake сочетать гибкость простого языка сценариев с определением рабочего процесса на основе Python."
  },
  {
    "objectID": "17.snakemake.html#wildcards",
    "href": "17.snakemake.html#wildcards",
    "title": "Snakemake",
    "section": "Wildcards",
    "text": "Wildcards\nSnakemake rule\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    shell:\n        \"bwa mem {input} | samtools view -Sb - > {output}\"\nStart snakemake\nsnakemake -np mapped_reads/B.bam\n\nалгоритм планирования Snakemake может быть ограничен приоритетами, предоставленными ядрами и настраиваемыми ресурсами, а также обеспечивает общую поддержку распределенных вычислений (например, кластерных или пакетных систем). Следовательно, рабочий процесс Snakemake масштабируется без изменений от одноядерных рабочих станций и многоядерных серверов до кластерных или пакетных систем."
  },
  {
    "objectID": "17.snakemake.html#configuration",
    "href": "17.snakemake.html#configuration",
    "title": "Snakemake",
    "section": "Configuration",
    "text": "Configuration\nSnakemake rule\nconfigfile: \"config.yaml\"\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - > {output}\"\nconfig.yaml\nsamples:\n    A: data/samples/A.fastq\n    B: data/samples/B.fastq\nStart snakemake\nsnakemake -np bcftools_call"
  },
  {
    "objectID": "17.snakemake.html#additional-features",
    "href": "17.snakemake.html#additional-features",
    "title": "Snakemake",
    "section": "Additional features",
    "text": "Additional features\nBenchmark\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        lambda wildcards: config[\"samples\"][wildcards.sample]\n    output:\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=\"@RG\\tID:{sample}\\tSM:{sample}\"\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    benchmark:\n        \"benchmarks/{sample}.bwa.benchmark.txt\"\n    threads: 8\n    shell:\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - > {output}) 2> {log}\"\n\nModules\ninclude: \"path/to/other.snakefile\"\ninclude: \"path/to/other.smk\"\n\n\nConda enviroments\nrule samtools_index:\n  input:\n      \"sorted_reads/{sample}.bam\"\n  output:\n      \"sorted_reads/{sample}.bam.bai\"\n  conda:\n      \"envs/samtools.yaml\"\n  shell:\n      \"samtools index {input}\""
  },
  {
    "objectID": "17.snakemake.html#clusters",
    "href": "17.snakemake.html#clusters",
    "title": "Snakemake",
    "section": "Clusters",
    "text": "Clusters\n\nКластера\nKubernetes\nGoogle cloud\nAWS\n\n\nПо умолчанию Snakemake выполняет задания на локальном компьютере, на котором он запущен. Кроме того, он может выполнять задания в распределенных средах, например, в вычислительных кластерах или пакетных системах . Если узлы используют общую файловую систему, Snakemake поддерживает три альтернативных режима выполнения. так же он может работать с данными из s3 хранилища. Кроме класетров, есть поддержка kubernetess и некоторых облаков. Snakemake может выполнять задачи на них."
  },
  {
    "objectID": "17.snakemake.html#proscons",
    "href": "17.snakemake.html#proscons",
    "title": "Snakemake",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\nНе зависит от языка\nDAG на основе файлов\nПравила рядом с кодом\nРаспределенное выполнение\nЕсть работа с ограничениями\nPython like синтаксис\nИнтегрирован с conda и docker\nМодульность\nПараметризация на основе wildcards\n\n\nCons\n\nСложно осознать некоторые моменты\nОграничение по ресурсам фиктивно\nНе централизованные распределенные вычесления"
  },
  {
    "objectID": "2.replication_crisis.html",
    "href": "2.replication_crisis.html",
    "title": "Replication crisis",
    "section": "",
    "text": "Машинное обучение (МО) нашло применение в исследованиях всех областей науки и во многом заменило традиционную статистику. И хотя для анализа данных зачастую проще использовать именно МО, присущий этой технологии «подход чёрной коробки» вызывает серьёзные проблемы при интерпретации результатов.\nТермин «кризис воспроизводимости» означает, что тревожно большое количество результатов научных экспериментов не нашли своего подтверждения при проведении тех же манипуляций другими группами учёных. Это может означать, что результаты, полученные в ходе изначальных работ, ошибочны. Согласно данным одного анализа, до 85 % всех проведённых в мире исследовательских работ в области биомедицины не привели к значимым результатам."
  },
  {
    "objectID": "2.replication_crisis.html#научный-метод-проведения-исследований",
    "href": "2.replication_crisis.html#научный-метод-проведения-исследований",
    "title": "Replication crisis",
    "section": "Научный метод проведения исследований",
    "text": "Научный метод проведения исследований\n\n\n\n\n\n\n\n\n\nОбъективность\nВоспроизводимость\nВалидность\n\n\n\n\nСобственно в научном сообществе есть методология проведения исследований и познания. Называется она научный метод. Основными требованиями к которому являются объективность, воспроизводимость и валидность.\nОбъекти́вность — отношение к объекту (явлению) и его характеристикам, процессам, как к независимому от воли и желания человека. Объективность подразумевает наличие знаний как таковых об объекте (явлении). Устойчивость объективности зависит от количества и точности понимания различных параметров объекта и/или процессов явления.\nВоспроизводимость - главный принцип научного метода . Это означает, что результат, полученный в ходе эксперимента или наблюдательного исследования , должен быть снова достигнут с высокой степенью согласия, когда исследование повторяется с использованием одной и той же методологии разными исследователями. Только после одного или нескольких таких успешных повторений результат должен быть признан научным знанием.\nВали́дность — обоснованность и пригодность применения методик и результатов исследования в конкретных условиях. Более прикладное определение понятия «валидность» — мера соответствия методик и результатов исследования поставленным задачам.\nДавайте уделим чуть больше внимания воспроизводимости. Так как сегодня мы рассматриваем кризис воспроизводимости."
  },
  {
    "objectID": "2.replication_crisis.html#какая-бывает-воспроизводимость",
    "href": "2.replication_crisis.html#какая-бывает-воспроизводимость",
    "title": "Replication crisis",
    "section": "Какая бывает воспроизводимость1?",
    "text": "Какая бывает воспроизводимость1?\n\nПовторяемость измерений(также сходимость результатов измерений, англ. Repeatability)\nПовторяемость исследований (англ. Replicability) (Different team, same experimental setup)\nВоспроизводимость (англ. Reproducibility)\n\n\n\n\n\nНа основании определений Ассоциации вычислительной техники я предлагаю принять следующие определения:\nПовторяемость измерений(также сходимость результатов измерений, англ. Repeatability) (Same team, same experimental setup): Результат может быть получен с заявленной точностью одной и той же командой, используя одну и ту же процедуру измерения, одну и ту же измерительную систему, при одинаковых рабочих условиях, в одном месте при нескольких испытаниях. Для вычислительных экспериментов это означает, что исследователь может надежно повторить собственное вычисление.\nПовторяемость исследований (англ. Replicability) (Different team, same experimental setup): Результат может быть получено с заявленной точностью другой командой с использованием той же процедуры измерения, той же измерительной системы, в тех же рабочих условиях, в том же или другом месте при нескольких испытаниях. Для вычислительных экспериментов это означает, что независимая группа может получить тот же результат, используя наработки автора, вплоть до его реализаций.\nВоспроизводимость (англ. Reproducibility): Результат может быть получено с заявленной точностью другой командой, другой измерительной системой, в другом месте на нескольких испытаниях. Для вычислительных экспериментов это означает, что независимая группа может получить тот же результат, используя артефакты, которые они разрабатывают полностью независимо."
  },
  {
    "objectID": "2.replication_crisis.html#что-такое-воспроизводимое-исследованиеви",
    "href": "2.replication_crisis.html#что-такое-воспроизводимое-исследованиеви",
    "title": "Replication crisis",
    "section": "Что такое воспроизводимое исследование(ВИ)",
    "text": "Что такое воспроизводимое исследование(ВИ)\n\n«Цель воспроизводимых исследований - привязать конкретные инструкции к анализу данных и экспериментальным данным, чтобы исследование можно было воссоздать, лучше понять и проверить».\n\n\nВоспроизводимые исследования (Reproducible research) - это термин, используемый в некоторых областях исследований для обозначения определенного способа проведения анализа, который предоставляет:\n\nинструменты преобразующие необработанные данные и метаданные в обработанные данные;\nинструменты выполняющие анализ данных;\nинструменты агрегирующие анализы в отчет.\n\nВ некоторых случаях указывают конкретно какие инструменты и шаги имеются в виду, это может быть программный продукт, публикации, определенные лабораторные условия и т.д.\nКогда предоставляются объекты анализа, например данные, и инструменты, а также алгоритмы последовательного решения задачи, это позволяет другим исследователям:\n\nвыполнять анализ, о котором не сообщалось первыми исследователями;\nпроверить правильность исследований, выполненных первыми исследователями;"
  },
  {
    "objectID": "2.replication_crisis.html#в-чем-причина-кризиса-воспроизводимости",
    "href": "2.replication_crisis.html#в-чем-причина-кризиса-воспроизводимости",
    "title": "Replication crisis",
    "section": "В чем причина кризиса воспроизводимости?",
    "text": "В чем причина кризиса воспроизводимости?\n\n\n\nнедостаточное понимание алгоритма ML.\nнедостаточное знакомство с исходными данными.\nневерная интерпретация результатов.\n\n\n \n\n\n\nНедостаточное понимание алгоритма — очень распространённая проблема в машинном обучении. Это серьезная проблема при работе с нейросетями ввиду множества параметров (зачастую для глубоких нейросетей количество параметров может составлять миллионы). Помимо этих параметров надо принимать в расчёт и гиперпараметры — такие, как скорость обучения, метод инициализации, количество итераций, архитектура нейросети.\nДля решения проблемы мало осознать, что исследователь недостаточно хорошо понимает работу алгоритма. Как можно сравнить результаты, если в разных работах применялись отличающиеся по структуре нейронные сети? Многослойная нейронная сеть имеет очень сложную динамическую структуру. Поэтому даже добавление единственной переменной или смена одного гиперпараметра может значительно повлиять на результаты.\nПлохое понимание исходных данных также является серьёзной проблемой, но эта проблема существовала и во время работы с традиционными статистическими методами. Ошибки в сборе данных — такие, как ошибки квантования, неточности считывания и использование замещающих переменных — самые распространённые затруднения. Субоптимальные данные всегда будут проблемой, но понимать, какой алгоритм применить к какому типу данных — невероятно важно, это значительно повлияет на результат.\nОшибочная оценка результатов может быть весьма распространена в научном мире. Одна из причин — видимая корреляция не всегда отражает реальную взаимосвязь. Есть несколько причин, почему переменные А и B могут коррелировать:\n\nA может изменяться при изменении B;\nB может изменяться при изменении A;\nA и B могут изменяться при изменении общей базовой переменной, C;\nкорреляция A и B может быть ложной. Продемонстрировать корреляцию двух значений легко, гораздо сложнее определить её причину. Погуглив «spurious correlations» (ложная корреляция), вы найдёте весьма интересные и забавные примеры, имеющие статистическое значение:\n\nсобственно на слайде вы можете видеть некоторые из них, например расходы США на науку, космос и технологии явно коррелируют с самоубийствами путем повешения, а от количества разводов в Мэне зависит потребление маргарина на душу населения.\nВсё это может выглядеть забавными совпадениями, но смысл в том, что алгоритм машинного обучения, обработав эти переменные единым набором, воспримет их как взаимозависимые, не подвергая эту зависимость сомнению. То есть алгоритм будет неточным или ошибочным, поскольку ПО выделит в датасете паттерны, которых не существует в реальном мире. Это примеры ложных корреляций, которые в последние годы опасно распространились в связи с использованием наборов данных из тысяч переменных.\nТак же есть такие техники как p-hacking или форсирование корреляций.\nСуть p-hacking’а состоит в дотошном поиске в наборе данных статистически значимых корреляций и принятии их за научно обоснованные.\nЕщё одна проблема алгоритмов машинного обучения заключается в том, что алгоритм должен делать предположения. Алгоритм не может «ничего не найти». Это означает, что алгоритм либо найдёт способ интерпретировать данные независимо от того, насколько они соотносятся между собой, либо не придёт к какому-либо определённому заключению (обычно это означает, что алгоритм был неверно настроен или данные плохо подготовлены)."
  },
  {
    "objectID": "2.replication_crisis.html#правила-проведения-воспроизводимых-исследований",
    "href": "2.replication_crisis.html#правила-проведения-воспроизводимых-исследований",
    "title": "Replication crisis",
    "section": "Правила проведения воспроизводимых исследований",
    "text": "Правила проведения воспроизводимых исследований\n\nДля каждого полученного результата сохраните алгоритм его получения.\nИзбегайте этапов ручного управления данными или процессом.\nСохраните точные версии всех использованных внешних инструментов.\nИспользуйте контроль версий.\nХраните все промежуточные результаты в стандартизированном виде.\nДля алгоритмов использующих случайность записывайте их random_state.\nВсегда храните вместе с графиками данные.\nИерархический подход при генерировании результатов анализа.\nВсегда указывайте вместе текстовые утверждения и результаты исследования.\nОбеспечивайте доступность ваших результатов, данных и исследований.\n\n\n\nВажно знать каким образом вы получили те или иные результаты. Знание того, как вы перешли от необработанных данных к заключению, позволяет вам: защищать, обновлять, воспроизводить и передавать свои результаты исследований.\nМожет возникнуть соблазн открыть файлы данных в редакторе и вручную исправить пару ошибок форматирования или удалить выбросы. Кроме того, современные редакторы позволяют легко форматировать файлы огромных размеров. Однако соблазну сократить ваш алгоритм следует сопротивляться. Ручная обработка данных — это скрытая манипуляция.\nВам необходимо задокументировать выпуск и версию всего используемого программного обеспечения, включая операционную систему. Незначительные изменения в программном обеспечении могут повлиять на результаты. В идеале вы должны настроить виртуальную машину или контейнер со всем программным обеспечением, используемым для запуска ваших скриптов. Это позволяет сделать снимок вашей аналитической экосистемы, что упрощает воспроизведение ваших результатов.\nДля отслеживания версий ваших скриптов следует использовать систему контроля версий, такую ​​как Git. Вы должны пометить (сделать снимок) текущее состояние скриптов и ссылаться на этот тег во всех получаемых вами результатах. Если вы затем решите изменить свои алгоритмы, что вы обязательно сделаете, можно будет вернуться во времени и получить точные сценарии, которые использовались для получения заданного результата.\nЕсли вы соблюдаете Правило № 1, в теории уже возможно воссоздать любые результаты на основе необработанных данных. Однако хотя это может быть теоретически возможно, на практике могут быть ограничивающие факторы. Проблемы могут быть следующие:\n\n\nотсутствие ресурсов для запуска результатов с нуля (например, если использовались значительные вычислительные ресурсы кластера)\nотсутствие лицензий на некоторые инструменты, если использовались коммерческие инструменты\nнедостаточная техническая доступность некоторых инструментов\n\nВ этих случаях может быть полезно начать исследование с набора производных данных, которые уже могут представлять больше пользы или быть более удобными, чем необработанных данных. Хранение этих промежуточных наборов данных (например, в формате CSV) предоставляет больше возможностей для дальнейшего анализа и может упростить определение проблемных результатов, когда они ошибочны, поскольку нет необходимости все переделывать.\n\nОдна вещь, которую специалисты по данным часто не могут сделать — это установить исходные значения для своего анализа. Это делает невозможным точное воссоздание исследований машинного обучения. Многие алгоритмы машинного обучения включают стохастический элемент, и, хотя надежные результаты могут быть статистически воспроизводимыми, нет ничего, что можно было бы сравнить с теплым сиянием в глазах проверяющего при точном совпадении результатов.\nЕсли вы используете скриптовый язык программирования, ваши графики, скорее всего, автоматически. Однако если вы используете такой инструмент, как Excel, убедитесь, что вы сохранили начальные данные. Это позволяет не только воспроизвести график, но также более детально просмотреть лежащие в основе данные. Также стоит всегда сохранять алгоритмы, которые вы использовали для получения графиков, на основе которых вы потом приводите какие-либо утверждения.\nНаша задача как специалистов по обработке данных — обобщить данные в той или иной форме. Вот что включает в себя извлечение информации из данных. Однако резюмирование также является простым способом неправильного использования данных, поэтому важно, чтобы заинтересованные стороны могли разбить сводку на отдельные точки данных. Для каждого итогового результата укажите ссылку на данные, использованные для расчета итогового значения.\nВ конце работы результаты анализа данных оформляются в текстовом виде. И слова неточны. Иногда бывает трудно определить связь между выводами и анализом. Поскольку отчет часто является самой важной частью исследования, важно, чтобы его можно было связать с результатами и, в соответствии с правилом № 1, с исходными данными.\nВ коммерческих условиях может быть нецелесообразно предоставлять открытый доступ ко всем данным. Однако имеет смысл предоставить доступ другим пользователям в вашей организации. Облачные системы управления исходным кодом, такие как Bitbucket и GitHub, позволяют создавать частные репозитории, к которым могут получить доступ любые авторизованные коллеги."
  },
  {
    "objectID": "2.replication_crisis.html#заключение",
    "href": "2.replication_crisis.html#заключение",
    "title": "Replication crisis",
    "section": "Заключение",
    "text": "Заключение\nМашинное обучение в науке представляет проблему из-за того, что результаты недостаточно воспроизводимы. Однако учёные в курсе этой проблемы и работают над моделями ML, дающими более воспроизводимый и прозрачный результат. Настоящий прорыв произойдет, когда эта задача будет решена для нейросети.\nКак сказал физик Ричард Фейнман в своей речи перед выпускниками Калифорнийского технологического института в 1974 году:\n\n“Первый принцип науки заключается в том, чтобы не одурачить самого себя. И как раз себя-то одурачить проще всего.”"
  },
  {
    "objectID": "2.replication_crisis.html#список-источников",
    "href": "2.replication_crisis.html#список-источников",
    "title": "Replication crisis",
    "section": "Список источников",
    "text": "Список источников\n\nКризис машинного обучения в научных исследованиях\nReproducibility vs. Replicability: A Brief History of a Confused Terminology\nRepeatability, Repr)oducibility, and Replicability: Tackling the 3R challenge in biointerface science and engineering\n10 RULES FOR CREATING REPRODUCIBLE RESULTS IN DATA SCIENCE"
  }
]