[
  {
    "objectID": "4.git_workflow.html",
    "href": "4.git_workflow.html",
    "title": "Git workflow",
    "section": "",
    "text": "Вкратце рассмотрим структуру git - популярную системы контроля версий. Она позволяет совместно работать над проектом множеству разработчиков. В большинстве workflow есть некоторый единый удаленный репозиторий, который хранит в себе все изменения от всех разработчиков. У каждого разработчика есть локальный репозиторий, синхронизация репозиториев происходит с помощью push и fetch команд. Кроме локально репозитория у разработчика есть индекс (некоторая база отслеживающая файлы и изменения в них) и workspace. При создании нового файла его сначала нужно добавить в index командой git add, а потом создать коммит git commit. Откатиться на изменения в индексе или локальном репозитории можно командами checkout и checkout HEAD. Сравнить состояние workspace можно командами deff и diff HEAD. Команды pull и rebase позволяют подтягивать изменения с удаленного репозитория сразу и в локальный репозиторий, и в workspace. В целом это основной набор команд, который вам пригодиться и ими вы чаще всего будете пользоваться при работе с git."
  },
  {
    "objectID": "4.git_workflow.html#какие-workflow-существуют",
    "href": "4.git_workflow.html#какие-workflow-существуют",
    "title": "Git workflow",
    "section": "Какие workflow существуют?",
    "text": "Какие workflow существуют?\n\ngitflow\ngithub flow\nforking workflow\ndata science lifecycle process\n\n\nСегодня мы рассмотрим 4 подхода: git, github, forking flow подходы, используемые в разработке, data science lifecycle process это уже некоторая попытка адаптировать flow под исследования."
  },
  {
    "objectID": "4.git_workflow.html#gitflow",
    "href": "4.git_workflow.html#gitflow",
    "title": "Git workflow",
    "section": "Gitflow",
    "text": "Gitflow\n\n\n\n\n\n\nGitflow одна из первых методологий управления проектом направленное на периодические, не частые релизы. В ней есть основная ветка master, в которой хранятся все релизы. До релиза разрабатываемый код находится в ветке develop. Каждое нововведение (фичу) один разработчик разрабатывает в отдельной ветке, после этого она сливается с веткой develop. Когда выполнен весь объем фич для релиза, из develop создается ветка release, в которой ведется доработка релиза и потом она вливается в master и тегируется, а также вливается в develop. Теги отвечают за версии релизов. В случае обнаружение багов в релизе появляется ветка hotfix в которой исправляются ошибки и она снова вливается в master и develop.\nКак видите, методология достаточно тяжеловесная и применима в основном в enterprise разработке, так как она подстраивается под большинство фреймворков управления проектов по водопадной модели. В исследования применить такую методологию можно, но в результате вы получите очень много overhead для проекта по управлению ветками."
  },
  {
    "objectID": "4.git_workflow.html#github-flow",
    "href": "4.git_workflow.html#github-flow",
    "title": "Git workflow",
    "section": "Github flow",
    "text": "Github flow\n\n\n\n\n\n\nGithub flow это модификация gitflow предложенная github. Она направлена на снижение количества веток и упрощение поддерживания проекта. В ней оставаться две ветке - master и feature. Разработка новой функциональности проводится в отдельной фича-ветке, после этого происходит merge в master, после этого происходит новый релиз проекта.\nЭта методология построена для проектов, управляемых в различных гибких методологиях. Одной из важных рекомендаций авторов, является открытие merge request заранее и обсуждение в нем различных моментов заранее, до окончания работ. В целом методология достаточно легковесная и гибкая, может подойти для проведения исследований, но в исследовании нет потребности в такой частоте релизов на начальных этапах."
  },
  {
    "objectID": "4.git_workflow.html#forking-workflow",
    "href": "4.git_workflow.html#forking-workflow",
    "title": "Git workflow",
    "section": "Forking workflow",
    "text": "Forking workflow\n\n\n\n\n\n\nForking workflow это методология поверх github workflow для разработки opensource проектов. Основная сложность в opensource связана с тем, что изначально доступа к изменению в репозитории проекта у потенциального контрибьютора нет, поэтому ему предлагается следующая последовательность шагов:\n\nРазработчик «разветвляет» «официальный» серверный репозиторий. Это создает их собственную копию на стороне сервера.\nНовая серверная копия клонируются в их локальную систему.\nВ локальный клон добавлен удаленный путь Git для «официального» репозитория.\nСоздана новая локальная ветка функции.\nРазработчик вносит изменения в новую ветку.\nДля изменений создаются новые коммиты.\nВетвь помещается в собственную серверную копию разработчика.\nРазработчик открывает запрос на перенос из новой ветки в «официальный» репозиторий.\nЗапрос на перенос утверждается для слияния и объединяется в исходный серверный репозиторий.\n\nТаким образом организована разработка в opensource проектах. Могу сказать, что данная методология для исследований подходит плохо, из нее можно позаимствовать идею раздельных репозиториев, что если в команде много исследователей, то возможно их нужно разнести по разным репозиториям и периодически вытаскивать полезные нововведения в “официальный” репозиторий."
  },
  {
    "objectID": "4.git_workflow.html#data-branch",
    "href": "4.git_workflow.html#data-branch",
    "title": "Git workflow",
    "section": "Data branch",
    "text": "Data branch\n\n\nВводятся различные ветки, например есть data branches, в которых реализуется код для обработки данных, пишется отчет об этом и тесты для кода. На слайде представлена последовательность действий в такой ветке."
  },
  {
    "objectID": "4.git_workflow.html#explore-and-experiment-branches",
    "href": "4.git_workflow.html#explore-and-experiment-branches",
    "title": "Git workflow",
    "section": "Explore and experiment branches",
    "text": "Explore and experiment branches\n \n\nДля исследования данных предлагается использовать explore branches. Эти ветки остаются висеть в истории гита без мерджа, потому что код, который проводит исследование нужен разово, чтобы подтвердить или опровергнуть гипотезу.\nДля экспериментов есть две стратегии, если эксперимент опроверг подход, то такая ветка не получает merge request и просто оставаться висеть в истории, так как знания о неудачных экспериментах тоже важны и влияют на принятие следующих решений в развитии проекта.\nДля успешного эксперимента ветка переходит в ветку моделей."
  },
  {
    "objectID": "4.git_workflow.html#model-branches",
    "href": "4.git_workflow.html#model-branches",
    "title": "Git workflow",
    "section": "Model branches",
    "text": "Model branches\n\n\nКогда был найден удачный подход то появляется новая ветка model branch в которой происходит настройка и исследование модели, а также написание отчетов и тестов. После этого происходит влитие ветки в master branch и релиз модели."
  },
  {
    "objectID": "4.git_workflow.html#хорошая-методология",
    "href": "4.git_workflow.html#хорошая-методология",
    "title": "Git workflow",
    "section": "Хорошая методология?",
    "text": "Хорошая методология?\n\n\nПлюсы\n\nВ основной ветке только важный код.\nСохраняется информация о всех исследованиях.\nИмеет “логичные” разделения веток для разных задач.\n\n\nМинусы\n\nСложно автоматизировать воспроизведение всех исследований.\nИнформация об исследованиях “размазана” по репозиторию.\nВ теории выглядит хорошо, а на практике…\n\n\n\n\nМетодология конечно неплохая, но сильно оторвана от реальности. По факту в проекте большая часть работы связана с обработкой и подготовкой данных, так как чистые датасеты вы встретите только на kaggle, в реальности придется очень долго разбираться с данными и возвращаться к ним постоянно. Поэтому хотелось бы как-то просто обновлять результаты уже проведенных исследований.\nВ целом методология рабочая, но больше подходит для уже существующего проекта, где есть собранные и подготовленные датасеты и основной задачей является моделирование."
  },
  {
    "objectID": "4.git_workflow.html#выводы",
    "href": "4.git_workflow.html#выводы",
    "title": "Git workflow",
    "section": "Выводы",
    "text": "Выводы\n\nflow должен быть удобным команде\nне надо его перегружать, если нет необходимости\nиногда, не надо пытаться сразу объединять все результаты исследований\n\n\nСамое важное не то какую методологию вы выберите, а то что вы этой методологии будете следовать всей командой. Все эти git workflow направлены на унификацию процесса развития проекта с точки зрения гита.\nНа мой взгляд наиболее рабочей методологией будет github workflow, с оговоркой, что релизом будет не каждый merge в master, а тег. Все остальное с названиями веток и прочим, большой когнитивный оверхед для работы исследователей."
  },
  {
    "objectID": "5.dependencies.html#почему-это-важно",
    "href": "5.dependencies.html#почему-это-важно",
    "title": "Dependency management",
    "section": "Почему это важно?",
    "text": "Почему это важно?\n\nRepeatability\nReplicability\nReusable\n\n\nСегодня мы поговорим о зависимостях и рядом с ними. Начнем с простого вопроса почему они важны?\nВы проводите исследование, пишите код и довольно часто используете библиотеки. Отсюда и появляется вся важность управления зависимостями. Во-первых, когда вы добавляете новую библиотеку, вам надо сказать об этом вашей команде, чтобы они тоже использовали эту библиотеку той же версии. Во-вторых, если вам нужно изменить версию уже используемой библиотеки вам надо сообщить об этом всей команде и проверить что с новой версией ваш код будет работать корректно. В-третьих, вам надо будет передавать информацию о библиотеках разработчикам, которые будут встраивать ваш пайплайн в сервис.\nВажными моментами управления зависимостями являются:\n\nПовторяемость - при переустановке библиотек у себя выв получаете тот же результат, что и ранее.\nВоспроизводимость - при каждой новой сборке окружения вами, вашими коллегами или ci-cd, выбираются и устанавливаются одни и те же зависимости.\nПереисползуемость - механизм управления зависимостями позволяет передавать информацию о выбранных версиях библиотек разработчикам.\n\nВам надо поддерживать свои зависимости и следить за их версиями для того чтобы ваши исследования были воспроизводимы и пере используемы. В ручном режиме это делать довольно проблематично, поэтому сообщество python (и не только его) разработало множество инструментов для этого. Сегодня о них и поговорим."
  },
  {
    "objectID": "5.dependencies.html#история-развития",
    "href": "5.dependencies.html#история-развития",
    "title": "Dependency management",
    "section": "История развития",
    "text": "История развития\n\n\nЧуть-чуть истории развития питона. Хотя Python появился примерно в 1990-х годах, ему потребовалось довольно много времени, чтобы адаптироваться к шаблонам, которые другие языки программирования уже использовали при распространении программного обеспечения. До 2000 года, чтобы распространять свою программу, вам практически приходилось загружать ее где-нибудь в Интернете, где вам нужно было указать точные инструкции по установке программного обеспечения в интерпретатор Python. Таким образом, сообщество запустило distutils. Это привело к появлению файлов setup.py, setup.cfg, содержащих инструкции по установке программного обеспечения и другие полезные метаданные. Сообществу Python требовалось нечто большее, чем distutils, поэтому был создан setuptools. Он был больше как оболочка над стандартной библиотекой. Я не буду вдаваться в подробности того, что предлагается по сравнению со стандартной библиотекой. Одной из ключевых важных функций была easy_install, которая позволяла вам устанавливать другие пакеты в ваш интерпретатор python.\nИ тут начался этап хаоса в зависимостях. Управление зависимостями быстро превратилось в хаос, поскольку сообщество python начало раскалываться, одни использовали distutils, а другие setuptools. Независимо от этого, все пакеты были собраны в одном месте, что вызывает проблему с корневым доступом. Внесение путаницы в процесс развертывания и отклонение от некоторых стандартных принципов безопасности, таких как никогда ничего не запускать от имени пользователя root. Возникла потребность в изолированной среде, и примерно в 2007 году появился virtualenv. Эта концепция позволяла разработчикам иметь несколько настроенных интерпретаторов Python для каждого проекта, изолируя интерпретатор Python от базовой установки пакетов в Python. Благодаря этому улучшилось управление зависимостями, что дало множество преимуществ, таких как экспорт среды и изоляция среды.\nСледующие шаги по упорядочению этого хаоса заключались в создании нового пакетного менеджера, ставшего стандартом. Pip был представлен и заменил причудливый easy_install, pip также представил концепцию requirements.txt, устанавливающую стандарт внутри сообщества Python. Файл может быть создан путем извлечения всех зависимостей из текущего окружения virtualenv. Позже, чтобы pip мог прочитать файл и воссоздать ту же виртуальную среду.\nДо 2017 pip и virtualenv заставляли разработчиков страдать, поскольку им приходилось использовать два отдельных инструмента для изоляции своей среды и управления своими зависимостями. Первым шагом по объединению этих инструментов стал pipenv. Несмотря на то, что pipenv был прекрасен для разработчиков, разрабатывающих приложения, он не очень помог библиотекам. Теперь вам нужно обрабатывать еще больше файлов Pipfile, Pipfile.lock, setup.py и т. Д. поэтому на его смену пришел poetry.\nPoetry - это полноценный менеджер пакетов, который предлагает больше, чем просто управление зависимостями и упаковку. Он также пытается обеспечить соблюдение таких стандартов, как семантическое управление версиями, структура папок и шаблоны упаковки, с которыми, скорее всего, знакомы программисты из других сообществ.\nТеперь разберем пару инструментов, которые упоминали. Сразу скажу, что мы не будем рассматривать первые инструменты управления зависимостями в питоне, а перейдем к более актуальным."
  },
  {
    "objectID": "5.dependencies.html#install-packages",
    "href": "5.dependencies.html#install-packages",
    "title": "Dependency management",
    "section": "Install packages",
    "text": "Install packages\nУстановка пакетов.\npip install <package1> <package2>\nУстановка пакета определенной версии.\npip install <package1==version1> '<package2>=version2>'\nУстановка пакетов из requirements.txt.\npip install -r requirements.txt\nОбновление уже установленного пакета.\npip install --upgrade <package>\nИспользование приватного индекса пакетов.\npip install --index-url <index-url> <package>\nУстановка пакетов из git.\npip install git+https://<git-package-url>@<version>\nУстановка пакетов с расширениями.\npip install <package>[<extras>]\nУстановка пререлизного пакета.\npip install --pre <package>\nУстановка своих пакетов в редактируемой режиме.\npip install --editable ./<package-path>\nУстановка своих пакетов из исходного кода.\npip install --no-binary <package>\n\nОсновные возможности по установке пакетов:\n\nвы можете установить один или несколько пакетов\nможно установить пакеты определенных версий выставив ограничения, или же установить ограничение на минимальную доступную версию\nустановить пакеты из файла зависимостей\nобновить уже установленный пакет\nустановить пакеты из приватных или отдельных репозиториев(индексов) пакетов\nУстановить версию из гита, с точностью до коммита или версии\nУстановить пакет с расширением\nУстановить пакет пререлизной версии\nустановить разрабатываемый пакет в редактируемом режиме для облегчения разработки\nустановить пакет из исходного кода."
  },
  {
    "objectID": "5.dependencies.html#dependecy-resolver",
    "href": "5.dependencies.html#dependecy-resolver",
    "title": "Dependency management",
    "section": "Dependecy resolver",
    "text": "Dependecy resolver\n\nПоявился backtracking dependencies\nУвеличена строгость резолвера\nПоявился state managment\nУлучшилась поддержка constraints\n\n\n\nбета-версия появилась в 20.2 (2020-07-29)\nстабильная версия появилась в 20.3 (2020-11-30)\n\nДалее он развивался в течении двух лет и достиг довольно неплохих результатов.\nВ этом dependecy resolver появилась поддержка отката версий зависимости, если не находится кандидат, то версии зависимостей могут быть понижены для разрешения зависимостей. Ранее это игнорировалось и ставился первый попавшийся кандидат.\nТак же теперь resover запрещает ставить несовместимые пакеты и выдает ошибку, ранее он выдавал только warning. Так же он начал обращать свое внимание на constraints и учитывать их при поиске кандидатов и установке зависимостей.\nНу и наконец-то в нем появился state managment, то есть он начал адекватно работать текущее окружение начало влиять на выбор совместимых кандидатов.\nС появлением этого апдейта сборки у pip стали воспроизводимы и повторяемы. В целом текущее развитие pip dependecy resolver направлено на улучшение воспроизводимости окружения. Главное не используйте pip моложе 20.3 версии, там практически нет dependecy resolver."
  },
  {
    "objectID": "5.dependencies.html#requirement-dependecies",
    "href": "5.dependencies.html#requirement-dependecies",
    "title": "Dependency management",
    "section": "Requirement dependecies",
    "text": "Requirement dependecies\nЧто бы зафиксировать требуемые зависимости можно:\npip freeze > requirements.txt\n\n\n\n\nСтрогая фиксация requirements.txt:\ncertifi==x.y.z\ncharset-normalizer==x.y.z\nidna==x.y.z\nrequests==x.y.z\nurllib3==x.y.z\n\nТонкая настройка requirements.txt:\ncertifi>=x.y.z\ncharset-normalizer>=x.y.z\nidna>=x.y.z\nrequests>=x.y.z\nurllib3>=x.y.z\n\n\n\nКогда вы делитесь своим проектом Python с другими разработчиками, вы можете захотеть, чтобы они использовали те же версии внешних пакетов, что и вы. Возможно, конкретная версия пакета содержит новую функцию, на которую вы полагаетесь, или версия пакета, которую вы используете, несовместима с предыдущими версиями. Поэтому их стоит фиксировать.\nВ pip они фиксируются в иде списка пакетов с их версиями в фале requirements. txt. Pip по сути делает слепок текущего окружения.\nПо умолчанию Pip строго фиксирует версии библиотек, как это показано на слайде. Проблема строгого кодирования версий и зависимостей ваших пакетов заключается в том, что пакеты часто обновляются с исправлениями ошибок и безопасности. Вы, вероятно, захотите использовать эти обновления, как только они будут опубликованы. К сожалению, в pip нет возможности тонка настроить requirements, и нужно их корректировать ручками после каждого pip freeze."
  },
  {
    "objectID": "5.dependencies.html#devprod-dependecies",
    "href": "5.dependencies.html#devprod-dependecies",
    "title": "Dependency management",
    "section": "Dev/Prod dependecies",
    "text": "Dev/Prod dependecies\nЗависимости для развертывания\n# requirements.txt\npackage==1.0\npackage==1.0\npackage==1.0\nЗависитмости для разработки\n# dev_requirements.txt\n-r requirements.txt\ndev_package==1.0\ndev_package==1.0\ndev_package==1.0\n\nЕще важный момент это разделение prod и dev зависимостей. И в pip это реализовано очень плохо. Автоматического разделения нет и придётся все зависимости разделять ручками, с учетом, что бывает 2 или 3, а то и больше, уровней зависимостей. Соответственно после каждого freeze нужно самому разделить их на два файла."
  },
  {
    "objectID": "5.dependencies.html#proscons",
    "href": "5.dependencies.html#proscons",
    "title": "Dependency management",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\ndefault tool\ndeterministic builds\ndependecy resolver\n\n\nCons\n\nno built-in isolation tool\nproblematical dev and prod dependency split\nno tools for lib packaging and publishing\nbuildings from source\n\n\n\n\nC версии 20.3 появились первые значимые плюсы. Появились воспроизводимые сборки и инструмент разрешения зависимостей, выше мы обсудили какой большой impact они вносят в работу пакетного менеджера.\nНет инструмента для изоляции окружения проекта. Это не сильно плохо, но тоже является проблемой. Так как вам надо самому создать виртуальное окружение используя для этого отдельный инструмент.\nНет возможности разделить зависимости на dev и prod, у вас в одном файле будут содержаться и инструменты разработчика линтеры, форматеры и анализаторы кода, в том числе и jupyter notebook, и продовские зависимости numpy, sklearn и другие. Это очень сложно разобрать и поддерживать. Кроме этого у вас в одном файле хранятся все ваши зависимости и зависимости зависимостей и так далее, то есть очень много технического мусора, который вы напрямую не используете. В итоге у вас есть один файл — большая помойка зависимостей.\nНет инструментов для сборки и публикации пакетов. То есть на вас перекладывается проблема по поддержанию структуры проекта, файлов конфигурации сборки и продовских зависимостей. Всем этим вам придется заниматься вручную и это может стать большой проблемой когда проект станет довольно большим.\nСборка из исходного кода. Не могу сказать, что это большая проблема, пока мы говорим про исходный код на питоне. Но, так как питон не очень быстрый язык, то часто используются библиотеки, написанные на C, особенно в data science. Наиболее популярны библиотеки уже имеют бинарные сборки, помещенные в репозиторий пакетов, но не дай бог вам попадется библиотека, которую и придется собирать из исходного кода с элементами C. Тогда ваш проект будет зависеть не только от питоновского окружения, но и от окружения для С, а это не сильно приятно. Так как если у вас другая версия компилятора, то проект не соберется. Ради интереса можете попробовать собрать tensorflow и прочувствовать всю боль."
  },
  {
    "objectID": "5.dependencies.html#conda-packages",
    "href": "5.dependencies.html#conda-packages",
    "title": "Dependency management",
    "section": "Conda packages",
    "text": "Conda packages\n\n\nСтруктура пакета\n.\n├── bin\n│   └── pyflakes\n├── info\n│   ├── LICENSE.txt\n│   ├── files\n│   ├── index.json\n│   ├── paths.json\n│   └── recipe\n└── lib\n    └── python3.5\n\nПоиск пакетов:\nconda search <package>\nУстановка пакетов:\nconda install <package>\nСборка пакетов с помощью conda-build:\nconda build <my_package>\n\n\n\nПакет conda представляет собой сжатый файл tarball (.tar.bz2) или файл .conda, который содержит:\n\nбиблиотеки системного уровня.\nPython или другие модули.\nисполняемые программы и другие компоненты.\nметаданные в info/ каталоге.\nнабор файлов, которые устанавливаются непосредственно в install префикс.\n\nЕсли смотреть на структуру пакета, то в данные распределяются так:\n\nbin содержит соответствующие двоичные файлы для пакета.\nlib содержит соответствующие файлы библиотек (например, файлы .py).\ninfo содержит метаданные пакета.\n\nConda отслеживает зависимости между пакетами и платформами. Формат пакета conda идентичен для разных платформ и операционных систем.\nТак же хочется упомянуть про noarch пакеты. Пакеты Noarch — это пакеты, которые не зависят от архитектуры и поэтому должны быть собраны только один раз. Пакеты Noarch являются либо общими, либо Python. Общие пакеты Noarch позволяют пользователям распространять документы, наборы данных и исходный код в пакетах conda. Пакеты Noarch Python сокращают накладные расходы на создание нескольких разных чистых пакетов Python для разных архитектур и версий Python за счет сортировки различий, зависящих от платформы и версии Python, во время установки."
  },
  {
    "objectID": "5.dependencies.html#conda-channels",
    "href": "5.dependencies.html#conda-channels",
    "title": "Dependency management",
    "section": "Conda channels",
    "text": "Conda channels\nУстановка из определенного channel\nconda install scipy --channel conda-forge\nДобавление channel по умолчанию\nconda config --add channels new_channel\nСписок доступных channels:\nanaconda\nr\nconda_forge\nbioconda\nastropy\nmetachannel\njavascript\nprivate\n\nСобственно, у нас есть пакеты, и появляется закономерный вопрос, а откуда их брать? Тут нам помогут conda channels.\nУ conda есть возможность искать пакеты из разных каналов, так же можно указывать приоритете поиска по каналам, добавлять или удалять каналы. Я привел небольшой список каналов сообщества conda. Наиболее обширный канал это conda-forge. В этот канал публикуется большая часть питоновских пакетов из pypi с помощью сообщества. Очень советую им пользоваться.\nБольшим плюсом является возможность поднятие собственных каналов, в том числе зеркал существующих. То есть, если вы, например, работаете в компании, то вы можете в контуре компании скачивать все эти пакеты в каналы и устанавливать из них. То есть на стороне компании будет находиться весь набор необходимых зависимостей для сборки вашего проекта."
  },
  {
    "objectID": "5.dependencies.html#conda-environments",
    "href": "5.dependencies.html#conda-environments",
    "title": "Dependency management",
    "section": "Conda environments",
    "text": "Conda environments\nСоздание пустого окружения\nconda create --name <myenv>\nСоздание питоноского окружения\nconda create -n <myenv> python=3.9\nУстановка пакета в определенное окружение\nconda install -n <myenv> <package>\nСоздание файла зависимостей\n# только те которые были устанволены в ручную\nconda env export --from-history > environment.yml\n# все зависимости и сабзависимости\nconda env export > environment.yml\nСоздание окружения из файла зависимостей\nconda env create -f environment.yml\nОбновление окружения из файла зависимостей\nconda env update -f environment.yml\n\nЗамечу что в этом инструменте есть нативная поддержка изолированных окружений. Куда лучше, чем питоновские, которые формируются на основе слепка интерпретатора. Здесь в каждом окружение может находиться несколько версий интерпретатора питона для нормальной работы всех зависимостей. То есть conda рассматривает сам питон как зависимость.\nУдобно то что все окружения хранятся централизована в кеше конды, есть возможность создавать файлы зависимостей как всех установленных библиотек, так и фиксировать версии только тех библиотек, которые вы сами устанавливали. Так же на основе такого файла можно создать у себя окружение с нуля или обновить существующее.\nКонда предлагает отличный инструмент менеджмента виртуальных окружений. Я сам им постоянно пользуюсь, как минимум потому что питон в конде изолирован от системного полностью, и я могу легко менять его версии при необходимости."
  },
  {
    "objectID": "5.dependencies.html#proscons-1",
    "href": "5.dependencies.html#proscons-1",
    "title": "Dependency management",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\ndeterministic builds\ndependecy resolver\nbuilt-in isolation tool\ntools for lib packaging and publishing\nbuilded binary packages\n\n\nCons\n\nvery slow dependency resolver\na lot of steps for preparing package build\nno dependencies split\n\n\n\n\nОсновными достоинствами conda являются детерминированные сборки, разрешение конфликтов зависимостей, встроенный менеджер виртуальных окружений, инструменты для упаковки и сбор пакетов. Самое главное, что в conda используются уже собранные бинарные пакеты, что сильно сокращает время работы при установке зависимостей и не требует повторных сборок из исходников."
  },
  {
    "objectID": "5.dependencies.html#project-structure",
    "href": "5.dependencies.html#project-structure",
    "title": "Dependency management",
    "section": "Project structure",
    "text": "Project structure\n\n\nСтруктура проекта\npoetry-demo\n├── pyproject.toml\n├── poetry.lock\n├── README.md\n├── poetry_demo\n│   └── __init__.py\n└── tests\n    └── __init__.py\nИнициализация проекта\npoetry new <project-path>\n\nPyproject.toml\n[tool.poetry]\nname = \"poetry-demo\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"Kovalenko Lev\"]\nreadme = \"README.md\"\npackages = [{include = \"poetry_demo\"}]\n \n[tool.poetry.dependencies]\npython = \"^3.9\"\n \n \n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\n\nВо-первых, poetry добавляет свою структуру проекта, в которой создаются основные директории, а также файлы для описания зависимостей и метаинформации проекта. Для создания нового poetry проекта, можно вызвать одну команду и получить готовый шаблон.\nСтоит уделить вниманию что poetry использует Pyptojeсt file согласно PEP 621 о хранении метаинформации. В нем poetry фиксирует зависимости проекта.\nТакже poetry создает poetry.lock файл, в котором хранится информация о том, какие нужны зависимости, зависимости зависимостей и так далее для сборки окружения, из каких источников ставить эти зависимости."
  },
  {
    "objectID": "5.dependencies.html#dependency-install",
    "href": "5.dependencies.html#dependency-install",
    "title": "Dependency management",
    "section": "Dependency install",
    "text": "Dependency install\nУстановка пакета\npoetry add <package>\nУстановка пакета определенной версии\n# Allow >=2.0.5, <3.0.0 versions\npoetry add package@^2.0.5\n# Allow >=2.0.5, <2.1.0 versions\npoetry add package@~2.0.5\n# Allow >=2.0.5 versions, without upper bound\npoetry add \"package>=2.0.5\"\n# Allow only 2.0.5 version\npoetry add package==2.0.5\nУстановка пакета из git\npoetry add git+https://github.com/sdispater/package.git#version\nУстановка своих пакетов в редактируемой режиме\npoetry add ---editable ./<package-path>\nДобавление зависимости в определенную группу\npoetry add <package> --group <group>\n# для dev зависимостей\npoetry add <package> --group dev\npoetry add --dev <package>\nУстановка пререлизных зависимостей\npoetry add --allow-prereleases <package>\nУстановка зависимостей из различных репозиториев\npoetry source add <source> https://<source-link>\npoetry add --source <source> <package>\nУстановка расширений зависимостей\npoetry add \"<package>[<extras>]\"\nПосмотреть наличие обновлений\npoetry show --tree\nПровести обновление зависимостей\n# обновить конкретную зависимость\npoetry add <package>@latest\npoetry update <package>\n# обновить под зависимости\npoetry update\n\nPoetry позваляет проводить все те же операции что и pip. Работать с разными зависимостями, ставить пределенные версии, устанавливать зависимости из разных источников, прерлизные зависимости, расширения зависимостей. В плане установки зависимостей он полностью идентичен pip.\nБолее того он позволяет группировать зависимости, что позволяет сделать dev/prod split или же выделять какие-то дополнительные зависимости для оптимизации в группы.\nТакже есть функционал для отслеживания обновлений зависимостей и самого обновления зависимостей."
  },
  {
    "objectID": "5.dependencies.html#package-build-and-publish",
    "href": "5.dependencies.html#package-build-and-publish",
    "title": "Dependency management",
    "section": "Package build and publish",
    "text": "Package build and publish\nСборка пакетов локально\npoetry build\nПубликация собранного пакета\npoetry publish\nПубликация и сборка пакета\npoetry publish --build\nПубликация пакета в приватный репозиторий\npoetry publish -r <repo>\n\nТакже хочется отметить cli poetry для сборки и публикации пакетов. Он очень удобен и прост в использовании. Буквально одной командой можно собрать и опубликовать пакет и не нужно дополнительных плясок с бубнами c setup.py и прочим."
  },
  {
    "objectID": "5.dependencies.html#proscons-2",
    "href": "5.dependencies.html#proscons-2",
    "title": "Dependency management",
    "section": "Pros&Cons",
    "text": "Pros&Cons\n\n\nPros\n\ndeterministic builds\ndependecy resolver\ndependecies groups\nbuilt-in isolation tool\ntools for lib packaging and publishing\ndependencies update tracking\n\n\nCons\n\nbuildings from source\na very young tool\n\n\n\n\nПакетный менеджер собрал все основные best pracices. Единственные два минуса — если нет колес, то будет собирать из исходников, что бывает больно. А также это довольно молодой инструмент и у него встречаются различные баги, я сам встречал баги с работой с несколькими репозиториями, с приватными репозиториями. Думаю, со временем он будет развиваться и решит эти проблемы. Часть из них уже решил если что)"
  },
  {
    "objectID": "5.dependencies.html#выводы",
    "href": "5.dependencies.html#выводы",
    "title": "Dependency management",
    "section": "Выводы",
    "text": "Выводы\n\npip - дефолтный менеджер пакетов, соответствует всем pep\nconda - лидер по environments, позволяет работать только с бинарными пакетами\npoetry - собрал в себя все лучшие практики, но очень молодая технология\n\n\nМы рассмотрели с вами несколько пакетных менеджеров. У всех из них есть свои достоинства и недостатки. Но тут стоит для себя решить, что для вас критично, а что нет.\nЯ для себя и в своей команде использую связку conda и poetry. Conda нужна для работы с непитоновскими зависимостями и менеджмента виртуальных окружений. Poetry занимается установкой питоновских зависимостей и отвечает за сборку и публикацию библиотек."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Engineering practices in ML",
    "section": "",
    "text": "Для удобства коммуникаций предлагаю общаться в Telegram.\n\n\n\\[0.4\\cdot\\text{ДЗ} + 0.4\\cdot\\text{ФП} + 0.2\\cdot\\text{Т}\\]\nДЗ - домашние задания.\nФП - финальная презентация.\nТ - Теоретический вопрос.\nСводная таблица будет обновляться.\n\n\n\n\nВыбрать и опубликовать ML проект на GitHub.\nПеревести все jupyter notebook’и в скрипты (если это уже существующий проект, можете оставить jupyter notebook, но также создайте папку со скриптами).\nЗаполнить информацию о себе в сводной таблице\n\nДля тех, у кого нет своего проекта, советуем ознакомиться со следующими исследованиями, которые вы можете взять в качестве своего проекта, либо же составить свое исследование на их основе:\n\nA Statistical Analysis & ML workflow of Titanic\nTitanic: A complete approach for Data Scientists\nTitanic - Advanced Feature Engineering Tutorial\nExploratory Tutorial - Titanic\n\nДанное домашнее задание оценивается на 10 баллов, если оно сделано в срок. За каждый день опоздания в сдаче \\(-1\\) от исходной оценки. Дедлайн 15.11.2022.\n\n\n\n\nВыбрать пакетный менеджер.\nСохранить зависимости в поддерживаемом формате. (2 балла)\nРазделить dev и prod зависимости. (2 балла)\nНастроить сборку пакета и публикацию в pypi-test. (2 балла)\nОписать шаги в readme.md: (4 балла)\n\nустановка пакетного менеджера.\nразвертывание окружения.\nсборка пакета.\nссылка на пакет в pypi-test.\nустановка пакета из pypi-test.\n\n\nРезультатом работы будет добавление файлов зависимостей в репозиторий, а также описания. Если в проект не будет добавлено файлов, то оценить его не представляется возможным. Дедлайн 22.11.2022."
  },
  {
    "objectID": "6.docker.html#что-это-такое",
    "href": "6.docker.html#что-это-такое",
    "title": "Docker",
    "section": "Что это такое?",
    "text": "Что это такое?\n\n\nКак я уже говорил про воспроизводимые исследования - хорошо бы было передать вместе с кодом и данными все окружение, так скажем, сделать слепок системы со всеми библиотеками и инструментами.\nВ разработке ПО для этого используется docker. Docker — это открытая платформа для разработки, доставки и эксплуатации приложений. Он позволяет создавать неограниченное количество изолированных контейнеров.\nО, а кто знает, что такое контейнер? > Изолированный linux-процесс или же независимая среда выполнения с выделенными только для неё ресурсами железа. Очень похоже на виртуальную машину…. Но это не она.\nДокер позволяет отвязать исследование от вашей инфраструктуры и окружения, ну или же наоборот связать их вместе, кому как больше нравится считать. Суть в том, что ваше исследование теперь всегда и везде запускается в вашем окружении и инфраструктуре.\nЧуть-чуть расскажу историю , как и кем этот проект создавался. Проект начат как внутренняя собственническая разработка компании dotCloud, основанной Соломоном Хайксом (Solomon Hykes) в 2008 году с целью построения публичной PaaS-платформы с поддержкой различных языков программирования. В марте 2013 года код Docker был опубликован под лицензией Apache 2.0. В дальнейшем компания сменила название на docker. Желая подчеркнуть смену своего курса."
  },
  {
    "objectID": "6.docker.html#section",
    "href": "6.docker.html#section",
    "title": "Docker",
    "section": "",
    "text": "Как уже устроен докер? Docker использует архитектуру клиент-сервер. Docker клиент общается с демоном Docker, который берет на себя тяжесть создания, запуска, распределения ваших контейнеров. Оба, клиент и сервер могут работать на одной системе, вы можете подключить клиент к удаленному демону docker. Клиент и сервер общаются через сокет или через RESTful API.\nЧтобы понимать, из чего состоит docker, вам нужно знать о трех компонентах:\n\nобразы (images)\nреестр (registries)\nконтейнеры\n\nDocker-образ — это read-only шаблон. Например, образ может содержать операционку Ubuntu c NGiNX и приложением на ней. Образы используются для создания контейнеров. Сам образ описывается в Dockerfile. Docker позволяет легко создавать новые образы, обновлять существующие, или вы можете скачать образы созданные другими людьми. Образы — это компонента сборки docker-а.\nDocker-реестр хранит образы. Есть публичные и приватные реестры, из которых можно скачать либо загрузить образы. Публичный Docker-реестр — это Docker Hub. Компонента распространения.\nВ контейнерах содержится все, что нужно для работы приложения. Каждый контейнер создается из образа. Контейнеры могут быть созданы, запущены, остановлены, перенесены или удалены. Каждый контейнер изолирован и является безопасной платформой для приложения. Это компонента работы."
  },
  {
    "objectID": "6.docker.html#ds-docker-image",
    "href": "6.docker.html#ds-docker-image",
    "title": "Docker",
    "section": "DS docker image",
    "text": "DS docker image\nСоздаем DockerFile\ntouch DockerFile\nВыбираем базовый образ\nFROM nvidia/cuda:11.7.0-cudnn8-runtime-ubuntu20.04\nДобавляем conda:\nRUN apt update && \\\n    apt install -y curl gpg && \\\n    curl https://repo.anaconda.com/pkgs/misc/gpgkeys/anaconda.asc | \\\n    gpg --dearmor > /etc/apt/trusted.gpg.d/anaconda.gpg && \\\n    echo 'deb https://repo.anaconda.com/pkgs/misc/debrepo/conda stable main' > /etc/apt/sources.list.d/conda.list && \\\n    apt update && \\\n    apt install -y conda graphviz && \\\n    touch ~/.bashrc && \\\n    /opt/conda/bin/conda init bash\nСоздаем окружение и устанавливаем основные зависимости:\nRUN . ~/.bashrc && \\\n    conda install -y -n base -c conda-forge mamba  && \\\n    mamba create -y --name research python=3.10.6  && \\\n    mamba install -y -n research -c conda-forge -c bioconda snakemake conda-build gcc git && \\\n    mamba clean -ya\nСоздаем рабочую директорию\nWORKDIR /workspace\nУстанавливаем poetry зависимости (опциональный шаг)\nCOPY pyproject.toml poetry.lock ./\nRUN . /opt/conda/etc/profile.d/conda.sh &&\\\n    conda activate research &&\\\n    pip install poetry==1.2.2 && \\\n    poetry config virtualenvs.in-project false && \\\n    poetry config virtualenvs.path /opt/conda/envs && \\\n    poetry install --no-root --no-interaction --no-ansi\n\nДля создания образа, нужно создать docker file. Затем указать какой образ будет использоваться, если нужны видеокарты, то стоит использовать образ от nvideo. В нем есть cuda, cudnn и поддержка видеокарт.\nДалее устанавливаем conda, это не очень просто, потому что нужно сделать несколько магических шагов с добавлением репозитория conda. А также инициализация conda в bash shell.\nПосле этого можно создать в ней окружение, например, окружение и поставить основные зависимости типа python, git, gcc и других пакетов которые потребуются для сборки библиотек.\nПосле этого можно создать рабочую директорию.\nТак же можно посмотреть на опциональный шаг - установка зависимостей poetry. Можно скопировать файлы зависимостей и установить их. Но так делать не рекомендую, так как размер базовоо образа для вашего решения увеличиться в разы. Как же с этим работать?"
  },
  {
    "objectID": "6.docker.html#как-использовать-это",
    "href": "6.docker.html#как-использовать-это",
    "title": "Docker",
    "section": "Как использовать это?",
    "text": "Как использовать это?\nСборка образа\ndocker build -t research_image .\nЗапуск контейнера\ndocker run -it --gpus all -v <path/to/project>:/workspace/project --name research research_image /bin/bash\nДобавить запуск jupyter\n#!/bin/bash\n\nsource /opt/conda/etc/profile.d/conda.sh\nconda activate snakemake\n# Run jupyetr lab\njupyter lab --ip 0.0.0.0 --port 8888 --no-browser --allow-root\nВключить его в docker\nENV JUPYTER_TOKEN=\"password\" \\\n    PATH=\"/opt/conda/bin/conda:$PATH\" \\\n    SHELL=\"/bin/bash\"\nCOPY docker/entrypoint.sh /opt/docker/bin/entrypoint.sh\nENTRYPOINT [ \"/bin/bash\", \"/opt/docker/bin/entrypoint.sh\" ]\nПодключиться из vscode\n\nDev containers > Attach to Running Container > research\n\n\nДля начала нужно собрать образ, после этого его запустить. Когда вы его запустите то попадете в его консоль.\nИспользовать так не очень практично. Лучше установить в него jupyter и настроить его запуск через entrypoint.sh.\nЛучшим вариантом будет подключиться к нему из vscode и таким образом вы будете работать через vscode внутри контейнера. Очень удобный инструмент, который позволит изолировать вашу работу и сделать ее более воспроизводимой."
  },
  {
    "objectID": "6.docker.html#выводы",
    "href": "6.docker.html#выводы",
    "title": "Docker",
    "section": "Выводы",
    "text": "Выводы\n\nИзоляция\nВоспроизводимость\nВерсионирование\nПередача\n\n\nПодведем итоги, с помощью docker вы можете изолировать свое окружение и спокойно в нем проводить эксперименты. Так же инструкции его создания зафиксированы и спокойно могут быть воспроизведены. Более того docker позволяет создать образы на основе контейнеров, можно создать новый образ и разместить его в реестре. Также docker позволяет версионировать образы на уровне тегов в реестре. Ну и таким образом его можно передать, более того можно выгрузить докер образ в архив и таким образом его передать.\nСогласитесь, докер удивительный и полезный инструмент."
  },
  {
    "objectID": "3.team_work.html",
    "href": "3.team_work.html",
    "title": "Team work",
    "section": "",
    "text": "Представьте, вы работаете в огромной команде над каким-то исследованием в области анализа данных. Собственно, вопрос, какие были бы общие артефакты у этой команды? Чем бы вам понадобилось делиться с коллегами по работе?\n\n\n\n\n\nОбщие договоренности.\n\n\n\n\nЗадачи, план исследований.\n\n\n\n\nКод, окружение, среда выполнения.\n\n\n\n\nЗнания(отчеты об исследованиях).\n\n\n\n\nИсходные, подготовленные данные и модели.\n\n\nВо-первых, это общие договоренности: ролевая модель, различные процессы, требования. То есть некоторые условности, которые приняты в команде, для решения спорных моментов и некоторой унификации артефактов, кода, задач и отчетов.\nВо-вторых, это задачи. В команде нужно как-то организовать рабочий процесс, разделить ответственность и обязанности на членов команду, выбрать основные направления для исследования, определить ключевые точки в исследованиях. Это все нужно сделать что бы каждый член команды знал, что ему нужно делать, какие эксперименты проводить, какой код писать. При этом нужно организовать так что бы обязанности не дублировались или дублировались по минимуму.\nВ-третьих, код и окружение. Вам всем придется писать код для экспериментов, переиспользовать его, поддерживать, периодически обновлять и перезапускать. Кроме написания кода, вам придется этот код запускать, для этого нужно воспроизводимое окружение, а лучше общая инфраструктура.\nВ-четвертых, это знания полученные в ходе исследования. Их нужно фиксировать, обновлять, версионировать и передавать команде. Вот, например, сотрудник проводил важные эксперименты, но вышел в отпуск или заболел, нужно как-то его знания о проведенных исследованиях, полученных результатах и сформулированных выводах. Для всего этого надо как-то управлять знаниями.\nВ-пятых, нужно решить с большими и громоздкими артефактами — данными и моделями, их нужно как-то хранить и версионировать, а еще делиться с коллегами. Более того нужно писать такой код, который бы выдавал воспроизводимые результаты, всегда один и тот же бинарный объект или массив данных."
  },
  {
    "objectID": "3.team_work.html#примеры-contributing",
    "href": "3.team_work.html#примеры-contributing",
    "title": "Team work",
    "section": "Примеры Contributing",
    "text": "Примеры Contributing\n\npython devguide\ncontribute to scipy\nsklearn contributing\ncontribute to tensorflow\n\n\nКак видите в них описывалось все, начиная от того, как развернуть проект, как оформить merge request, как написать документацию к изменениям, заканчивая часами работы core developers и тем как отвечать на вопросы в stack overflow.\nДовольно обширные своды правил и информации как разрабатывать проект. Такие практики стоит перенимать и задействовать в своих командах."
  },
  {
    "objectID": "3.team_work.html#что-стоит-включать-в-договоренности",
    "href": "3.team_work.html#что-стоит-включать-в-договоренности",
    "title": "Team work",
    "section": "Что стоит включать в договоренности?",
    "text": "Что стоит включать в договоренности?\n\nВсе где возникают разногласия в команде\n\n\nКак и куда писать код\nКак и куда писать тесты\nКак настроить среду разработки\nКак и куда писать документацию\nКак и куда сохранять данные\nКак и где составлять задачи\nКак брать задачу на себя\nКак оформлять merge requests\nКто за что отвечает\nУ кого можно получить помощь по процессам\nКакие инструменты можно и нельзя использоваться\nКак этими инструментами пользоваться\nКак организован CI в проекте\nЧто писать в комитах\nКак происходит починка багов\nКакой у проекта development cycle\nУ кого можно получить помощь по процессам\n\nи многое другое.\n\nВсе это стоит фиксировать и поддерживать в актуальном состоянии. Это полезно. Полезно по двум причинам:\n\nКейс первый. Вы работаете с Петей и Васей. Вы договорились с Васей писать код в стиле А и начали работать. Петя об этом не знает и пишет его по-своему, в стиле Б. Когда вы объединяете свои наработки, у вас получается не консистентный код, в одном модуле функции и классы, в другом весь код написан в глобальной области как один скрипт. Получается, что текущие наработки нужно будет рефакторить и приводить в порядок. А все это произошло потому что информация не была передана всем участникам команды. Если вы о чем-то договорились, то это стоит зафиксировать в общедоступном месте и донести до всех членов команды.\nКейс второй. К вам в команду пришел новый сотрудник. Для того что бы рассказать ему все и помочь настроить проект вам потребуется несколько дней с ним общаться и передавать знания. При этом важно ничего не забыть. Но если все договоренности зафиксированы, то сотрудник сможет сам разобраться с этим и к вам придет уже с конкретными вопросами."
  },
  {
    "objectID": "3.team_work.html#резюме",
    "href": "3.team_work.html#резюме",
    "title": "Team work",
    "section": "Резюме",
    "text": "Резюме\n\nДоговоренности фиксируются в общедоступном месте.\nДоговоренности несут пользу процессу и членам команды.\nДоговоренности можно отменять, если они мешают команде."
  },
  {
    "objectID": "3.team_work.html#defenition-of-ready-dor",
    "href": "3.team_work.html#defenition-of-ready-dor",
    "title": "Team work",
    "section": "Defenition of ready (DoR)",
    "text": "Defenition of ready (DoR)\n\nМотивация появления задачи.\nПроверяемая гипотеза.\nНа каких данных проводить исследование.\nОписание действий для обработки данных.\nКонкретный ожидаемый результат.\n\n\nВажно иметь список критериев, описывающий состояние задачи, когда ее можно взять в работу. Поскольку задачи в проекте могут возникать не только от тимлида, но и от самих исследователей, так как во время эксперимента были выявлены какие-то аномалии в данных или же необходимо проверить какие-то дополнительные гипотезы, нужно, что бы такие договоренности были в команде и все могли формировать достаточно подробные задачи.\nВ задаче стоит указать, почему она вообще появилась и какой impact она сделает для проекта, например, в проекте по классификации электронных сообщений проводилось исследование по ключевым словам для каждой темы, в какой-то теме ключевыми словами стали здравствуйте и привет, соответственно в проекте появилась задача по удалению приветствий и в ней стоит упомянуть причину ее появления.\nНужно указать проверяемую гипотезу, то есть что именно нужно проверить в ходе исследования. Например, проверить наличие зависимости между признаками, или выбор стратегии заполнения пропусков для такого-то признака, или выделение ключевых слов на основе какого-то подхода, или же построение такой-то модели на данных проекта.\nТак же важно указать какие данные стоит использовать в исследовании, потому что если в проекте хранятся промежуточные версии данных, то сложно с ходу разобраться, а какая из них нужна для проверки гипотезы.\nЕсли нет готовых данных или задача заключается в обработке данных, то нужно описать какие действия нужно предпринять для подготовки данных, что бы исполнитель задачи не догадывался как нужно объединить два датасета что бы получилась корректная выборка.\nНу и последнее конкретный результат, то есть что вы ожидаете при выполнении задачи, например, если вернутся к примеру, про классификацию писем, то ожидаемый результат, что приветствия пропадут из списков ключевых слов, или же какая-то модель получит более высокие метрики. То есть, что вы, как автор задачи, ожидаете после ее исполнения.\nЕсли вы как автор не можете сформулировать такую задачу и для нее нужны дополнительные знания, то стоит сначала создать задачу для получения этих знаний, например, задача по выбору источника данных погоды или же задача по исследованию применения нейростей определенной архитектуры в задачах определенной тематики."
  },
  {
    "objectID": "3.team_work.html#типы-задач",
    "href": "3.team_work.html#типы-задач",
    "title": "Team work",
    "section": "Типы задач",
    "text": "Типы задач\n\nЗадачи обработки данных.\nЗадачи проведения исследований.\nВспомогательные задачи.\n\n\nЗадачи обработки данных появляются в основном на первых этапах исследования и связаны с очисткой и подготовкой датасета. Постановка такой задачи должна содержать:\n\nмотивацию для появления этой задачи, зачем она нужна, как будет использоваться результат;\nназвание предполагаемого(ых) датасета(ов) для выполнения, в процессе работы исполнитель может их изменить при необходимости (если поймет, что есть данные более подходящего формата);\nописание действий над данными: фильтрация, очистка, заполнение пропусков, объединение и тд.\nпри необходимости фиксируются параметры или же ссылки на участки кода, которые необходимо применить;\nдополнительная известная информация, касающаяся данной задачи.\n\nЗадачи проведения исследований описывают постановку исследования, такие задачи составляют наибольшую часть всех задач и появляются на протяжении всего проекта. В описании карточки необходимо указать:\n\nнаправление исследования, в какой области мы копаемся;\nпроверяемую гипотезу в рамках исследования;\nожидаемый результат, что подтвердит выдвинутую гипотезу.\n\nВспомогательные задачи появляются на протяжении всего проекта, они связаны с различными дополнительными работами или исправлением багов. В постановке такой задачи описывается или баг, или работы, которые надо провести. Для описания бага стоит отметить:\n\nВ чем заключается баг?\nКак его воспроизвести?\nГде проблема в коде? В отчете?\nКакие файлы надо изменить?\nЧто надо настроить и как?"
  },
  {
    "objectID": "3.team_work.html#defenition-of-done-dod",
    "href": "3.team_work.html#defenition-of-done-dod",
    "title": "Team work",
    "section": "Defenition of Done (DoD)",
    "text": "Defenition of Done (DoD)\n\nСоставлен отчет о проведенном исследовании.\nКод соответствует принятым стандартам качества.\nДля новой функциональности написаны тесты.\nПройдено ревью кода и исследования.\n\n\nDoD - Состояние задачи, когда она готова. Членам команды так же важно понимать, что нужно сделать что бы считать задачу выполненной.\nЭто знание необходимо что бы понимать, какой объем работ нужно провести для выполнения задачи, помимо самого исследования, потому что, напоминаю, вы работаете в команде и важно не только провести исследование, но и составить для него описание, которое позволило бы делиться знаниями с другими членами команды, привести в порядок код и оттестировать его.\nНа слайде приведен пример что может быть в DoD, в целом эта договоренность формируется также командой и все члены команды должны понимать зачем им это нужно."
  },
  {
    "objectID": "7.code_styles.html#почему-это-важно",
    "href": "7.code_styles.html#почему-это-важно",
    "title": "CodeStyle",
    "section": "Почему это важно?",
    "text": "Почему это важно?\n\n\nНачинающий программист \n\nОпытный программист \n\n\n\nПотому что датасаентисты работают с кодом. В процессе работы приходиться писать и дебажить код, изучать библиотеки, примеры использования инструментов, а также читать и разбирать код коллег. Это все может занимать значительную долю времени. Для экономии своего времени и времени коллег (ну кому охота полдня читать код?) приходиться договариваться о стиле и структуре кода и приводить код к одному виду. Это ключевой фактор, который характеризует читабельность кода.\nВ процессе сбора материала для презентации я набрел на исследования, в которых датчики следили за движениями глаз начинающего и опытного программистов. Code style как стандарт разработки\nОбратите внимание, чем занимается опытный программист. Он выделяет блоки кода, декомпозирует их и читает поблочно, выделяя ключевые части и анализируя их работу. Начинающий же мечется построчно, просто пытается понять, что тут вообще происходит.\nВидео довольно старое, от 2012 года, и запись была направлена на исследование процессов мозга программиста. Чуть подробнее можно почитать тут."
  },
  {
    "objectID": "7.code_styles.html#основная-мотивация-появления-codestyle",
    "href": "7.code_styles.html#основная-мотивация-появления-codestyle",
    "title": "CodeStyle",
    "section": "Основная мотивация появления CodeStyle",
    "text": "Основная мотивация появления CodeStyle\n\nУскорить понимания и ревью кода.\nУменьшить стилистическое разнообразие кода.\nУменьшить сложность кода.\nЗапретить использование плохих практик."
  },
  {
    "objectID": "7.code_styles.html#читаемость-кода",
    "href": "7.code_styles.html#читаемость-кода",
    "title": "CodeStyle",
    "section": "Читаемость кода",
    "text": "Читаемость кода\nЕсть 2 способа ускорить чтение и понимания кода:\n\nПостоянно наращивать базу знаний разработчиков по тому, как может выглядеть код.\nПривести весь код к одному стандарту, задать тот самый code style\n\n\nКонечно стоит идти по второму пути, поскольку это снизит нагрузку на разработчика, выделили следующие поинты, которые даст повышение читабельности кода:\n\nЕсли вы обеспечили историческое написание понятного кода, то, сколько бы не приходило и не уходило разработчиков, вы всегда имеете равное качество кода, что позволяет проекту динамично расти вне зависимости от его объема.\nЕсли ваш код написан понятно, новый специалист быстро разберется и начнет приносить пользу. Есть такое понятие, как точка самоокупаемости сотрудника. Это точка, с которой сотрудник начинает приносить только пользу. А если код написан понятно, новым сотрудникам научится читать ваш код, без необходимости детально разбираться. И чем быстрее он это сделает, тем быстрее перестанет задавать тонны вопросов остальным специалистам, отнимая у них время. А время специалистов, которые прошли точку самоокупаемости, значительно дороже для команды и компании с точки зрения потенциальной ценности приносимой продукту.\nНе нужно будет постоянно спотыкаться о чью-то оригинальность и специфическое оформление.\nВаш мозг меньше сопротивляется, т.к. не нужно вникать в чужую стилистику. Ментальных ресурсов на чтение понятного кода нужно намного меньше.\nОчень скоро, после прибытия в новую компанию, программист начнет делиться впечатлениями с бывшими коллегами и друзьями. И он либо скажет, что тут все круто, либо выделит негативные моменты в работе с кодом.\nЗадача программиста лежит на более низком уровне, чем будущее всей компании, но важно донести понимание, что понятность и читаемость кода сейчас влияет на динамику дальнейшей разработки."
  },
  {
    "objectID": "7.code_styles.html#section",
    "href": "7.code_styles.html#section",
    "title": "CodeStyle",
    "section": "",
    "text": "Собственно, действительно можно собрать свой собственный набор стилей для написания кода. И для этого есть множество удобных инструментов, которые позволяют автоматизировать множество проверок стиля и дополнить их написанием своих собственных."
  },
  {
    "objectID": "7.code_styles.html#autoformatters",
    "href": "7.code_styles.html#autoformatters",
    "title": "CodeStyle",
    "section": "AutoFormatters",
    "text": "AutoFormatters\n\n\nНа сегодняшний день есть несколько популярных автоформаттеров для питона. Yapf, Autopep8 и Black. Какой выбирать решайте сами.\n\nYapf - google\nAutopep8 - Hideo Hattori\nBlack - Python Software Foundation\n\nВ качестве инструмента форматирования согласно PEP 8 используют autopep8 (есть и другие, но этот самый популярный). Он запускается через командную строку, поддерживает множество аргументов. Но ожидать, что ваш Python-код станет повсюду единообразным не стоит, поскольку PEP 8 не дает строгих рекомендаций. Но уж лучше с ним, чем без него.\nYAPF является не официальным продуктом от Google, но кодом, который, так случилось, владеет Google. YAPF основан на таком инструменте форматирования кода Си-подобных языков, как clang-format. Он берет код и форматирует его в соответствии с заданными настройками, причем отформатирует код даже в случае, если он не нарушает PEP 8. Например, вы можете задать необходимую компоновку словарей, а все остальное сделать как в Black. Эти настройки сохраняются и передаются другим программистам.\nBlack является наиболее строгим инструментом форматирования. С одной стороны, это помогает придерживаться одного стиля форматирования кода, с другой, некоторые правила могут вызывать внутреннее отторжение. Кроме того, в отличие от YAPF, он не поддерживает множество настроек. По факту Black имеет две перенастраиваемые опции:\n\nизменение допустимой длины строки (по умолчанию стоит 88),\nразрешение использования одинарных кавычек (по умолчанию разрешаются только двойные).\nПоддержка isort\n\nКак привести код в порядок мы обсудили, теперь же поговорим о том, как отслеживать качество кода и находить отклонения от code style."
  },
  {
    "objectID": "7.code_styles.html#linters",
    "href": "7.code_styles.html#linters",
    "title": "CodeStyle",
    "section": "Linters",
    "text": "Linters\n\n\n\nLinter\nCategory\nDescription\n\n\n\n\nPylint\nLogical & Stylistic\nChecks for errors, tries to enforce a coding standard, looks for code smells\n\n\nPyFlakes\nLogical\nAnalyzes programs and detects various errors\n\n\nPycodestyle\nStylistic\nChecks against some of the style conventions in PEP8\n\n\nPydocstyle\nStylistic\nChecks compliance with Python docstring conventions\n\n\nBandit\nLogical\nAnalyzes code to find common security issues\n\n\nMyPy\nLogical\nChecks for optionally-enforced static types\n\n\n\n\nТеперь давайте обсудим их, зачем же их так много.\nОни разбиваются на две группы: Logical и Stylistic.\nПервая позволяет находить, паттерны опасного кода и потенциальное неоднозначное поведение. Стилистический линтеринг предназначен для проверки стилистических договоренностей.\n\nPylint сложный и мощный анализатор. Который умеет многое из коробки.\nPyFlake базовый flake linter который анализирует программу и ищет потенциальные ошибки\nPycodestyle - проверка на соглашение по pep8\nPydocstyle - проверка docstrings по соглашению\nBandit - проверка на безопасность кода, уязвимости в используемых библиотеках и прочее\nMyPy - проверка типов на основе аннотаций\n\nЭти инструменты направлены на проверку соглашений по написанию кода, которые были заключены, на поиск плохих практик, ошибок."
  },
  {
    "objectID": "7.code_styles.html#flake8-plugins",
    "href": "7.code_styles.html#flake8-plugins",
    "title": "CodeStyle",
    "section": "Flake8 plugins",
    "text": "Flake8 plugins\n\n\nХочется отметить среди линтеров flake8. Это по-настоящему мощный инструмент который объединяет в себе все другие за счет расширений и плагинов. Вы видите наиболее популярную часть плагинов и расширений для flake8.\nТо есть за счет компоновки плагинов вы можете настроить все необходимые вам инструменты в одной библиотеке и проверять все одной командой, при этом за вас соберут все дублирующиеся ошибки и варнинги и выведут в приоритетном порядке.\nЕсли сравнивать с pylint это инструмент, который включает в себя сразу большое количество проверок, но не дает гибкого интерфейса для настройки и расширения проверок, то flake8 его во много выигрывает за счет огромной экосистемы плагинов, которые увеличивают его мощь и строгость.\nОдин из принципов python dzen звучит что complex is better than complicated. Это как раз и применимо в этом сравнении, составное лучше сложного."
  },
  {
    "objectID": "7.code_styles.html#анализаторы-кода---radon",
    "href": "7.code_styles.html#анализаторы-кода---radon",
    "title": "CodeStyle",
    "section": "Анализаторы кода - Radon",
    "text": "Анализаторы кода - Radon\n\nЦикломатическая сложность\nМетрика Холстеда\nИндекс поддерживаемости кода\n\n\nА как вообще команде проверить что договорённости, которые она заключила помогают в достижении изначальных целей? Как проверить читабельность кода?\nДля этого существуют анализаторы кода, например, Radon. Он позволяет посчитать некоторые метрики:\n\nЦикломатическая сложность - Чем больше в коде переходов (if-else), циклов, генераторов, обработчиков исключений и логических операторов — тем больше у программы вариантов исполнения и тем сложнее удержать в голове различные состояния системы. Метрика, которая измеряет сложность кода, опираясь на количество этих операций, называется цикломатической сложностью программы. При подсчете ее с помощью радона вы получите список файлов, классов, методов и функций в вашем проекте и их индекс сложности, от очень простого до очень сложного. Индекс укажет на перегруженные логикой места, которые можно разбить на куски помельче, упростить или переписать (если есть такая возможность — алгоритм может быть очень сложным сам по себе и попытки его разбить на куски могут только ухудшить понимабельность кода).\nМетрика Холстеда - Тут считается количество уникальных операторов и операндов в коде и их общее количество. Полученные значения подставляются в формулы и получается набор чисел, который описывает сложность программы и количество усилий, предположительно затраченное на написание и понимание кода.\nИндекс поддерживаемости кода - Этот индекс говорит нам о том, насколько сложно будет поддерживать или редактировать кусок программы. Этот параметр рассчитывается на основе чисел, полученных из метрик, посчитанных выше."
  },
  {
    "objectID": "7.code_styles.html#mypy-проверка-типов",
    "href": "7.code_styles.html#mypy-проверка-типов",
    "title": "CodeStyle",
    "section": "Mypy проверка типов",
    "text": "Mypy проверка типов\n\n\nFrom Python…\ndef fib(n):\n    a, b = 0, 1\n    while a < n:\n        yield a\n        a, b = b, a+b\n\n…to statically typed Python\ndef fib(n: int) -> Iterator[int]:\n    a, b = 0, 1\n    while a < n:\n        yield a\n        a, b = b, a+b\n\n\n\nКак вы знаете в питоне есть аннотации типов. Mypy использует для статической проверки типов и собственно проверяет везде ли и корректно ли они указаны.\nMypy имеет мощную современную систему типов с такими функциями, как двунаправленный вывод типов, обобщения, вызываемые типы, абстрактные базовые классы, множественное наследование и типы кортежей.\nЗачем это DS? У них же один dataframe или numpy array. На самом деле такой подход увеличивает читаемость кода, сразу понятно какой dataframe к вам пришел, какие стобцы в нем есть и каких типов. Есть разные библиотеки для этого, например, pandera, но об этом мы поговорим позже.\nОсновная цель добавления статической типизации или аннотирования типов заключается в повышении читаемости, вам и вашим коллегам не придётся догадываться что и какого типа нужно передать той или иной функции. Да и такой подход позволит избавится от части ошибок, которые вы можете допустить в коде."
  },
  {
    "objectID": "7.code_styles.html#pre-commit-автоматизация",
    "href": "7.code_styles.html#pre-commit-автоматизация",
    "title": "CodeStyle",
    "section": "Pre-commit автоматизация",
    "text": "Pre-commit автоматизация\n.pre-commit-config.yaml\ndefault_stages: [commit, push]\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n    -   id: check-ast\n    -   id: check-merge-conflict\n-   repo: https://github.com/python/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n-   repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n    -   id: isort\n        name: isort (python)\n-   repo: https://github.com/PyCQA/flake8\n    rev: 4.0.1\n    hooks:\n    -   id: flake8\n        additional_dependencies: [\n            yastyleguide==0.1.0\n        ]\n\nИтак, теперь мы знаем, как использовать линтеры и автоформатеры, и поняли, насколько они полезны! Следующий шаг — начать применять их в наших проектах. Это можно сделать с помощью хуков перед комитом. Хуки перед комитом позволяют нам проверять наш код на наличие проблем со стилем и форматированием каждый раз, когда происходит коммит изменения, тем самым обеспечивая сохранение единого стиля на протяжении всего проекта.\nТо есть в репозиторий не попадает плохого кода. Мы не даем человеку его писать и комитить, а уже потом когда-нибудь рефакторить. Мы делаем условие что в репозиторий код попадает как минимум в соответствии ос стилем и уровнем качества удовлетворяющим линтеры.\nЭто также снимает с самого разработчика ответственность за запуск различных линтеров и форматеров, облегчает рутинную работу так сказать."
  },
  {
    "objectID": "7.code_styles.html#выводы",
    "href": "7.code_styles.html#выводы",
    "title": "CodeStyle",
    "section": "Выводы",
    "text": "Выводы\n\nЧитаемость и качество важны.\nЗа качеством нужно следить.\nНужно автоматизировать рутинные операции."
  },
  {
    "objectID": "7.code_styles.html#как-работает-python",
    "href": "7.code_styles.html#как-работает-python",
    "title": "CodeStyle",
    "section": "Как работает python?",
    "text": "Как работает python?\n\n\nPython не преобразует свой код в машинный код, который может понять аппаратное обеспечение. Он фактически преобразует его в то, что называется байт-кодом. Таким образом, внутри Python компиляция происходит, но не на машинном языке. Он транслирует в байтовый код (.pyc или .pyo), и этот байтовый код не может быть понят процессором. Поэтому нам нужен интерпретатор, называемый виртуальной машиной python, для выполнения байт-кодов.\nИсходный код Python выполняет следующие действия для создания исполняемого кода:\n\nШаг 1: Транслятор Python читает исходный код или инструкцию Python. Затем проверяется правильность форматирования инструкции, т. е. проверяется синтаксис каждой строки. Если он обнаруживает ошибку, он немедленно останавливает перевод и показывает сообщение об ошибке.\nШаг 2: Если ошибки нет, т. е. если инструкция или исходный код Python правильно отформатированы, то компилятор переводит их в эквивалентную форму на промежуточном языке, называемом «Байт-код».\nШаг 3: Затем байт-код отправляется на виртуальную машину Python (PVM), которая является интерпретатором Python. PVM преобразует байт-код Python в машинно-исполняемый код. Если во время этой интерпретации возникает ошибка, то преобразование останавливается с сообщением об ошибке.\n\nДавайте повнимательнее разберемся с шагами 1 И 2, как именно питон проверяет синтаксис каждой строки и в какую структуру данных преобразует код для перевода его в byte code."
  },
  {
    "objectID": "7.code_styles.html#abstract-syntax-tree",
    "href": "7.code_styles.html#abstract-syntax-tree",
    "title": "CodeStyle",
    "section": "Abstract syntax tree",
    "text": "Abstract syntax tree\n\n\n\n\n\nАбстрактное синтаксическое дерево (AST) — это структура данных, используемая для рассуждения о грамматике языка программирования в контексте инструкций, представленных в исходном коде.\nТакой подход используется и в python для преобразования кода в байт-коды.\n\nПри наличии некоторого текстового исходного кода компилятор сначала размечает текст, чтобы идентифицировать ключевые слова языка программирования, переменные, литералы и т. д. Каждая лексема представляет собой «атом» инструкции.\nЗатем токены перестраиваются в AST, дерево, узлы которого являются «атомами» инструкций, и ограничивает отношения между атомами на основе грамматики языка программирования. Например, AST делает явным наличие вызова функции, соответствующих входных аргументов, инструкций, составляющих функцию, и т. д.\nЗатем компилятор может применить несколько оптимизаций к AST и в конечном итоге преобразовать его в двоичный код.\n\nСобственно, большинство анализаторов кода используют AST для анализа. В нем можно рассматривать взаимодействия нод с друг-другом, сравнивать это с паттернами, искать плохие практики и даже генерировать код в нужном стиле. В питоне есть специальный модуль ast для этого."
  },
  {
    "objectID": "7.code_styles.html#ast",
    "href": "7.code_styles.html#ast",
    "title": "CodeStyle",
    "section": "AST",
    "text": "AST\n\nimport ast\n\ncode = \"one_plus_two = 1 + 2\"\ntree = ast.parse(code)\nprint(ast.dump(tree, indent=4))\n\nModule(\n    body=[\n        Assign(\n            targets=[\n                Name(id='one_plus_two', ctx=Store())],\n            value=BinOp(\n                left=Constant(value=1),\n                op=Add(),\n                right=Constant(value=2)))],\n    type_ignores=[])\n\n\n\nТут чуть-чуть углубимся в сам питон. В нем есть модуль ast, который мы можем импортить и использовать.\nСначала это может быть неочевидно, но результат, сгенерированный на ast.dump()самом деле, представляет собой дерево:\n\nСлова, начинающиеся с заглавной буквы, являются узлами дерева.\nАтрибутами узлов являются либо ребра дерева, либо метаданные."
  },
  {
    "objectID": "7.code_styles.html#ast-1",
    "href": "7.code_styles.html#ast-1",
    "title": "CodeStyle",
    "section": "AST",
    "text": "AST\n\n\nДавайте переработаем вывод в диаграмму со следующими соглашениями:\n\nОдин прямоугольник для каждого узла, выделенный жирным шрифтом соответствующий тип узла.\nАтрибуты узлов, собирающие метаданные, отображаются синим цветом.\nДругие атрибуты узла аннотируются их типом.\nУзлы связаны на основе их атрибутов.\n\nИмея под рукой эту визуализацию, мы можем наблюдать несколько вещей.\n\nКорнем дерева является Module узел.\nУ Module есть атрибут body - список инструкций в модуле.\nНаш пример состоит из одной операции присваивания и, следовательно, Module.body содержит только один Assign узел.\nОперация присваивания имеет правую часть, определяющую операцию для выполнения, и левую часть, определяющую место назначения операции.\nСправа у нас бинарная операция, которая содержит два операнда(константы) и операцию - сложение.\nСлева же имя переменной и ссылка ctx на переменную в программе.\n\nСобственно осознав эту структуру данных мы можем по ней передвигаться."
  },
  {
    "objectID": "7.code_styles.html#ast-visit-functions",
    "href": "7.code_styles.html#ast-visit-functions",
    "title": "CodeStyle",
    "section": "AST visit functions",
    "text": "AST visit functions\n\nimport ast\n\ncode = \"one_plus_two = 1 + 2\"\ntree = ast.parse(code)\nfor node in ast.walk(tree):\n        print(node.__class__.__name__)\n\nModule\nAssign\nName\nBinOp\nStore\nConstant\nAdd\nConstant\n\n\n\nfor name, value in ast.iter_fields(tree):\n        print(name, value)\n\nbody [<ast.Assign object at 0x7fec84e46320>]\ntype_ignores []\n\n\n\nfor node in ast.iter_child_nodes(tree):\n        print(node.__class__.__name__)\n\nAssign\n\n\n\nДавайте рассмотрим функции для итерирования по дереву. Например функция walk позволяет совершать обход в глубину ast. А функции iter_fields, iter_child_nodes просмотреть поля ноды или просмотреть ее потомков, они не рекурсивны и применяются только к одному узлу дерева.\nС помощью них можно анализировать ast дерево, но это не очень удобно, поэтому предлагаю взглянуть на NodeVisitor."
  },
  {
    "objectID": "7.code_styles.html#nodevisitor",
    "href": "7.code_styles.html#nodevisitor",
    "title": "CodeStyle",
    "section": "NodeVisitor",
    "text": "NodeVisitor\n\nimport ast\n\nclass BinOpVisitor(ast.NodeVisitor):\n\n    def visit_BinOp(self, node):\n        print(f\"found BinOp at line: {node.lineno}\")\n        self.generic_visit(node)\n\ncode = \"one_plus_two = 1 + 2\"\ntree = ast.parse(code)\nBinOpVisitor().visit(tree)\n\nfound BinOp at line: 1\n\n\n\nЭто инструмент для рекурсивного спуска по ast. При этом использует generic_visit, то есть смотрит есть ли у него функции visit_<NODE_NAME> и если есть вызывает ее. Соответственно вам нужно определить такие функция для определенных nodes и вы сможете проверять различные конструкции в коде использую такой класс."
  },
  {
    "objectID": "7.code_styles.html#а-что-если-написать-свой-линтер",
    "href": "7.code_styles.html#а-что-если-написать-свой-линтер",
    "title": "CodeStyle",
    "section": "А что если написать свой линтер?",
    "text": "А что если написать свой линтер?\n\nfrom typing import Any, Generator, Tuple\n\n__version__ = \"0.1.0\"\nERROR = Tuple[int, int, str, Any]\nERROR_GENERATOR = Generator[ERROR, None, None]\n\nclass NoConstantPlugin:\n    name = __name__\n    version = __version__\n\n    def __init__(self, tree: ast.AST):\n        self._tree = tree\n\n    def run(self) -> ERROR_GENERATOR:\n        visitor = Visitor()\n        visitor.visit(self._tree)\n\n        for line, col, msg in visitor.errors:\n            yield line, col, msg, type(self)\n\n\nЭто сделать достаточно легко, как я говорил ранее flake8 имеет огромное количество плагинов и написать их не составляет труда. Давайте напишем плагин, который будет запрещать использовать константы отличные от 0, 1, 10 и 100.\nДля этого вам нужно создать класс, в конструктор которого будет передаваться abstract syntax tree. А также он будет иметь метод-генератор, который будет выдавать ошибки при их наличии.\nОсталось только разобраться с классом visitor и зарегистрировать наш плагин."
  },
  {
    "objectID": "7.code_styles.html#plugin-visitor",
    "href": "7.code_styles.html#plugin-visitor",
    "title": "CodeStyle",
    "section": "Plugin visitor",
    "text": "Plugin visitor\n\nimport ast\n\nclass Visitor(ast.NodeVisitor):\n    errors: list[tuple[int, int, str]] = []\n   \n    def visit_Constant(self, node: ast.AST):\n        if node.value not in (0, 1, 10, 100):\n            self.errors.append((node.lineno, node.col_offset, \"NAC100 Not allowed CONSTANT in code\"))\n        self.generic_visit(node)\n\nТеперь запустим\n\ncode = \"one_plus_two = 1 + 2\"\ntree = ast.parse(code)\nnext(NoConstantPlugin(tree).run())\n\n(1, 19, 'NAC100 Not allowed CONSTANT in code', __main__.NoConstantPlugin)\n\n\n\nДля класса visitor мы можем воспользоваться ast.NodeVisitor и написать функцию для посещения node Constant. В ней сделаем проверку на значение константы, если оно отлично, то добавляем в список ошибку, если нет, то все верно и мы продолжаем итерироваться по ast.\nТеперь нужно сделать так что бы наш код мог использоваться flake8."
  },
  {
    "objectID": "7.code_styles.html#добавляем-плагин-flake8",
    "href": "7.code_styles.html#добавляем-плагин-flake8",
    "title": "CodeStyle",
    "section": "Добавляем плагин Flake8",
    "text": "Добавляем плагин Flake8\nsetup.py\nflake8_entry_point = # ...\n\nsetuptools.setup(\n    # snip ...\n    entry_points={\n        flake8_entry_point: [\n            'NAC = flake8_example:NoConstantPlugin',\n        ],\n    },\n    # snip ...\n)\npyproject.toml\n[tool.poetry.plugins.\"flake8.extension\"]\nNAC = \"flake8_example:NoConstantPlugin\"\n\nЧто бы его зарегистрировать надо добавить в информацию о проекте flake8 entry point, в котором указано где определен ваш класс плагина.\nНа слайде показано как это сделать если вы используете традиционный setup.py и как если вы используете pyptoject. После этого ваш пакет можно собрать, опубликовать и устанавливать вместе с flake8 и другими плагинами для анализа кода."
  },
  {
    "objectID": "2.replication_crisis.html",
    "href": "2.replication_crisis.html",
    "title": "Replication crisis",
    "section": "",
    "text": "Машинное обучение (МО) нашло применение в исследованиях всех областей науки и во многом заменило традиционную статистику. И хотя для анализа данных зачастую проще использовать именно МО, присущий этой технологии «подход чёрной коробки» вызывает серьёзные проблемы при интерпретации результатов.\nТермин «кризис воспроизводимости» означает, что тревожно большое количество результатов научных экспериментов не нашли своего подтверждения при проведении тех же манипуляций другими группами учёных. Это может означать, что результаты, полученные в ходе изначальных работ, ошибочны. Согласно данным одного анализа, до 85 % всех проведённых в мире исследовательских работ в области биомедицины не привели к значимым результатам."
  },
  {
    "objectID": "2.replication_crisis.html#научный-метод-проведения-исследований",
    "href": "2.replication_crisis.html#научный-метод-проведения-исследований",
    "title": "Replication crisis",
    "section": "Научный метод проведения исследований",
    "text": "Научный метод проведения исследований\n\n\n\n\n\n\n\n\n\nОбъективность\nВоспроизводимость\nВалидность\n\n\n\n\nСобственно в научном сообществе есть методология проведения исследований и познания. Называется она научный метод. Основными требованиями к которому являются объективность, воспроизводимость и валидность.\nОбъекти́вность — отношение к объекту (явлению) и его характеристикам, процессам, как к независимому от воли и желания человека. Объективность подразумевает наличие знаний как таковых об объекте (явлении). Устойчивость объективности зависит от количества и точности понимания различных параметров объекта и/или процессов явления.\nВоспроизводимость - главный принцип научного метода . Это означает, что результат, полученный в ходе эксперимента или наблюдательного исследования , должен быть снова достигнут с высокой степенью согласия, когда исследование повторяется с использованием одной и той же методологии разными исследователями. Только после одного или нескольких таких успешных повторений результат должен быть признан научным знанием.\nВали́дность — обоснованность и пригодность применения методик и результатов исследования в конкретных условиях. Более прикладное определение понятия «валидность» — мера соответствия методик и результатов исследования поставленным задачам.\nДавайте уделим чуть больше внимания воспроизводимости. Так как сегодня мы рассматриваем кризис воспроизводимости."
  },
  {
    "objectID": "2.replication_crisis.html#какая-бывает-воспроизводимость",
    "href": "2.replication_crisis.html#какая-бывает-воспроизводимость",
    "title": "Replication crisis",
    "section": "Какая бывает воспроизводимость1?",
    "text": "Какая бывает воспроизводимость1?\n\nПовторяемость измерений(также сходимость результатов измерений, англ. Repeatability)\nПовторяемость исследований (англ. Replicability) (Different team, same experimental setup)\nВоспроизводимость (англ. Reproducibility)\n\n\n\n\n\nНа основании определений Ассоциации вычислительной техники я предлагаю принять следующие определения:\nПовторяемость измерений(также сходимость результатов измерений, англ. Repeatability) (Same team, same experimental setup): Результат может быть получен с заявленной точностью одной и той же командой, используя одну и ту же процедуру измерения, одну и ту же измерительную систему, при одинаковых рабочих условиях, в одном месте при нескольких испытаниях. Для вычислительных экспериментов это означает, что исследователь может надежно повторить собственное вычисление.\nПовторяемость исследований (англ. Replicability) (Different team, same experimental setup): Результат может быть получено с заявленной точностью другой командой с использованием той же процедуры измерения, той же измерительной системы, в тех же рабочих условиях, в том же или другом месте при нескольких испытаниях. Для вычислительных экспериментов это означает, что независимая группа может получить тот же результат, используя наработки автора, вплоть до его реализаций.\nВоспроизводимость (англ. Reproducibility): Результат может быть получено с заявленной точностью другой командой, другой измерительной системой, в другом месте на нескольких испытаниях. Для вычислительных экспериментов это означает, что независимая группа может получить тот же результат, используя артефакты, которые они разрабатывают полностью независимо."
  },
  {
    "objectID": "2.replication_crisis.html#что-такое-воспроизводимое-исследованиеви",
    "href": "2.replication_crisis.html#что-такое-воспроизводимое-исследованиеви",
    "title": "Replication crisis",
    "section": "Что такое воспроизводимое исследование(ВИ)",
    "text": "Что такое воспроизводимое исследование(ВИ)\n\n«Цель воспроизводимых исследований - привязать конкретные инструкции к анализу данных и экспериментальным данным, чтобы исследование можно было воссоздать, лучше понять и проверить».\n\n\nВоспроизводимые исследования (Reproducible research) - это термин, используемый в некоторых областях исследований для обозначения определенного способа проведения анализа, который предоставляет:\n\nинструменты преобразующие необработанные данные и метаданные в обработанные данные;\nинструменты выполняющие анализ данных;\nинструменты агрегирующие анализы в отчет.\n\nВ некоторых случаях указывают конкретно какие инструменты и шаги имеются в виду, это может быть программный продукт, публикации, определенные лабораторные условия и т.д.\nКогда предоставляются объекты анализа, например данные, и инструменты, а также алгоритмы последовательного решения задачи, это позволяет другим исследователям:\n\nвыполнять анализ, о котором не сообщалось первыми исследователями;\nпроверить правильность исследований, выполненных первыми исследователями;"
  },
  {
    "objectID": "2.replication_crisis.html#в-чем-причина-кризиса-воспроизводимости",
    "href": "2.replication_crisis.html#в-чем-причина-кризиса-воспроизводимости",
    "title": "Replication crisis",
    "section": "В чем причина кризиса воспроизводимости?",
    "text": "В чем причина кризиса воспроизводимости?\n\n\n\nнедостаточное понимание алгоритма ML.\nнедостаточное знакомство с исходными данными.\nневерная интерпретация результатов.\n\n\n \n\n\n\nНедостаточное понимание алгоритма — очень распространённая проблема в машинном обучении. Это серьезная проблема при работе с нейросетями ввиду множества параметров (зачастую для глубоких нейросетей количество параметров может составлять миллионы). Помимо этих параметров надо принимать в расчёт и гиперпараметры — такие, как скорость обучения, метод инициализации, количество итераций, архитектура нейросети.\nДля решения проблемы мало осознать, что исследователь недостаточно хорошо понимает работу алгоритма. Как можно сравнить результаты, если в разных работах применялись отличающиеся по структуре нейронные сети? Многослойная нейронная сеть имеет очень сложную динамическую структуру. Поэтому даже добавление единственной переменной или смена одного гиперпараметра может значительно повлиять на результаты.\nПлохое понимание исходных данных также является серьёзной проблемой, но эта проблема существовала и во время работы с традиционными статистическими методами. Ошибки в сборе данных — такие, как ошибки квантования, неточности считывания и использование замещающих переменных — самые распространённые затруднения. Субоптимальные данные всегда будут проблемой, но понимать, какой алгоритм применить к какому типу данных — невероятно важно, это значительно повлияет на результат.\nОшибочная оценка результатов может быть весьма распространена в научном мире. Одна из причин — видимая корреляция не всегда отражает реальную взаимосвязь. Есть несколько причин, почему переменные А и B могут коррелировать:\n\nA может изменяться при изменении B;\nB может изменяться при изменении A;\nA и B могут изменяться при изменении общей базовой переменной, C;\nкорреляция A и B может быть ложной. Продемонстрировать корреляцию двух значений легко, гораздо сложнее определить её причину. Погуглив «spurious correlations» (ложная корреляция), вы найдёте весьма интересные и забавные примеры, имеющие статистическое значение:\n\nсобственно на слайде вы можете видеть некоторые из них, например расходы США на науку, космос и технологии явно коррелируют с самоубийствами путем повешения, а от количества разводов в Мэне зависит потребление маргарина на душу населения.\nВсё это может выглядеть забавными совпадениями, но смысл в том, что алгоритм машинного обучения, обработав эти переменные единым набором, воспримет их как взаимозависимые, не подвергая эту зависимость сомнению. То есть алгоритм будет неточным или ошибочным, поскольку ПО выделит в датасете паттерны, которых не существует в реальном мире. Это примеры ложных корреляций, которые в последние годы опасно распространились в связи с использованием наборов данных из тысяч переменных.\nТак же есть такие техники как p-hacking или форсирование корреляций.\nСуть p-hacking’а состоит в дотошном поиске в наборе данных статистически значимых корреляций и принятии их за научно обоснованные.\nЕщё одна проблема алгоритмов машинного обучения заключается в том, что алгоритм должен делать предположения. Алгоритм не может «ничего не найти». Это означает, что алгоритм либо найдёт способ интерпретировать данные независимо от того, насколько они соотносятся между собой, либо не придёт к какому-либо определённому заключению (обычно это означает, что алгоритм был неверно настроен или данные плохо подготовлены)."
  },
  {
    "objectID": "2.replication_crisis.html#правила-проведения-воспроизводимых-исследований",
    "href": "2.replication_crisis.html#правила-проведения-воспроизводимых-исследований",
    "title": "Replication crisis",
    "section": "Правила проведения воспроизводимых исследований",
    "text": "Правила проведения воспроизводимых исследований\n\nДля каждого полученного результата сохраните алгоритм его получения.\nИзбегайте этапов ручного управления данными или процессом.\nСохраните точные версии всех использованных внешних инструментов.\nИспользуйте контроль версий.\nХраните все промежуточные результаты в стандартизированном виде.\nДля алгоритмов использующих случайность записывайте их random_state.\nВсегда храните вместе с графиками данные.\nИерархический подход при генерировании результатов анализа.\nВсегда указывайте вместе текстовые утверждения и результаты исследования.\nОбеспечивайте доступность ваших результатов, данных и исследований.\n\n\n\nВажно знать каким образом вы получили те или иные результаты. Знание того, как вы перешли от необработанных данных к заключению, позволяет вам: защищать, обновлять, воспроизводить и передавать свои результаты исследований.\nМожет возникнуть соблазн открыть файлы данных в редакторе и вручную исправить пару ошибок форматирования или удалить выбросы. Кроме того, современные редакторы позволяют легко форматировать файлы огромных размеров. Однако соблазну сократить ваш алгоритм следует сопротивляться. Ручная обработка данных — это скрытая манипуляция.\nВам необходимо задокументировать выпуск и версию всего используемого программного обеспечения, включая операционную систему. Незначительные изменения в программном обеспечении могут повлиять на результаты. В идеале вы должны настроить виртуальную машину или контейнер со всем программным обеспечением, используемым для запуска ваших скриптов. Это позволяет сделать снимок вашей аналитической экосистемы, что упрощает воспроизведение ваших результатов.\nДля отслеживания версий ваших скриптов следует использовать систему контроля версий, такую ​​как Git. Вы должны пометить (сделать снимок) текущее состояние скриптов и ссылаться на этот тег во всех получаемых вами результатах. Если вы затем решите изменить свои алгоритмы, что вы обязательно сделаете, можно будет вернуться во времени и получить точные сценарии, которые использовались для получения заданного результата.\nЕсли вы соблюдаете Правило № 1, в теории уже возможно воссоздать любые результаты на основе необработанных данных. Однако хотя это может быть теоретически возможно, на практике могут быть ограничивающие факторы. Проблемы могут быть следующие:\n\n\nотсутствие ресурсов для запуска результатов с нуля (например, если использовались значительные вычислительные ресурсы кластера)\nотсутствие лицензий на некоторые инструменты, если использовались коммерческие инструменты\nнедостаточная техническая доступность некоторых инструментов\n\nВ этих случаях может быть полезно начать исследование с набора производных данных, которые уже могут представлять больше пользы или быть более удобными, чем необработанных данных. Хранение этих промежуточных наборов данных (например, в формате CSV) предоставляет больше возможностей для дальнейшего анализа и может упростить определение проблемных результатов, когда они ошибочны, поскольку нет необходимости все переделывать.\n\nОдна вещь, которую специалисты по данным часто не могут сделать — это установить исходные значения для своего анализа. Это делает невозможным точное воссоздание исследований машинного обучения. Многие алгоритмы машинного обучения включают стохастический элемент, и, хотя надежные результаты могут быть статистически воспроизводимыми, нет ничего, что можно было бы сравнить с теплым сиянием в глазах проверяющего при точном совпадении результатов.\nЕсли вы используете скриптовый язык программирования, ваши графики, скорее всего, автоматически. Однако если вы используете такой инструмент, как Excel, убедитесь, что вы сохранили начальные данные. Это позволяет не только воспроизвести график, но также более детально просмотреть лежащие в основе данные. Также стоит всегда сохранять алгоритмы, которые вы использовали для получения графиков, на основе которых вы потом приводите какие-либо утверждения.\nНаша задача как специалистов по обработке данных — обобщить данные в той или иной форме. Вот что включает в себя извлечение информации из данных. Однако резюмирование также является простым способом неправильного использования данных, поэтому важно, чтобы заинтересованные стороны могли разбить сводку на отдельные точки данных. Для каждого итогового результата укажите ссылку на данные, использованные для расчета итогового значения.\nВ конце работы результаты анализа данных оформляются в текстовом виде. И слова неточны. Иногда бывает трудно определить связь между выводами и анализом. Поскольку отчет часто является самой важной частью исследования, важно, чтобы его можно было связать с результатами и, в соответствии с правилом № 1, с исходными данными.\nВ коммерческих условиях может быть нецелесообразно предоставлять открытый доступ ко всем данным. Однако имеет смысл предоставить доступ другим пользователям в вашей организации. Облачные системы управления исходным кодом, такие как Bitbucket и GitHub, позволяют создавать частные репозитории, к которым могут получить доступ любые авторизованные коллеги."
  },
  {
    "objectID": "2.replication_crisis.html#заключение",
    "href": "2.replication_crisis.html#заключение",
    "title": "Replication crisis",
    "section": "Заключение",
    "text": "Заключение\nМашинное обучение в науке представляет проблему из-за того, что результаты недостаточно воспроизводимы. Однако учёные в курсе этой проблемы и работают над моделями ML, дающими более воспроизводимый и прозрачный результат. Настоящий прорыв произойдет, когда эта задача будет решена для нейросети.\nКак сказал физик Ричард Фейнман в своей речи перед выпускниками Калифорнийского технологического института в 1974 году:\n\n“Первый принцип науки заключается в том, чтобы не одурачить самого себя. И как раз себя-то одурачить проще всего.”"
  },
  {
    "objectID": "2.replication_crisis.html#список-источников",
    "href": "2.replication_crisis.html#список-источников",
    "title": "Replication crisis",
    "section": "Список источников",
    "text": "Список источников\n\nКризис машинного обучения в научных исследованиях\nReproducibility vs. Replicability: A Brief History of a Confused Terminology\nRepeatability, Repr)oducibility, and Replicability: Tackling the 3R challenge in biointerface science and engineering\n10 RULES FOR CREATING REPRODUCIBLE RESULTS IN DATA SCIENCE"
  },
  {
    "objectID": "1.entry.html",
    "href": "1.entry.html",
    "title": "Engineering practices in ML",
    "section": "",
    "text": "Познакомиться с повсеместными проблемами отрасли и их глубиной.\n\n\n\nУзнать их негативное влияние на разных уровнях (разработчик, команда, компания, индустрия).\n\n\n\n\nИзучить инструменты и практики позволяющие снизить это влияние.\n\n\n\n\nПоднять уровень компетенций в области ML."
  },
  {
    "objectID": "1.entry.html#почему-такое-название",
    "href": "1.entry.html#почему-такое-название",
    "title": "Engineering practices in ML",
    "section": "Почему такое название?",
    "text": "Почему такое название?\n\n\n\n\n\n\nСовременный data science и machine learning требуют написания большого количества кода. Прошли те дни когда все считалось на бумажках или в экселе. Теперь для разработки ML проектов требуется использование языков программирования общего пользования, библиотек и прочего. Собственно требуется “разработка ПО”. А это область давно и хорошо изучена."
  },
  {
    "objectID": "1.entry.html#инженерные-практики-в-разработке-по",
    "href": "1.entry.html#инженерные-практики-в-разработке-по",
    "title": "Engineering practices in ML",
    "section": "Инженерные практики в разработке ПО",
    "text": "Инженерные практики в разработке ПО\n\nTest driven development\nRefactoring\nDesign Improvement\nContinuous integration - continuous delivery\nPair programming\nAutomated tests\n\n\nВ области разработки ПО уже давно используются различные инженерные практики, общепринятые подходы для развития и увеличения производительности команды разработки ПО. Зачастую они упоминаются в различных agile методологиях, но на самом деле могут существовать отдельно. Применение таких практик позволяет поддерживать хорошее качество кода и архитектуры, быстрее поставлять новые фичи, повышать уровень развития команды. Собственно применение этих практик уже устоявшаяся вещь в индустрии разработки ПО."
  },
  {
    "objectID": "1.entry.html#какой-план",
    "href": "1.entry.html#какой-план",
    "title": "Engineering practices in ML",
    "section": "Какой план?",
    "text": "Какой план?\n\nПонимание проблематики\nОрганизация процесса в команде\nИнструменты воспроизводимости исследований\n\n\nСобственно каков план на этот курс. Сегодня мы будем разбираться с проблематикой. На следующих занятиях будем затрагивать темы организации процесса в команде и инструменты необходимые для воспроизводимости исследований."
  },
  {
    "objectID": "1.entry.html#i-dontt-like-jupyter-notebooks",
    "href": "1.entry.html#i-dontt-like-jupyter-notebooks",
    "title": "Engineering practices in ML",
    "section": "I dont’t like jupyter notebooks1",
    "text": "I dont’t like jupyter notebooks1\n\nскрытые состояния, которые можно забыть или испортить\nвозможность запускать код в произвольном порядке\nнет поддержки линтеров, форматеров и автокомплита\nне поддерживают модульность и переиспользование кода\nпоощрают написание не тестируемого кода\nприходиться писать смешанный код\nне удобны для версионировании в гите\nнет requirements\n\nРезюме: jupyter notebooks поощряют вредные привычки и плохие процессы, а также препятствуют появлению хороших привычек."
  }
]